 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:38.676Z debug(replica): 2n: on_request: forwarding register to primary (view=0)
2025-12-10 11:42:38.676Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 175618757085245920522970776582928013076, .checksum_padding = 0, .checksum_body = 80262522984338905420932325260476423013, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 0, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 0, .timestamp = 0, .request = 0, .operation = vsr.Operation.register, .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:38.676Z debug(message_bus): 2: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Request{ .checksum = 175618757085245920522970776582928013076, .checksum_padding = 0, .checksum_body = 80262522984338905420932325260476423013, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 0, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 0, .timestamp = 0, .request = 0, .operation = vsr.Operation.register, .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:38.687Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-10 11:42:38.687Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-10 11:42:38.687Z debug(vsr): 1: pulse_timeout fired
2025-12-10 11:42:38.687Z debug(vsr): 1: pulse_timeout reset
2025-12-10 11:42:38.707Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-10 11:42:38.676Z debug(replica): 2n: on_message: view=0 status=normal vsr.message_header.Header.Request{ .checksum = 175618757085245920522970776582928013076, .checksum_padding = 0, .checksum_body = 80262522984338905420932325260476423013, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 0, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 0, .timestamp = 0, .request = 0, .operation = vsr.Operation.register, .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:38.707Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-10 11:42:39.115Z debug(replica): 2n: on_request: forwarding register to primary (view=0)
2025-12-10 11:42:39.115Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 175618757085245920522970776582928013076, .checksum_padding = 0, .checksum_body = 80262522984338905420932325260476423013, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 0, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 0, .timestamp = 0, .request = 0, .operation = vsr.Operation.register, .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z debug(message_bus): 2: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Request{ .checksum = 175618757085245920522970776582928013076, .checksum_padding = 0, .checksum_body = 80262522984338905420932325260476423013, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 0, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 0, .timestamp = 0, .request = 0, .operation = vsr.Operation.register, .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z info(message_bus): 1: set_and_verify_peer connection from client=204864005026426173254654640007015434914
2025-12-10 11:42:39.115Z debug(replica): 2n: on_message: view=0 status=normal vsr.message_header.Header.Request{ .checksum = 334940521279488267999903271169790815749, .checksum_padding = 0, .checksum_body = 199074541229396917298351072521439575149, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 5760, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 218038287841370738680574724088127301579, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 1, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 4294967295, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z debug(replica): 2n: on_request: ignoring (view=0 header.view=1)
2025-12-10 11:42:39.115Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z debug(replica): 2n: on_message: view=0 status=normal vsr.message_header.Header.Request{ .checksum = 192731837790586772831793888025568735385, .checksum_padding = 0, .checksum_body = 116644293290483624186003533138370079655, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 944, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 96411276156427999208461718944991637041, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 4, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 76546699, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z debug(replica): 2n: on_request: ignoring (view=0 header.view=1)
2025-12-10 11:42:39.115Z debug(replica): 1N: on_request: replying to duplicate request
2025-12-10 11:42:39.115Z debug(client_replies): 1: read_reply: start (client=204864005026426173254654640007015434914 reply=180721629067901931180688463199873763473)
2025-12-10 11:42:39.115Z info(message_bus): 2: set_and_verify_peer connection from client=204864005026426173254654640007015434914
2025-12-10 11:42:39.115Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PingClient{ .checksum = 30881730860482118781911000227227867210, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 204864005026426173254654640007015434914, .ping_timestamp_monotonic = 16850820215552084, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z debug(replica): 2n: on_message: view=0 status=normal vsr.message_header.Header.PingClient{ .checksum = 30881730860482118781911000227227867210, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 204864005026426173254654640007015434914, .ping_timestamp_monotonic = 16850820215552084, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z debug(replica): 1N: sending pong_client to client 204864005026426173254654640007015434914: vsr.message_header.Header.PongClient{ .checksum = 25300282711930289531846288672059451546, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850820215552084, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z debug(replica): 2n: sending pong_client to client 204864005026426173254654640007015434914: vsr.message_header.Header.PongClient{ .checksum = 261477026881093701522492997510004284004, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850820215552084, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:39.115Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:39.115Z info(message_bus): 2: set_and_verify_peer: connection from replica=0
2025-12-10 11:42:39.115Z debug(replica): 2n: on_message: view=0 status=normal vsr.message_header.Header.DoViewChange{ .checksum = 58774689605745891168782441821594914957, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.115Z debug(replica): 2n: jump_view: jumping to view change
2025-12-10 11:42:39.115Z info(replica): 2n: transition_to_view_change_status: view=0..1 status=vsr.replica.Status.normal..vsr.replica.Status.view_change
2025-12-10 11:42:39.115Z debug(replica): 2v: view_durable_update: view_durable=0..1 log_view_durable=0..0
2025-12-10 11:42:39.115Z debug(superblock): 2: view_change: commit_min=0..0 commit_max=0..0 commit_min_checksum=108034676951432761169128540124443993015..108034676951432761169128540124443993015 log_view=0..0 view=0..1 head=108034676951432761169128540124443993015..108034676951432761169128540124443993015
2025-12-10 11:42:39.115Z debug(superblock): 2: view_change: started
2025-12-10 11:42:39.115Z debug(superblock): 2: view_change: write_header: checksum=645496bdd28292132a94347ec0c43410 sequence=2 copy=0 size=8192 offset=0
2025-12-10 11:42:39.115Z debug(vsr): 2: ping_timeout started
2025-12-10 11:42:39.115Z debug(vsr): 2: commit_message_timeout stopped
2025-12-10 11:42:39.115Z debug(vsr): 2: normal_heartbeat_timeout stopped
2025-12-10 11:42:39.115Z debug(vsr): 2: start_view_change_window_timeout stopped
2025-12-10 11:42:39.115Z debug(vsr): 2: start_view_change_message_timeout started
2025-12-10 11:42:39.115Z debug(vsr): 2: view_change_status_timeout started
2025-12-10 11:42:39.115Z debug(vsr): 2: do_view_change_message_timeout started
2025-12-10 11:42:39.115Z debug(vsr): 2: repair_sync_timeout stopped
2025-12-10 11:42:39.115Z debug(vsr): 2: prepare_timeout stopped
2025-12-10 11:42:39.115Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-12-10 11:42:39.115Z debug(vsr): 2: pulse_timeout stopped
2025-12-10 11:42:39.115Z debug(vsr): 2: grid_repair_budget_timeout started
2025-12-10 11:42:39.115Z debug(vsr): 2: grid_scrub_timeout started
2025-12-10 11:42:39.115Z debug(vsr): 2: upgrade_timeout stopped
2025-12-10 11:42:39.115Z debug(vsr): 2: journal_repair_timeout stopped
2025-12-10 11:42:39.115Z debug(vsr): 2: journal_repair_budget_timeout started
2025-12-10 11:42:39.115Z debug(vsr): 2: request_start_view_message_timeout started
2025-12-10 11:42:39.115Z debug(replica): 2v: reset 0 do_view_change message(s) from view=null
2025-12-10 11:42:39.116Z debug(replica): 2v: sending do_view_change to replica 0: vsr.message_header.Header.DoViewChange{ .checksum = 322002469577275634469150461777001036937, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: send_message_to_replica: dropped do_view_change (view_durable=0 message.view=1)
2025-12-10 11:42:39.116Z debug(replica): 2v: sending do_view_change to replica 1: vsr.message_header.Header.DoViewChange{ .checksum = 322002469577275634469150461777001036937, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: send_message_to_replica: dropped do_view_change (view_durable=0 message.view=1)
2025-12-10 11:42:39.116Z debug(replica): 2v: on_do_view_change: ignoring (backup awaiting start_view)
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.DoViewChange{ .checksum = 58774689605745891168782441821594914957, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: on_do_view_change: ignoring (backup awaiting start_view)
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.Ping{ .checksum = 2810438873101453633151025687735443980, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850807227334137, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 270060309782368307165168412877648657743, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850807227334137, .pong_timestamp_wall = 1765366959116217689, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.DoViewChange{ .checksum = 58774689605745891168782441821594914957, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: on_do_view_change: ignoring (backup awaiting start_view)
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.DoViewChange{ .checksum = 58774689605745891168782441821594914957, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: on_do_view_change: ignoring (backup awaiting start_view)
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.Ping{ .checksum = 15982128190501135122133648001620584154, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850808235388594, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 186010777246995164761573017148404393518, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850808235388594, .pong_timestamp_wall = 1765366959116460689, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.DoViewChange{ .checksum = 58774689605745891168782441821594914957, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: on_do_view_change: ignoring (backup awaiting start_view)
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.Ping{ .checksum = 246508384922182745091084624129350706429, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850812191635725, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 131184576439597533196990516560702317017, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850812191635725, .pong_timestamp_wall = 1765366959116626450, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.Ping{ .checksum = 127811450771588796341404259212655433352, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850813196496731, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 317322137391859433171283117681860196241, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850813196496731, .pong_timestamp_wall = 1765366959116738290, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.Ping{ .checksum = 53580429881408311525027792759807061033, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850814201044936, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 272306191919599247036647943798177529569, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850814201044936, .pong_timestamp_wall = 1765366959116849211, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.Ping{ .checksum = 253899815740350467296992187272859183418, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850815204737018, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.116Z debug(replica): 2v: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 194829792206230157782301025222970250523, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850815204737018, .pong_timestamp_wall = 1765366959116958371, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.117Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.Ping{ .checksum = 99302775456918164832269580641357534188, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850816208698342, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.117Z debug(replica): 2v: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 262622685580606016223366171432181345335, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850816208698342, .pong_timestamp_wall = 1765366959117071092, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.117Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.Ping{ .checksum = 44731581344026427296724922717439636980, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850817216965239, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.117Z debug(replica): 2v: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 298610249424375185995024168520024384843, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850817216965239, .pong_timestamp_wall = 1765366959117179412, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.117Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.Ping{ .checksum = 252080760021262511515847854814217989907, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850818223804812, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.117Z debug(replica): 2v: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 256273518063284222829478482984619802111, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850818223804812, .pong_timestamp_wall = 1765366959117298252, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.117Z warning(replica): 2v: on_messages: message count=14 suspended=0
2025-12-10 11:42:39.117Z debug(client_replies): 1: read_reply: done (client=204864005026426173254654640007015434914 reply=180721629067901931180688463199873763473)
2025-12-10 11:42:39.117Z debug(replica): 1N: on_request: repeat reply (client=204864005026426173254654640007015434914 request=5)
2025-12-10 11:42:39.117Z debug(replica): 1N: sending reply to client 204864005026426173254654640007015434914: vsr.message_header.Header.Reply{ .checksum = 180721629067901931180688463199873763473, .checksum_padding = 0, .checksum_body = 262991487050001328813960342374172054376, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 544896, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 276407604296675853898847308413824879213, .request_checksum_padding = 0, .context = 280577675950734272305822183788388095003, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 7, .commit = 7, .timestamp = 1765366956846578035, .request = 5, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.117Z debug(superblock): 2: view_change: write_header: checksum=645496bdd28292132a94347ec0c43410 sequence=2 copy=1 size=8192 offset=24576
2025-12-10 11:42:39.117Z debug(superblock): 2: view_change: write_header: checksum=645496bdd28292132a94347ec0c43410 sequence=2 copy=2 size=8192 offset=49152
2025-12-10 11:42:39.117Z debug(superblock): 2: view_change: write_header: checksum=645496bdd28292132a94347ec0c43410 sequence=2 copy=3 size=8192 offset=73728
2025-12-10 11:42:39.117Z debug(superblock): 2: view_change: read_header: copy=0 size=8192 offset=0
2025-12-10 11:42:39.117Z debug(superblock): 2: view_change: read_header: copy=1 size=8192 offset=24576
2025-12-10 11:42:39.118Z debug(superblock): 2: view_change: read_header: copy=2 size=8192 offset=49152
2025-12-10 11:42:39.118Z debug(superblock): 2: view_change: read_header: copy=3 size=8192 offset=73728
2025-12-10 11:42:39.118Z debug(superblock_quorums): copy: 0/4: checksum=645496bdd28292132a94347ec0c43410 parent=1350be559db7dd9939178c4f0d892564 sequence=2
2025-12-10 11:42:39.118Z debug(superblock_quorums): copy: 1/4: checksum=645496bdd28292132a94347ec0c43410 parent=1350be559db7dd9939178c4f0d892564 sequence=2
2025-12-10 11:42:39.118Z debug(superblock_quorums): copy: 2/4: checksum=645496bdd28292132a94347ec0c43410 parent=1350be559db7dd9939178c4f0d892564 sequence=2
2025-12-10 11:42:39.118Z debug(superblock_quorums): copy: 3/4: checksum=645496bdd28292132a94347ec0c43410 parent=1350be559db7dd9939178c4f0d892564 sequence=2
2025-12-10 11:42:39.118Z debug(superblock_quorums): quorum: checksum=645496bdd28292132a94347ec0c43410 parent=1350be559db7dd9939178c4f0d892564 sequence=2 count=4 valid=true
2025-12-10 11:42:39.118Z debug(superblock): 2: view_change: installed working superblock: checksum=645496bdd28292132a94347ec0c43410 sequence=2 release=0.0.1 cluster=00000000000000000000000000000000 replica_id=134075807420264837279280775697797238201 size=1141374976 free_set_blocks_acquired_size=0 free_set_blocks_released_size=0 client_sessions_size=0 checkpoint_id=f222e9ce156b309eaeb4af665242ac18 commit_min_checksum=108034676951432761169128540124443993015 commit_min=0 commit_max=0 log_view=0 view=1 sync_op_min=0 sync_op_max=0 manifest_oldest_checksum=0 manifest_oldest_address=0 manifest_newest_checksum=0 manifest_newest_address=0 manifest_block_count=0 snapshots_block_checksum=0 snapshots_block_address=0
2025-12-10 11:42:39.118Z debug(superblock): 2: view_change: vsr_header: op=0 checksum=108034676951432761169128540124443993015
2025-12-10 11:42:39.118Z debug(superblock): 2: view_change: complete
2025-12-10 11:42:39.118Z debug(replica): 2v: view_durable_update_callback: (view_durable=1 log_view_durable=0)
2025-12-10 11:42:39.118Z debug(vsr): 2: view_change_status_timeout reset
2025-12-10 11:42:39.118Z debug(replica): 2v: sending do_view_change to replica 0: vsr.message_header.Header.DoViewChange{ .checksum = 322002469577275634469150461777001036937, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.118Z debug(replica): 2v: sending do_view_change to replica 1: vsr.message_header.Header.DoViewChange{ .checksum = 322002469577275634469150461777001036937, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.118Z debug(message_bus): 2: send_message_to_replica: no connection to=1 header=vsr.message_header.Header.DoViewChange{ .checksum = 322002469577275634469150461777001036937, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.125Z info(message_bus): 2: set_and_verify_peer: connection from replica=1
2025-12-10 11:42:39.125Z debug(replica): 2v: on_message: view=1 status=view_change vsr.message_header.Header.StartView{ .checksum = 244506651939017080641097768823128919758, .checksum_padding = 0, .checksum_body = 136272832586586177267924634842501247068, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1536, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.start_view, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .nonce = 0, .op = 0, .commit_max = 0, .checkpoint_op = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.125Z debug(journal): 2: remove_entries_from: op_min=1
2025-12-10 11:42:39.128Z debug(replica): 2v: on_start_view_set_journal: view=1 op=0..0 commit_max=0..0
2025-12-10 11:42:39.128Z info(replica): 2n: transition_to_normal_from_view_change_status: view=1..1 backup
2025-12-10 11:42:39.128Z debug(replica): 2n: view_durable_update: view_durable=1..1 log_view_durable=0..1
2025-12-10 11:42:39.128Z debug(superblock): 2: view_change: commit_min=0..0 commit_max=0..0 commit_min_checksum=108034676951432761169128540124443993015..108034676951432761169128540124443993015 log_view=0..1 view=1..1 head=108034676951432761169128540124443993015..108034676951432761169128540124443993015
2025-12-10 11:42:39.128Z debug(superblock): 2: view_change: started
2025-12-10 11:42:39.128Z debug(superblock): 2: view_change: write_header: checksum=7b32c8d46df0002da29939ed96939def sequence=3 copy=0 size=8192 offset=0
2025-12-10 11:42:39.128Z debug(vsr): 2: ping_timeout started
2025-12-10 11:42:39.128Z debug(vsr): 2: commit_message_timeout stopped
2025-12-10 11:42:39.128Z debug(vsr): 2: normal_heartbeat_timeout started
2025-12-10 11:42:39.128Z debug(vsr): 2: start_view_change_window_timeout stopped
2025-12-10 11:42:39.128Z debug(vsr): 2: start_view_change_message_timeout started
2025-12-10 11:42:39.128Z debug(vsr): 2: view_change_status_timeout stopped
2025-12-10 11:42:39.128Z debug(vsr): 2: do_view_change_message_timeout stopped
2025-12-10 11:42:39.128Z debug(vsr): 2: request_start_view_message_timeout stopped
2025-12-10 11:42:39.128Z debug(vsr): 2: repair_sync_timeout started
2025-12-10 11:42:39.128Z debug(vsr): 2: journal_repair_timeout started
2025-12-10 11:42:39.128Z debug(vsr): 2: grid_repair_budget_timeout started
2025-12-10 11:42:39.128Z debug(vsr): 2: grid_scrub_timeout started
2025-12-10 11:42:39.128Z debug(replica): 2n: reset 0 do_view_change message(s) from view=null
2025-12-10 11:42:39.128Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 18081821241008343383634529347251656618, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 108034676951432761169128540124443993015, .parent_padding = 0, .request_checksum = 278576268602162528949557794356482148330, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 0, .op = 1, .commit = 0, .timestamp = 1765366948714034019, .request = 0, .operation = vsr.Operation.pulse, .reserved = { 0, 0, 0 } }
2025-12-10 11:42:39.128Z debug(replica): 2n: on_prepare: caching prepare.op=1 (commit_min=0 op=0 commit_max=0 prepare_max=1007)
2025-12-10 11:42:39.128Z debug(replica): 2n: on_prepare: advancing: op=0..1 checksum=108034676951432761169128540124443993015..18081821241008343383634529347251656618
2025-12-10 11:42:39.128Z debug(journal): 2: set_header_as_dirty: op=1 checksum=18081821241008343383634529347251656618
2025-12-10 11:42:39.128Z debug(replica): 2n: append: appending to journal op=1
2025-12-10 11:42:39.128Z debug(journal): 2: write: view=1 slot=1 op=1 len=256: 18081821241008343383634529347251656618 starting
2025-12-10 11:42:39.129Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=1048576 len=4096 locked
2025-12-10 11:42:39.129Z debug(replica): 2n: repair_prepare: op=1 checksum=18081821241008343383634529347251656618 (already writing)
2025-12-10 11:42:39.129Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.StartView{ .checksum = 169664772576058242392648434478917957707, .checksum_padding = 0, .checksum_body = 228892895432759713061823750143282859599, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1792, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.start_view, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .nonce = 0, .op = 1, .commit_max = 1, .checkpoint_op = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.129Z debug(journal): 2: remove_entries_from: op_min=2
2025-12-10 11:42:39.131Z debug(replica): 2n: on_start_view_set_journal: view=1 op=1..1 commit_max=0..1
2025-12-10 11:42:39.131Z debug(replica): 2n: repair_prepare: op=1 checksum=18081821241008343383634529347251656618 (already writing)
2025-12-10 11:42:39.131Z debug(replica): 2n: commit_start_journal: cached prepare op=1 checksum=18081821241008343383634529347251656618
2025-12-10 11:42:39.131Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 196032104091022842131016459366917054237, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850811783792582, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.131Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.131Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.131Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.131Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 65719415497020922678049581424719074099, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850812290112538, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.131Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 24089493979190781414095289305932080651, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850812290112538, .pong_timestamp_wall = 1765366959131946102, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.132Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 189227403206225167813841001545904874093, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850812290295099, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.132Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.132Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.132Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.132Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 98343145170820896037359488058921268715, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850812793011203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.132Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.132Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.132Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.132Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 265367745125018423467616987133454872134, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850813296742791, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.132Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 5002099605156886575715632813008053437, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850813296742791, .pong_timestamp_wall = 1765366959132222503, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.132Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 57512153279626889336052105955470689851, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850813296938111, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.132Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.132Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.132Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.132Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 305962856260728407932417052189347287979, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850813800082097, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.132Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.132Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.135Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-10 11:42:39.670Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.670Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-10 11:42:39.670Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 16301796693886779916246653143124670525, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850814303181642, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.670Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 170520567663093017059547125165050952279, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850814303181642, .pong_timestamp_wall = 1765366959670560608, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.670Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 229758773797032685213464175381220019920, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850814303357043, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.670Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.670Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.670Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.671Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 120444804580537033010435985280403305393, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850814806521309, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.671Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.671Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.671Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.671Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.671Z debug(replica): 1N: on_request: replying to duplicate request
2025-12-10 11:42:39.671Z debug(client_replies): 1: read_reply: start (client=204864005026426173254654640007015434914 reply=180721629067901931180688463199873763473)
2025-12-10 11:42:39.671Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 7606540928410782813205723823754864820, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850815309966575, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.671Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 85025695870020327005711475559878094487, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850815309966575, .pong_timestamp_wall = 1765366959671295930, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.671Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 24089493979190781414095289305932080651, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850812290112538, .pong_timestamp_wall = 1765366959131946102, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.671Z debug(clock): 1: learn: m0=16850812290112538 < window.monotonic=16850818603563140
2025-12-10 11:42:39.671Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 4971262988131162549243484240184587938, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850815310151616, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.671Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.671Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.671Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.671Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 104381687639409022989045460900027058575, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850815812029437, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.671Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.671Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.671Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.671Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 181604144354019470543774617531578620913, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850816314230259, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.671Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 79643426239505077245501876790005127120, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850816314230259, .pong_timestamp_wall = 1765366959671936932, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.672Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 136377151747889813307039971653263589688, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850816314382860, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.672Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.672Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.672Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.672Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 70240605744962707863724804534003194535, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850816817676806, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.672Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.672Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.672Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.672Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 43790630318338124855983490579792493221, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850817321628674, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.672Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 137837272549377028054120880903587260932, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850817321628674, .pong_timestamp_wall = 1765366959672568175, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.672Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 203352191496176042363569151002038614987, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850817321814555, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.672Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.672Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.672Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.672Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 247242133042122590256580098103045962465, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850817826820307, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.673Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.673Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.673Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.673Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 223265976286074847533798188660845537989, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 16850818329686251, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.673Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 320505438368692247823387156262981715782, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850818329686251, .pong_timestamp_wall = 1765366959673193857, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.673Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 200876616767081673706809013791914675343, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 18081821241008343383634529347251656618, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 1, .timestamp_monotonic = 16850818329845732, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.673Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.673Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.673Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.673Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 180712438674514203771522049436898328741, .checksum_padding = 0, .checksum_body = 32411413746896858696526384829824831279, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 18081821241008343383634529347251656618, .parent_padding = 0, .request_checksum = 175618757085245920522970776582928013076, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 2, .commit = 1, .timestamp = 1765366956539996796, .request = 0, .operation = vsr.Operation.register, .reserved = { 0, 0, 0 } }
2025-12-10 11:42:39.673Z debug(replica): 2n: on_prepare: caching prepare.op=2 (commit_min=0 op=1 commit_max=1 prepare_max=1007)
2025-12-10 11:42:39.673Z debug(replica): 2n: on_prepare: advancing: op=1..2 checksum=18081821241008343383634529347251656618..180712438674514203771522049436898328741
2025-12-10 11:42:39.673Z debug(journal): 2: set_header_as_dirty: op=2 checksum=180712438674514203771522049436898328741
2025-12-10 11:42:39.673Z debug(replica): 2n: append: appending to journal op=2
2025-12-10 11:42:39.673Z debug(journal): 2: write: view=1 slot=2 op=2 len=512: 180712438674514203771522049436898328741 starting
2025-12-10 11:42:39.673Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=2097152 len=4096 locked
2025-12-10 11:42:39.673Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.674Z debug(replica): 2n: repair_prepare: op=1 checksum=18081821241008343383634529347251656618 (already writing)
2025-12-10 11:42:39.674Z debug(replica): 2n: repair_prepare: op=2 checksum=180712438674514203771522049436898328741 (already writing)
2025-12-10 11:42:39.674Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.674Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 145628026037401329440524624120816611215, .checksum_padding = 0, .checksum_body = 199074541229396917298351072521439575149, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 5760, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 180712438674514203771522049436898328741, .parent_padding = 0, .request_checksum = 334940521279488267999903271169790815749, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 3, .commit = 2, .timestamp = 1765366956541199482, .request = 1, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:39.674Z debug(replica): 2n: on_prepare: advancing commit_max=1..2
2025-12-10 11:42:39.674Z debug(replica): 2n: on_prepare: caching prepare.op=3 (commit_min=0 op=2 commit_max=2 prepare_max=1007)
2025-12-10 11:42:39.674Z debug(replica): 2n: on_prepare: advancing: op=2..3 checksum=180712438674514203771522049436898328741..145628026037401329440524624120816611215
2025-12-10 11:42:39.674Z debug(journal): 2: set_header_as_dirty: op=3 checksum=145628026037401329440524624120816611215
2025-12-10 11:42:39.674Z debug(replica): 2n: append: appending to journal op=3
2025-12-10 11:42:39.674Z debug(journal): 2: write: view=1 slot=3 op=3 len=5760: 145628026037401329440524624120816611215 starting
2025-12-10 11:42:39.674Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=3145728 len=8192 locked
2025-12-10 11:42:39.674Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.674Z debug(replica): 2n: repair_prepare: op=1 checksum=18081821241008343383634529347251656618 (already writing)
2025-12-10 11:42:39.674Z debug(replica): 2n: repair_prepare: op=2 checksum=180712438674514203771522049436898328741 (already writing)
2025-12-10 11:42:39.674Z debug(replica): 2n: repair_prepare: op=3 checksum=145628026037401329440524624120816611215 (already writing)
2025-12-10 11:42:39.674Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.674Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 337681397177720383533166008301995067443, .checksum_padding = 0, .checksum_body = 116644293290483624186003533138370079655, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 944, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 145628026037401329440524624120816611215, .parent_padding = 0, .request_checksum = 176270417088940460935617877678172533171, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 4, .commit = 3, .timestamp = 1765366956543406687, .request = 2, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:39.674Z debug(replica): 2n: on_prepare: advancing commit_max=2..3
2025-12-10 11:42:39.674Z debug(replica): 2n: on_prepare: caching prepare.op=4 (commit_min=0 op=3 commit_max=3 prepare_max=1007)
2025-12-10 11:42:39.674Z debug(replica): 2n: on_prepare: advancing: op=3..4 checksum=145628026037401329440524624120816611215..337681397177720383533166008301995067443
2025-12-10 11:42:39.674Z debug(journal): 2: set_header_as_dirty: op=4 checksum=337681397177720383533166008301995067443
2025-12-10 11:42:39.674Z debug(replica): 2n: append: appending to journal op=4
2025-12-10 11:42:39.674Z debug(journal): 2: write: view=1 slot=4 op=4 len=944: 337681397177720383533166008301995067443 starting
2025-12-10 11:42:39.674Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=4194304 len=4096 locked
2025-12-10 11:42:39.674Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.674Z debug(replica): 2n: repair_prepare: op=1 checksum=18081821241008343383634529347251656618 (already writing)
2025-12-10 11:42:39.674Z debug(replica): 2n: repair_prepare: op=2 checksum=180712438674514203771522049436898328741 (already writing)
2025-12-10 11:42:39.674Z debug(replica): 2n: repair_prepare: op=3 checksum=145628026037401329440524624120816611215 (already writing)
2025-12-10 11:42:39.674Z debug(replica): 2n: repair_prepare: op=4 checksum=337681397177720383533166008301995067443 (already writing)
2025-12-10 11:42:39.675Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=0)
2025-12-10 11:42:39.676Z debug(client_replies): 1: read_reply: done (client=204864005026426173254654640007015434914 reply=180721629067901931180688463199873763473)
2025-12-10 11:42:39.676Z debug(replica): 1N: on_request: repeat reply (client=204864005026426173254654640007015434914 request=5)
2025-12-10 11:42:39.676Z debug(replica): 1N: sending reply to client 204864005026426173254654640007015434914: vsr.message_header.Header.Reply{ .checksum = 180721629067901931180688463199873763473, .checksum_padding = 0, .checksum_body = 262991487050001328813960342374172054376, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 544896, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 276407604296675853898847308413824879213, .request_checksum_padding = 0, .context = 280577675950734272305822183788388095003, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 7, .commit = 7, .timestamp = 1765366956846578035, .request = 5, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.679Z warning(replica): 2n: on_messages: message count=27 suspended=0
2025-12-10 11:42:39.680Z debug(replica): 2n: execute_op: executing view=1 primary=false op=1 checksum=18081821241008343383634529347251656618 (pulse)
2025-12-10 11:42:39.680Z debug(replica): 2n: execute_op: commit_timestamp=0 prepare.header.timestamp=1765366948714034019
2025-12-10 11:42:39.690Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-10 11:42:39.690Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-10 11:42:39.704Z debug(forest): entering forest.compact() op=1 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-10 11:42:39.705Z warning(replica): 2n: commit_dispatch: slow request, request=0 size=256 pulse time=573ms
2025-12-10 11:42:39.705Z debug(replica): 2n: commit_start_journal: cached prepare op=2 checksum=180712438674514203771522049436898328741
2025-12-10 11:42:39.705Z debug(replica): 2n: execute_op: executing view=1 primary=false op=2 checksum=180712438674514203771522049436898328741 (register)
2025-12-10 11:42:39.705Z debug(replica): 2n: execute_op: commit_timestamp=1765366948714034019 prepare.header.timestamp=1765366956539996796
2025-12-10 11:42:39.705Z debug(replica): 2n: client_table_entry_create: write (client=204864005026426173254654640007015434914 session=2 request=0)
2025-12-10 11:42:39.705Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 117248789350883589178573648186078772863, .checksum_padding = 0, .checksum_body = 33380509746930834472700756938017657525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 320, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 175618757085245920522970776582928013076, .request_checksum_padding = 0, .context = 218038287841370738680574724088127301579, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 2, .commit = 2, .timestamp = 1765366956539996796, .request = 0, .operation = vsr.Operation.register, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.705Z debug(replica): 2n: sending reply to client 204864005026426173254654640007015434914: vsr.message_header.Header.Reply{ .checksum = 117248789350883589178573648186078772863, .checksum_padding = 0, .checksum_body = 33380509746930834472700756938017657525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 320, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 175618757085245920522970776582928013076, .request_checksum_padding = 0, .context = 218038287841370738680574724088127301579, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 2, .commit = 2, .timestamp = 1765366956539996796, .request = 0, .operation = vsr.Operation.register, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.705Z debug(forest): entering forest.compact() op=2 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-10 11:42:39.705Z debug(replica): 2n: commit_start_journal: cached prepare op=3 checksum=145628026037401329440524624120816611215
2025-12-10 11:42:39.705Z debug(replica): 2n: execute_op: executing view=1 primary=false op=3 checksum=145628026037401329440524624120816611215 (create_accounts)
2025-12-10 11:42:39.705Z debug(replica): 2n: execute_op: commit_timestamp=1765366956539996796 prepare.header.timestamp=1765366956541199482
2025-12-10 11:42:39.705Z debug(replica): 2n: client_table_entry_update: client=204864005026426173254654640007015434914 session=2 request=1
2025-12-10 11:42:39.705Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 300685265307446621979237549281768755859, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 334940521279488267999903271169790815749, .request_checksum_padding = 0, .context = 197907746165096050132981129292562572328, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 3, .commit = 3, .timestamp = 1765366956541199482, .request = 1, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.705Z debug(replica): 2n: sending reply to client 204864005026426173254654640007015434914: vsr.message_header.Header.Reply{ .checksum = 300685265307446621979237549281768755859, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 334940521279488267999903271169790815749, .request_checksum_padding = 0, .context = 197907746165096050132981129292562572328, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 3, .commit = 3, .timestamp = 1765366956541199482, .request = 1, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.706Z debug(forest): entering forest.compact() op=3 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-10 11:42:39.707Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.707Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-12-10 11:42:39.707Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.710Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 326907196772165731887244222878255458002, .checksum_padding = 0, .checksum_body = 149919202242495968050047197511312137527, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 549376, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 337681397177720383533166008301995067443, .parent_padding = 0, .request_checksum = 309837793894732803633338992281101510276, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 5, .commit = 4, .timestamp = 1765366956715562840, .request = 3, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:39.710Z debug(replica): 2n: on_prepare: advancing commit_max=3..4
2025-12-10 11:42:39.710Z debug(replica): 2n: on_prepare: caching prepare.op=5 (commit_min=3 op=4 commit_max=4 prepare_max=1007)
2025-12-10 11:42:39.710Z debug(replica): 2n: on_prepare: advancing: op=4..5 checksum=337681397177720383533166008301995067443..326907196772165731887244222878255458002
2025-12-10 11:42:39.710Z debug(journal): 2: set_header_as_dirty: op=5 checksum=326907196772165731887244222878255458002
2025-12-10 11:42:39.710Z debug(replica): 2n: append: appending to journal op=5
2025-12-10 11:42:39.710Z debug(journal): 2: write: view=1 slot=5 op=5 len=549376: 326907196772165731887244222878255458002 starting
2025-12-10 11:42:39.710Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=5242880 len=552960 locked
2025-12-10 11:42:39.710Z debug(replica): 2n: commit_start_journal: cached prepare op=4 checksum=337681397177720383533166008301995067443
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=4 checksum=337681397177720383533166008301995067443 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=5 checksum=326907196772165731887244222878255458002 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=1 checksum=18081821241008343383634529347251656618 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=2 checksum=180712438674514203771522049436898328741 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=3 checksum=145628026037401329440524624120816611215 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=3)
2025-12-10 11:42:39.710Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-10 11:42:39.710Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-10 11:42:39.710Z debug(vsr): 1: journal_repair_timeout fired
2025-12-10 11:42:39.710Z debug(vsr): 1: journal_repair_timeout reset
2025-12-10 11:42:39.710Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 290231585333059907760770543796980987607, .checksum_padding = 0, .checksum_body = 116644293290483624186003533138370079655, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 944, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 326907196772165731887244222878255458002, .parent_padding = 0, .request_checksum = 192731837790586772831793888025568735385, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 6, .commit = 5, .timestamp = 1765366956806574220, .request = 4, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:39.710Z debug(replica): 2n: on_prepare: advancing commit_max=4..5
2025-12-10 11:42:39.710Z debug(replica): 2n: on_prepare: caching prepare.op=6 (commit_min=3 op=5 commit_max=5 prepare_max=1007)
2025-12-10 11:42:39.710Z debug(replica): 2n: on_prepare: advancing: op=5..6 checksum=326907196772165731887244222878255458002..290231585333059907760770543796980987607
2025-12-10 11:42:39.710Z debug(journal): 2: set_header_as_dirty: op=6 checksum=290231585333059907760770543796980987607
2025-12-10 11:42:39.710Z debug(replica): 2n: append: appending to journal op=6
2025-12-10 11:42:39.710Z debug(journal): 2: write: view=1 slot=6 op=6 len=944: 290231585333059907760770543796980987607 starting
2025-12-10 11:42:39.710Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=6291456 len=4096 locked
2025-12-10 11:42:39.710Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=3)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=4 checksum=337681397177720383533166008301995067443 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=5 checksum=326907196772165731887244222878255458002 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=6 checksum=290231585333059907760770543796980987607 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=1 checksum=18081821241008343383634529347251656618 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=2 checksum=180712438674514203771522049436898328741 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: repair_prepare: op=3 checksum=145628026037401329440524624120816611215 (already writing)
2025-12-10 11:42:39.710Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=3)
2025-12-10 11:42:39.710Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 249849249301660541160381485282880325928, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 290231585333059907760770543796980987607, .parent_padding = 0, .request_checksum = 276407604296675853898847308413824879213, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 7, .commit = 6, .timestamp = 1765366956846578035, .request = 5, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:39.710Z debug(replica): 2n: on_prepare: advancing commit_max=5..6
2025-12-10 11:42:39.710Z debug(replica): 2n: on_prepare: caching prepare.op=7 (commit_min=3 op=6 commit_max=6 prepare_max=1007)
2025-12-10 11:42:39.710Z debug(replica): 2n: on_prepare: advancing: op=6..7 checksum=290231585333059907760770543796980987607..249849249301660541160381485282880325928
2025-12-10 11:42:39.710Z debug(journal): 2: set_header_as_dirty: op=7 checksum=249849249301660541160381485282880325928
2025-12-10 11:42:39.710Z debug(replica): 2n: append: appending to journal op=7
2025-12-10 11:42:39.710Z debug(journal): 2: write: view=1 slot=7 op=7 len=68336: 249849249301660541160381485282880325928 starting
2025-12-10 11:42:39.710Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=7340032 len=69632 locked
2025-12-10 11:42:39.710Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=3)
2025-12-10 11:42:39.711Z debug(replica): 2n: repair_prepare: op=4 checksum=337681397177720383533166008301995067443 (already writing)
2025-12-10 11:42:39.711Z debug(replica): 2n: repair_prepare: op=5 checksum=326907196772165731887244222878255458002 (already writing)
2025-12-10 11:42:39.711Z debug(replica): 2n: repair_prepare: op=6 checksum=290231585333059907760770543796980987607 (already writing)
2025-12-10 11:42:39.711Z debug(replica): 2n: repair_prepare: op=7 checksum=249849249301660541160381485282880325928 (already writing)
2025-12-10 11:42:39.711Z debug(replica): 2n: repair_prepare: op=1 checksum=18081821241008343383634529347251656618 (already writing)
2025-12-10 11:42:39.711Z debug(replica): 2n: repair_prepare: op=2 checksum=180712438674514203771522049436898328741 (already writing)
2025-12-10 11:42:39.711Z debug(replica): 2n: repair_prepare: op=3 checksum=145628026037401329440524624120816611215 (already writing)
2025-12-10 11:42:39.711Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=3)
2025-12-10 11:42:39.711Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 118526858455970754248641182297999044136, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 290231585333059907760770543796980987607, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 6, .timestamp_monotonic = 16850819589965163, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.711Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-10 11:42:39.711Z debug(replica): 2n: on_commit: checksum verified
2025-12-10 11:42:39.711Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=3)
2025-12-10 11:42:39.711Z debug(superblock): 2: view_change: write_header: checksum=7b32c8d46df0002da29939ed96939def sequence=3 copy=1 size=8192 offset=24576
2025-12-10 11:42:39.711Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=1048576 len=4096 unlocked
2025-12-10 11:42:39.711Z debug(journal): 2: write_header: op=1 sectors[0..4096]
2025-12-10 11:42:39.711Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-12-10 11:42:39.711Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=2097152 len=4096 unlocked
2025-12-10 11:42:39.711Z debug(journal): 2: write_header: op=2 sectors[0..4096]
2025-12-10 11:42:39.711Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=4194304 len=4096 unlocked
2025-12-10 11:42:39.711Z debug(journal): 2: write_header: op=4 sectors[0..4096]
2025-12-10 11:42:39.711Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=3145728 len=8192 unlocked
2025-12-10 11:42:39.711Z debug(journal): 2: write_header: op=3 sectors[0..4096]
2025-12-10 11:42:39.711Z debug(client_replies): 2: write_reply: wrote (client=204864005026426173254654640007015434914 request=0)
2025-12-10 11:42:39.711Z debug(replica): 2n: execute_op: executing view=1 primary=false op=4 checksum=337681397177720383533166008301995067443 (lookup_accounts)
2025-12-10 11:42:39.711Z debug(replica): 2n: execute_op: commit_timestamp=1765366956541199482 prepare.header.timestamp=1765366956543406687
2025-12-10 11:42:39.711Z debug(replica): 2n: client_table_entry_update: client=204864005026426173254654640007015434914 session=2 request=2
2025-12-10 11:42:39.711Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 261615738737887125599469659211839118923, .checksum_padding = 0, .checksum_body = 2911310325919007288616327726406574390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 5760, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 176270417088940460935617877678172533171, .request_checksum_padding = 0, .context = 244553827911628431185079770697769545493, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 4, .commit = 4, .timestamp = 1765366956543406687, .request = 2, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.711Z debug(replica): 2n: sending reply to client 204864005026426173254654640007015434914: vsr.message_header.Header.Reply{ .checksum = 261615738737887125599469659211839118923, .checksum_padding = 0, .checksum_body = 2911310325919007288616327726406574390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 5760, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 176270417088940460935617877678172533171, .request_checksum_padding = 0, .context = 244553827911628431185079770697769545493, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 4, .commit = 4, .timestamp = 1765366956543406687, .request = 2, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.711Z debug(forest): entering forest.compact() op=4 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-10 11:42:39.712Z debug(replica): 2n: commit_start_journal: cached prepare op=5 checksum=326907196772165731887244222878255458002
2025-12-10 11:42:39.715Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:39.715Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:39.715Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=6291456 len=4096 unlocked
2025-12-10 11:42:39.715Z debug(journal): 2: write_header: op=6 sectors[0..4096]
2025-12-10 11:42:39.715Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=5242880 len=552960 unlocked
2025-12-10 11:42:39.715Z debug(journal): 2: write_header: op=5 sectors[0..4096]
2025-12-10 11:42:39.715Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=7340032 len=69632 unlocked
2025-12-10 11:42:39.715Z debug(journal): 2: write_header: op=7 sectors[0..4096]
2025-12-10 11:42:39.715Z debug(superblock): 2: view_change: write_header: checksum=7b32c8d46df0002da29939ed96939def sequence=3 copy=2 size=8192 offset=49152
2025-12-10 11:42:39.715Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-12-10 11:42:39.715Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-12-10 11:42:39.715Z debug(journal): 2: write: view=1 slot=1 op=1 len=256: 18081821241008343383634529347251656618 complete, marking clean
2025-12-10 11:42:39.715Z debug(replica): 2n: send_prepare_ok: op=1 checksum=18081821241008343383634529347251656618
2025-12-10 11:42:39.715Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 152572554039500926519232514257022534232, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 108034676951432761169128540124443993015, .parent_padding = 0, .prepare_checksum = 18081821241008343383634529347251656618, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 0, .op = 1, .commit_min = 4, .timestamp = 1765366948714034019, .request = 0, .operation = vsr.Operation.pulse, .reserved = { 0, 0, 0 } }
2025-12-10 11:42:39.715Z debug(replica): 2n: send_message_to_replica: dropped prepare_ok (log_view_durable=0 log_view=1)
2025-12-10 11:42:39.715Z debug(client_replies): 2: write_reply: wrote (client=204864005026426173254654640007015434914 request=1)
2025-12-10 11:42:39.715Z debug(replica): 2n: execute_op: executing view=1 primary=false op=5 checksum=326907196772165731887244222878255458002 (create_transfers)
2025-12-10 11:42:39.715Z debug(replica): 2n: execute_op: commit_timestamp=1765366956543406687 prepare.header.timestamp=1765366956715562840
2025-12-10 11:42:39.730Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-10 11:42:39.730Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-10 11:42:39.730Z debug(vsr): 1: pulse_timeout fired
2025-12-10 11:42:39.730Z debug(vsr): 1: pulse_timeout reset
2025-12-10 11:42:39.743Z debug(replica): 2n: client_table_entry_update: client=204864005026426173254654640007015434914 session=2 request=3
2025-12-10 11:42:39.743Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 304696753562283721645328911122741241331, .checksum_padding = 0, .checksum_body = 50662820692233810949477498502740345627, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 544, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 309837793894732803633338992281101510276, .request_checksum_padding = 0, .context = 96411276156427999208461718944991637041, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 5, .commit = 5, .timestamp = 1765366956715562840, .request = 3, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.743Z debug(replica): 2n: sending reply to client 204864005026426173254654640007015434914: vsr.message_header.Header.Reply{ .checksum = 304696753562283721645328911122741241331, .checksum_padding = 0, .checksum_body = 50662820692233810949477498502740345627, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 544, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 309837793894732803633338992281101510276, .request_checksum_padding = 0, .context = 96411276156427999208461718944991637041, .context_padding = 0, .client = 204864005026426173254654640007015434914, .op = 5, .commit = 5, .timestamp = 1765366956715562840, .request = 3, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:39.743Z debug(forest): entering forest.compact() op=5 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-10 11:42:39.750Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-10 11:42:39.750Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-10 11:42:39.760Z debug(replica): 2n: commit_start_journal: cached prepare op=6 checksum=290231585333059907760770543796980987607
2025-12-10 11:42:39.760Z debug(superblock): 2: view_change: write_header: checksum=7b32c8d46df0002da29939ed96939def sequence=3 copy=3 size=8192 offset=73728
2025-12-10 11:42:39.760Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-12-10 11:42:39.760Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-12-10 11:42:39.760Z debug(journal): 2: write: view=1 slot=2 op=2 len=512: 180712438674514203771522049436898328741 complete, marking clean
2025-12-10 11:42:39.760Z debug(replica): 2n: send_prepare_ok: op=2 checksum=180712438674514203771522049436898328741
2025-12-10 11:42:39.760Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 135017302831148278198595943418418694409, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 18081821241008343383634529347251656618, .parent_padding = 0, .prepare_checksum = 180712438674514203771522049436898328741, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 2, .commit_min = 5, .timestamp = 1765366956539996796, .request = 0, .operation = vsr.Operation.register, .reserved = { 0, 0, 0 } }
2025-12-10 11:42:39.760Z debug(replica): 2n: send_message_to_replica: dropped prepare_ok (log_view_durable=0 log_view=1)
warning(client): 204864005026426173254654640007015434914: on_reply: slow request, request=5 op=7 size=68336 lookup_transfers time=3122ms
2025-12-10 11:42:39.770Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-10 11:42:40.173Z info(supervisor): 1: terminating replica
2025-12-10 11:42:40.633Z debug(client_replies): 2: write_reply: wrote (client=204864005026426173254654640007015434914 request=2)
2025-12-10 11:42:40.633Z debug(replica): 2n: execute_op: executing view=1 primary=false op=6 checksum=290231585333059907760770543796980987607 (lookup_accounts)
2025-12-10 11:42:40.633Z debug(replica): 2n: execute_op: commit_timestamp=1765366956715562840 prepare.header.timestamp=1765366956806574220
2025-12-10 11:42:40.633Z debug(replica): 2n: client_table_entry_update: client=204864005026426173254654640007015434914 session=2 request=4
2025-12-10 11:42:40.633Z debug(forest): entering forest.compact() op=6 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-10 11:42:40.633Z debug(superblock): 2: view_change: read_header: copy=0 size=8192 offset=0
2025-12-10 11:42:40.634Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-12-10 11:42:40.634Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-12-10 11:42:40.634Z debug(journal): 2: write: view=1 slot=4 op=4 len=944: 337681397177720383533166008301995067443 complete, marking clean
2025-12-10 11:42:40.634Z debug(replica): 2n: send_prepare_ok: op=4 checksum=337681397177720383533166008301995067443
2025-12-10 11:42:40.634Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 22270903673704707888739127564400586999, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 145628026037401329440524624120816611215, .parent_padding = 0, .prepare_checksum = 337681397177720383533166008301995067443, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 4, .commit_min = 6, .timestamp = 1765366956543406687, .request = 2, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:40.634Z debug(replica): 2n: send_message_to_replica: dropped prepare_ok (log_view_durable=0 log_view=1)
2025-12-10 11:42:40.634Z warning(replica): 2n: commit_dispatch: slow request, request=4 size=944 lookup_accounts time=874ms
2025-12-10 11:42:40.634Z debug(client_replies): 2: write_reply: wrote (client=204864005026426173254654640007015434914 request=3)
2025-12-10 11:42:40.634Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-12-10 11:42:40.634Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-12-10 11:42:40.634Z debug(journal): 2: write: view=1 slot=3 op=3 len=5760: 145628026037401329440524624120816611215 complete, marking clean
2025-12-10 11:42:40.634Z debug(replica): 2n: send_prepare_ok: op=3 checksum=145628026037401329440524624120816611215
2025-12-10 11:42:40.634Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 204211365972816057738147194618027109712, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 180712438674514203771522049436898328741, .parent_padding = 0, .prepare_checksum = 145628026037401329440524624120816611215, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 3, .commit_min = 6, .timestamp = 1765366956541199482, .request = 1, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:40.634Z debug(replica): 2n: send_message_to_replica: dropped prepare_ok (log_view_durable=0 log_view=1)
2025-12-10 11:42:40.634Z debug(superblock): 2: view_change: read_header: copy=1 size=8192 offset=24576
2025-12-10 11:42:40.635Z debug(client_replies): 2: write_reply: wrote (client=204864005026426173254654640007015434914 request=4)
2025-12-10 11:42:40.635Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-12-10 11:42:40.635Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-12-10 11:42:40.635Z debug(journal): 2: write: view=1 slot=6 op=6 len=944: 290231585333059907760770543796980987607 complete, marking clean
2025-12-10 11:42:40.635Z debug(replica): 2n: send_prepare_ok: op=6 checksum=290231585333059907760770543796980987607
2025-12-10 11:42:40.635Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 240120844774595889923395473379287683389, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 326907196772165731887244222878255458002, .parent_padding = 0, .prepare_checksum = 290231585333059907760770543796980987607, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 6, .commit_min = 6, .timestamp = 1765366956806574220, .request = 4, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:40.635Z debug(replica): 2n: send_message_to_replica: dropped prepare_ok (log_view_durable=0 log_view=1)
2025-12-10 11:42:40.635Z debug(superblock): 2: view_change: read_header: copy=2 size=8192 offset=49152
2025-12-10 11:42:40.635Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-12-10 11:42:40.635Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-12-10 11:42:40.635Z debug(journal): 2: write: view=1 slot=5 op=5 len=549376: 326907196772165731887244222878255458002 complete, marking clean
2025-12-10 11:42:40.635Z debug(replica): 2n: send_prepare_ok: op=5 checksum=326907196772165731887244222878255458002
2025-12-10 11:42:40.635Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 151570536510065599616412463502836583461, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 337681397177720383533166008301995067443, .parent_padding = 0, .prepare_checksum = 326907196772165731887244222878255458002, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 5, .commit_min = 6, .timestamp = 1765366956715562840, .request = 3, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:40.635Z debug(replica): 2n: send_message_to_replica: dropped prepare_ok (log_view_durable=0 log_view=1)
2025-12-10 11:42:40.635Z debug(superblock): 2: view_change: read_header: copy=3 size=8192 offset=73728
2025-12-10 11:42:40.635Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-12-10 11:42:40.635Z debug(journal): 2: write: view=1 slot=7 op=7 len=68336: 249849249301660541160381485282880325928 complete, marking clean
2025-12-10 11:42:40.635Z debug(replica): 2n: send_prepare_ok: op=7 checksum=249849249301660541160381485282880325928
2025-12-10 11:42:40.635Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 315632243738522627772625263425115137297, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 290231585333059907760770543796980987607, .parent_padding = 0, .prepare_checksum = 249849249301660541160381485282880325928, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 7, .commit_min = 6, .timestamp = 1765366956846578035, .request = 5, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:40.635Z debug(replica): 2n: send_message_to_replica: dropped prepare_ok (log_view_durable=0 log_view=1)
2025-12-10 11:42:40.635Z debug(superblock_quorums): copy: 0/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-10 11:42:40.635Z debug(superblock_quorums): copy: 1/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-10 11:42:40.636Z debug(superblock_quorums): copy: 2/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-10 11:42:40.636Z debug(superblock_quorums): copy: 3/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-10 11:42:40.636Z debug(superblock_quorums): quorum: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3 count=4 valid=true
2025-12-10 11:42:40.636Z debug(superblock): 2: view_change: installed working superblock: checksum=7b32c8d46df0002da29939ed96939def sequence=3 release=0.0.1 cluster=00000000000000000000000000000000 replica_id=134075807420264837279280775697797238201 size=1141374976 free_set_blocks_acquired_size=0 free_set_blocks_released_size=0 client_sessions_size=0 checkpoint_id=f222e9ce156b309eaeb4af665242ac18 commit_min_checksum=108034676951432761169128540124443993015 commit_min=0 commit_max=0 log_view=1 view=1 sync_op_min=0 sync_op_max=0 manifest_oldest_checksum=0 manifest_oldest_address=0 manifest_newest_checksum=0 manifest_newest_address=0 manifest_block_count=0 snapshots_block_checksum=0 snapshots_block_address=0
2025-12-10 11:42:40.636Z debug(superblock): 2: view_change: vsr_header: op=0 checksum=108034676951432761169128540124443993015
2025-12-10 11:42:40.636Z debug(superblock): 2: view_change: complete
2025-12-10 11:42:40.636Z debug(replica): 2n: view_durable_update_callback: (view_durable=1 log_view_durable=1)
2025-12-10 11:42:40.636Z debug(replica): 2n: send_prepare_ok: op=7 checksum=249849249301660541160381485282880325928
2025-12-10 11:42:40.636Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 315632243738522627772625263425115137297, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 290231585333059907760770543796980987607, .parent_padding = 0, .prepare_checksum = 249849249301660541160381485282880325928, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 7, .commit_min = 6, .timestamp = 1765366956846578035, .request = 5, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:40.642Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.642Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.657Z info(message_bus): 2: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-12-10 11:42:40.657Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.657Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-12-10 11:42:40.657Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.662Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.662Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.667Z info(supervisor): 0: unpausing replica
2025-12-10 11:42:40.667Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 270060309782368307165168412877648657743, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850807227334137, .pong_timestamp_wall = 1765366959116217689, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.667Z debug(clock): 0: learn: m0=16850807227334137 < window.monotonic=16850818604500343
2025-12-10 11:42:40.667Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 186010777246995164761573017148404393518, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850808235388594, .pong_timestamp_wall = 1765366959116460689, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.667Z debug(clock): 0: learn: m0=16850808235388594 < window.monotonic=16850818604500343
2025-12-10 11:42:40.667Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 131184576439597533196990516560702317017, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850812191635725, .pong_timestamp_wall = 1765366959116626450, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.667Z debug(clock): 0: learn: m0=16850812191635725 < window.monotonic=16850818604500343
2025-12-10 11:42:40.667Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 317322137391859433171283117681860196241, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850813196496731, .pong_timestamp_wall = 1765366959116738290, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.667Z debug(clock): 0: learn: m0=16850813196496731 < window.monotonic=16850818604500343
2025-12-10 11:42:40.667Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 272306191919599247036647943798177529569, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850814201044936, .pong_timestamp_wall = 1765366959116849211, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.667Z debug(clock): 0: learn: m0=16850814201044936 < window.monotonic=16850818604500343
2025-12-10 11:42:40.667Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 194829792206230157782301025222970250523, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850815204737018, .pong_timestamp_wall = 1765366959116958371, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.667Z debug(clock): 0: learn: m0=16850815204737018 < window.monotonic=16850818604500343
2025-12-10 11:42:40.667Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 262622685580606016223366171432181345335, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850816208698342, .pong_timestamp_wall = 1765366959117071092, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.667Z debug(clock): 0: learn: m0=16850816208698342 < window.monotonic=16850818604500343
2025-12-10 11:42:40.667Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 298610249424375185995024168520024384843, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850817216965239, .pong_timestamp_wall = 1765366959117179412, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.668Z debug(clock): 0: learn: m0=16850817216965239 < window.monotonic=16850818604500343
2025-12-10 11:42:40.668Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 256273518063284222829478482984619802111, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850818223804812, .pong_timestamp_wall = 1765366959117298252, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.668Z debug(clock): 0: learn: m0=16850818223804812 < window.monotonic=16850818604500343
2025-12-10 11:42:40.668Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.DoViewChange{ .checksum = 322002469577275634469150461777001036937, .checksum_padding = 0, .checksum_body = 338472410126490789317327957580574335997, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.do_view_change, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .present_bitset = 1, .nack_bitset = 0, .op = 0, .commit_min = 0, .checkpoint_op = 0, .log_view = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.668Z debug(replica): 0n: on_do_view_change: ignoring (view started)
2025-12-10 11:42:40.668Z warning(replica): 0n: on_messages: message count=10 suspended=0
2025-12-10 11:42:40.668Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 249849249301660541160381485282880325928, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 290231585333059907760770543796980987607, .parent_padding = 0, .request_checksum = 276407604296675853898847308413824879213, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 7, .commit = 6, .timestamp = 1765366956846578035, .request = 5, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:40.668Z warning(replica): 0n: on_prepare: not replicating op=7 commit_min=6 present=true
2025-12-10 11:42:40.668Z debug(replica): 0n: on_prepare: ignoring (repair)
2025-12-10 11:42:40.668Z debug(replica): 0n: on_repair: ignoring (duplicate)
2025-12-10 11:42:40.668Z debug(replica): 0n: send_prepare_ok: op=7 checksum=249849249301660541160381485282880325928
2025-12-10 11:42:40.668Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 294666355736811721292467104972195766896, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 290231585333059907760770543796980987607, .parent_padding = 0, .prepare_checksum = 249849249301660541160381485282880325928, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 204864005026426173254654640007015434914, .op = 7, .commit_min = 6, .timestamp = 1765366956846578035, .request = 5, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-10 11:42:40.668Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 118526858455970754248641182297999044136, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 290231585333059907760770543796980987607, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 6, .timestamp_monotonic = 16850819589965163, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.668Z debug(vsr): 0: normal_heartbeat_timeout reset
2025-12-10 11:42:40.668Z debug(replica): 0n: on_commit: checksum verified
2025-12-10 11:42:40.668Z info(message_bus): 0: set_and_verify_peer connection from client=204864005026426173254654640007015434914
2025-12-10 11:42:40.668Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.669Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-10 11:42:40.669Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.669Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.669Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-10 11:42:40.669Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 276407604296675853898847308413824879213, .checksum_padding = 0, .checksum_body = 194371026497289333738498280387356287603, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 68336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 86071061575356872955827594346731653060, .parent_padding = 0, .client = 204864005026426173254654640007015434914, .session = 2, .timestamp = 0, .request = 5, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 39085772, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.669Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.PingClient{ .checksum = 30881730860482118781911000227227867210, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 204864005026426173254654640007015434914, .ping_timestamp_monotonic = 16850820215552084, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.669Z debug(replica): 0n: sending pong_client to client 204864005026426173254654640007015434914: vsr.message_header.Header.PongClient{ .checksum = 220625426671319417943681960245292063362, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 16850820215552084, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-10 11:42:40.669Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.669Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.669Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-12-10 11:42:40.677Z info(supervisor): 1: starting replica
2025-12-10 11:42:40.678Z info(supervisor): going into 2m2s quiescence (no faults)
2025-12-10 11:42:40.679Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=68ms
2025-12-10 11:42:40.679Z info(io): opening "0_1.tigerbeetle"...
2025-12-10 11:42:40.683Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.683Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.689Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.689Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.703Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.703Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.706Z info(main): multiversioning: upgrades disabled for development (0.0.1) release.
2025-12-10 11:42:40.706Z info(main): release=0.0.1
2025-12-10 11:42:40.706Z info(main): release_client_min=0.0.1
2025-12-10 11:42:40.706Z info(main): releases_bundled={ 0.0.1 }
2025-12-10 11:42:40.706Z info(main): git_commit=cb2d50e177764f6bbefcd20e02a39a069dab4a7d
2025-12-10 11:42:40.706Z debug(superblock): null: open: started
2025-12-10 11:42:40.706Z debug(superblock): null: open: read_header: copy=0 size=8192 offset=0
2025-12-10 11:42:40.707Z debug(superblock): null: open: read_header: copy=1 size=8192 offset=24576
2025-12-10 11:42:40.707Z debug(superblock): null: open: read_header: copy=2 size=8192 offset=49152
2025-12-10 11:42:40.707Z debug(superblock): null: open: read_header: copy=3 size=8192 offset=73728
2025-12-10 11:42:40.707Z debug(superblock_quorums): copy: 0/4: checksum=990a93b479aab184f8c14fb44defac70 parent=9ab285961fc022daaa440588254fab17 sequence=3
2025-12-10 11:42:40.707Z debug(superblock_quorums): copy: 1/4: checksum=990a93b479aab184f8c14fb44defac70 parent=9ab285961fc022daaa440588254fab17 sequence=3
2025-12-10 11:42:40.707Z debug(superblock_quorums): copy: 2/4: checksum=990a93b479aab184f8c14fb44defac70 parent=9ab285961fc022daaa440588254fab17 sequence=3
2025-12-10 11:42:40.707Z debug(superblock_quorums): copy: 3/4: checksum=990a93b479aab184f8c14fb44defac70 parent=9ab285961fc022daaa440588254fab17 sequence=3
2025-12-10 11:42:40.707Z debug(superblock_quorums): quorum: checksum=990a93b479aab184f8c14fb44defac70 parent=9ab285961fc022daaa440588254fab17 sequence=3 count=4 valid=true
2025-12-10 11:42:40.707Z debug(superblock): null: open: installed working superblock: checksum=990a93b479aab184f8c14fb44defac70 sequence=3 release=0.0.1 cluster=00000000000000000000000000000000 replica_id=308943487097555535311203420603596972560 size=1141374976 free_set_blocks_acquired_size=0 free_set_blocks_released_size=0 client_sessions_size=0 checkpoint_id=f222e9ce156b309eaeb4af665242ac18 commit_min_checksum=108034676951432761169128540124443993015 commit_min=0 commit_max=0 log_view=1 view=1 sync_op_min=0 sync_op_max=0 manifest_oldest_checksum=0 manifest_oldest_address=0 manifest_newest_checksum=0 manifest_newest_address=0 manifest_block_count=0 snapshots_block_checksum=0 snapshots_block_address=0
2025-12-10 11:42:40.707Z debug(superblock): null: open: vsr_header: op=0 checksum=108034676951432761169128540124443993015
2025-12-10 11:42:40.707Z debug(superblock): null: open: complete
2025-12-10 11:42:40.708Z debug(journal): 1: slot_count=1024 size=1.000244140625GiB headers_size=256KiB prepares_size=1GiB
2025-12-10 11:42:40.709Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.709Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.709Z debug(vsr): 0: journal_repair_timeout fired
2025-12-10 11:42:40.709Z debug(vsr): 0: journal_repair_timeout reset
2025-12-10 11:42:40.713Z debug(vsr): 2: journal_repair_timeout fired
2025-12-10 11:42:40.713Z debug(vsr): 2: journal_repair_timeout reset
2025-12-10 11:42:40.723Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.723Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.729Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.729Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.743Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.743Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.747Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-12-10 11:42:40.747Z info(message_bus): 0: on_connect: connected to=1
2025-12-10 11:42:40.747Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-12-10 11:42:40.747Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-12-10 11:42:40.749Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=50ms
2025-12-10 11:42:40.749Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.749Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.753Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-12-10 11:42:40.763Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.763Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.770Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.770Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.783Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.783Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.790Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.790Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.799Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-12-10 11:42:40.799Z info(message_bus): 0: on_connect: connected to=1
2025-12-10 11:42:40.799Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-12-10 11:42:40.799Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-12-10 11:42:40.800Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=75ms
2025-12-10 11:42:40.803Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.803Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.810Z debug(vsr): 0: start_view_change_message_timeout fired
2025-12-10 11:42:40.810Z debug(vsr): 0: start_view_change_message_timeout reset
2025-12-10 11:42:40.810Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.810Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.810Z debug(vsr): 0: journal_repair_timeout fired
2025-12-10 11:42:40.810Z debug(vsr): 0: journal_repair_timeout reset
2025-12-10 11:42:40.810Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-12-10 11:42:40.810Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-12-10 11:42:40.813Z debug(vsr): 2: journal_repair_timeout fired
2025-12-10 11:42:40.813Z debug(vsr): 2: journal_repair_timeout reset
2025-12-10 11:42:40.823Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.823Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.831Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.831Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.839Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-12-10 11:42:40.841Z debug(manifest_log): 1: Manifest.Pace.half_bar_append_blocks_max = 1
2025-12-10 11:42:40.841Z debug(manifest_log): 1: Manifest.Pace.half_bar_compact_blocks_max = 2
2025-12-10 11:42:40.841Z debug(manifest_log): 1: Manifest.Pace.log_blocks_full_max = 586
2025-12-10 11:42:40.841Z debug(manifest_log): 1: Manifest.Pace.log_blocks_cycle_max = 1172
2025-12-10 11:42:40.841Z debug(manifest_log): 1: Manifest.Pace.log_blocks_max = 1466
2025-12-10 11:42:40.841Z debug(manifest_log): 1: Manifest.Pace.tables_max = 2396744
2025-12-10 11:42:40.843Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.843Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.851Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.852Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.863Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.863Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.872Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.872Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.875Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-12-10 11:42:40.875Z info(message_bus): 0: on_connect: connected to=1
2025-12-10 11:42:40.875Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-12-10 11:42:40.876Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-12-10 11:42:40.882Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=51ms
2025-12-10 11:42:40.883Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.883Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.892Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.892Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.900Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-12-10 11:42:40.904Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.904Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.912Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.912Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.912Z debug(vsr): 0: journal_repair_timeout fired
2025-12-10 11:42:40.912Z debug(vsr): 0: journal_repair_timeout reset
2025-12-10 11:42:40.914Z debug(vsr): 2: journal_repair_timeout fired
2025-12-10 11:42:40.914Z debug(vsr): 2: journal_repair_timeout reset
2025-12-10 11:42:40.924Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.924Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.932Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.932Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.933Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-12-10 11:42:40.933Z info(message_bus): 0: on_connect: connected to=1
2025-12-10 11:42:40.933Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-12-10 11:42:40.933Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-12-10 11:42:40.942Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=93ms
2025-12-10 11:42:40.944Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.944Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.952Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.952Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.962Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-12-10 11:42:40.964Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.964Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.972Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:40.972Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:40.972Z debug(vsr): 0: trace_emit_timeout fired
2025-12-10 11:42:40.972Z debug(vsr): 0: trace_emit_timeout reset
2025-12-10 11:42:40.976Z debug(statsd): 0: statsd packet: tb.replica_status:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_view:1|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_log_view:1|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_op:7|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_op_checkpoint:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_commit_min:6|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_commit_max:6|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_pipeline_queue_length:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_sync_stage:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_sync_op_min:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_sync_op_max:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_messages_in:7|g|#cluster:00000000000000000000000000000000,replica:0,command:ping
tb.replica_messages_in:18|g|#cluster:00000000000000000000000000000000,replica:0,command:pong
tb.replica_messages_in:1|g|#cluster:00000000000000000000000000000000,replica:0,command:ping_client
tb.replica_messages_in:19|g|#cluster:00000000000000000000000000000000,replica:0,command:request
tb.replica_messages_in:8|g|#cluster:00000000000000000000000000000000,replica:0,command:prepare

2025-12-10 11:42:40.976Z debug(statsd): 0: statsd packet: tb.replica_messages_in:15|g|#cluster:00000000000000000000000000000000,replica:0,command:commit
tb.replica_messages_in:1|g|#cluster:00000000000000000000000000000000,replica:0,command:do_view_change
tb.replica_messages_in:2|g|#cluster:00000000000000000000000000000000,replica:0,command:start_view
tb.replica_messages_out:18|g|#cluster:00000000000000000000000000000000,replica:0,command:ping
tb.replica_messages_out:7|g|#cluster:00000000000000000000000000000000,replica:0,command:pong
tb.replica_messages_out:1|g|#cluster:00000000000000000000000000000000,replica:0,command:pong_client
tb.replica_messages_out:6|g|#cluster:00000000000000000000000000000000,replica:0,command:request
tb.replica_messages_out:8|g|#cluster:00000000000000000000000000000000,replica:0,command:prepare_ok
tb.replica_messages_out:1|g|#cluster:00000000000000000000000000000000,replica:0,command:reply
tb.replica_messages_out:10|g|#cluster:00000000000000000000000000000000,replica:0,command:do_view_change
tb.replica_messages_out:2|g|#cluster:00000000000000000000000000000000,replica:0,command:request_start_view
tb.journal_dirty:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.journal_faulty:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.grid_blocks_acquired:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.grid_blocks_missing:0|g|#cluster:00000000000000000000000000000000,replica:0

2025-12-10 11:42:40.984Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:40.984Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:40.989Z debug(replica): 1R: init: replica_count=3 quorum_view_change=2 quorum_replication=2 release=0.0.1
2025-12-10 11:42:40.989Z info(replica): superblock release=0.0.1
2025-12-10 11:42:40.989Z debug(journal): 1: recover: recovering
2025-12-10 11:42:40.989Z debug(journal): 1: recover_headers: offset=0 size=262144 recovering
2025-12-10 11:42:40.989Z debug(journal): 1: recover_headers: offset=0 size=262144 recovered
2025-12-10 11:42:40.989Z debug(journal): 1: recover_headers: complete
2025-12-10 11:42:40.989Z debug(journal): 1: recover_prepare: recovering slot=0
2025-12-10 11:42:40.989Z debug(journal): 1: recover_prepare: recovering slot=1
2025-12-10 11:42:40.989Z debug(journal): 1: recover_prepare: recovering slot=2
2025-12-10 11:42:40.989Z debug(journal): 1: recover_prepare: recovering slot=3
2025-12-10 11:42:40.989Z debug(journal): 1: recover_prepare: recovering slot=4
2025-12-10 11:42:40.989Z debug(journal): 1: recover_prepare: recovering slot=5
2025-12-10 11:42:40.989Z debug(journal): 1: recover_prepare: recovering slot=6
thread 1 panic: reached unreachable code
2025-12-10 11:42:40.989Z debug(journal): 1: recover_prepare: recovering slot=7
2025-12-10 11:42:41.004Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:41.417Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:41.418Z debug(journal): 1: recover_prepare: recovering slot=8
2025-12-10 11:42:41.418Z debug(journal): 1: recover_prepare: recovering slot=9
2025-12-10 11:42:41.418Z debug(journal): 1: recover_prepare: recovering slot=10
2025-12-10 11:42:41.418Z debug(journal): 1: recover_prepare: recovering slot=11
2025-12-10 11:42:41.418Z debug(journal): 1: recover_prepare: recovering slot=12
2025-12-10 11:42:41.418Z debug(journal): 1: recover_prepare: recovering slot=13
2025-12-10 11:42:41.418Z debug(journal): 1: recover_prepare: recovering slot=14
2025-12-10 11:42:41.420Z debug(journal): 1: recover_prepare: recovering slot=15
2025-12-10 11:42:41.420Z debug(journal): 1: recover_prepare: recovering slot=16
2025-12-10 11:42:41.420Z debug(journal): 1: recover_prepare: recovering slot=17
2025-12-10 11:42:41.420Z debug(journal): 1: recover_prepare: recovering slot=18
2025-12-10 11:42:41.420Z debug(journal): 1: recover_prepare: recovering slot=19
2025-12-10 11:42:41.420Z debug(journal): 1: recover_prepare: recovering slot=20
2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=21
2025-12-10 11:42:40.976Z debug(statsd): 0: statsd packet: tb.grid_cache_hits:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.grid_cache_misses:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.lsm_nodes_free:8192|g|#cluster:00000000000000000000000000000000,replica:0
tb.lsm_manifest_block_count:0|g|#cluster:00000000000000000000000000000000,replica:0
tb.release:1|g|#cluster:00000000000000000000000000000000,replica:0
tb.replica_commit_us.min:1|g|#cluster:00000000000000000000000000000000,replica:0,stage:prefetch
tb.replica_commit_us.max:147|g|#cluster:00000000000000000000000000000000,replica:0,stage:prefetch
tb.replica_commit_us.avg:491|g|#cluster:00000000000000000000000000000000,replica:0,stage:prefetch
tb.replica_commit_us.sum:2949|c|#cluster:00000000000000000000000000000000,replica:0,stage:prefetch
tb.replica_commit_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,stage:prefetch
tb.replica_commit_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,stage:stall
tb.replica_commit_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,stage:stall
tb.replica_commit_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,stage:stall
tb.replica_commit_us.sum:4|c|#cluster:00000000000000000000000000000000,replica:0,stage:stall
tb.replica_commit_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,stage:stall

2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=22
2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.replica_commit_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,stage:reply_setup
tb.replica_commit_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,stage:reply_setup
tb.replica_commit_us.avg:1|g|#cluster:00000000000000000000000000000000,replica:0,stage:reply_setup
tb.replica_commit_us.sum:9|c|#cluster:00000000000000000000000000000000,replica:0,stage:reply_setup
tb.replica_commit_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,stage:reply_setup
tb.replica_commit_us.min:61|g|#cluster:00000000000000000000000000000000,replica:0,stage:execute
tb.replica_commit_us.max:161|g|#cluster:00000000000000000000000000000000,replica:0,stage:execute
tb.replica_commit_us.avg:3553|g|#cluster:00000000000000000000000000000000,replica:0,stage:execute
tb.replica_commit_us.sum:21322|c|#cluster:00000000000000000000000000000000,replica:0,stage:execute
tb.replica_commit_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,stage:execute
tb.replica_commit_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_durable
tb.replica_commit_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_durable
tb.replica_commit_us.avg:1456|g|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_durable

/root/tigerbeetle/zig/lib/std/debug.zig2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.replica_commit_us.sum:8737|c|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_durable
tb.replica_commit_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_durable
tb.replica_commit_us.min:191|g|#cluster:00000000000000000000000000000000,replica:0,stage:compact
tb.replica_commit_us.max:577950|g|#cluster:00000000000000000000000000000000,replica:0,stage:compact
tb.replica_commit_us.avg:98880|g|#cluster:00000000000000000000000000000000,replica:0,stage:compact
tb.replica_commit_us.sum:593283|c|#cluster:00000000000000000000000000000000,replica:0,stage:compact
tb.replica_commit_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,stage:compact
tb.replica_commit_us.min:1|g|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_data
tb.replica_commit_us.max:2|g|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_data
tb.replica_commit_us.avg:2|g|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_data
tb.replica_commit_us.sum:13|c|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_data
tb.replica_commit_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_data
tb.replica_commit_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_superblock

2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=23
2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.replica_commit_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_superblock
tb.replica_commit_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_superblock
tb.replica_commit_us.sum:4|c|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_superblock
tb.replica_commit_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,stage:checkpoint_superblock
tb.replica_request_us.min:761992|g|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_us.max:761992|g|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_us.avg:761992|g|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_us.sum:761992|c|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_us.min:163135|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts
tb.replica_request_us.max:163135|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts
tb.replica_request_us.avg:163135|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.replica_request_us.sum:163135|c|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts
tb.replica_request_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts
tb.replica_request_us.min:128537|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_us.max:128537|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_us.avg:128537|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_us.sum:128537|c|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_us.min:178569|g|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_us.max:619247|g|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_us.avg:398908|g|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_us.sum:797817|c|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_us.count:2|c|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.replica_request_execute_us.min:41247|g|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_execute_us.max:41247|g|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_execute_us.avg:41247|g|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_execute_us.sum:41247|c|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_execute_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_local_us.min:9202|g|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_local_us.max:9202|g|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_local_us.avg:9202|g|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_local_us.sum:9202|c|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_local_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,operation:pulse
tb.replica_request_local_us.min:812|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts
tb.replica_request_local_us.max:812|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.replica_request_local_us.avg:812|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts
tb.replica_request_local_us.sum:812|c|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts
tb.replica_request_local_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,operation:create_accounts
tb.replica_request_local_us.min:37259|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_local_us.max:37259|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_local_us.avg:37259|g|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_local_us.sum:37259|c|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_local_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,operation:create_transfers
tb.replica_request_local_us.min:566|g|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_local_us.max:578272|g|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_local_us.avg:289419|g|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts

:2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.replica_request_local_us.sum:578839|c|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.replica_request_local_us.count:2|c|#cluster:00000000000000000000000000000000,replica:0,operation:lookup_accounts
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.id
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.id
tb.compact_mutable_suffix_us.avg:32|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.id
tb.compact_mutable_suffix_us.sum:193|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.id
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.id
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_128
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_128
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_128
tb.compact_mutable_suffix_us.sum:4|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_128
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_128

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_64
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_64
tb.compact_mutable_suffix_us.avg:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_64
tb.compact_mutable_suffix_us.sum:7|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_64
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_64
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_32
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_32
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_32
tb.compact_mutable_suffix_us.sum:2|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_32
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.user_data_32
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.ledger
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.ledger

550:14: 0x126197b in assert (vortex)
2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.avg:6|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.ledger
tb.compact_mutable_suffix_us.sum:40|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.ledger
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.ledger
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.code
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.code
tb.compact_mutable_suffix_us.avg:3|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.code
tb.compact_mutable_suffix_us.sum:20|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.code
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.code
tb.compact_mutable_suffix_us.min:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.compact_mutable_suffix_us.avg:218|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.compact_mutable_suffix_us.sum:1308|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.imported
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.imported
tb.compact_mutable_suffix_us.avg:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.imported
tb.compact_mutable_suffix_us.sum:7|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.imported
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.imported
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.closed
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.closed
tb.compact_mutable_suffix_us.avg:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.closed
tb.compact_mutable_suffix_us.sum:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.closed
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.closed
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.id

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.id
tb.compact_mutable_suffix_us.avg:121|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.id
tb.compact_mutable_suffix_us.sum:727|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.id
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.id
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.debit_account_id
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.debit_account_id
tb.compact_mutable_suffix_us.avg:457|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.debit_account_id
tb.compact_mutable_suffix_us.sum:2743|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.debit_account_id
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.debit_account_id
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.credit_account_id
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.credit_account_id

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.avg:452|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.credit_account_id
tb.compact_mutable_suffix_us.sum:2717|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.credit_account_id
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.credit_account_id
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.amount
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.amount
tb.compact_mutable_suffix_us.avg:264|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.amount
tb.compact_mutable_suffix_us.sum:1586|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.amount
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.amount
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.pending_id
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.pending_id
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.pending_id
tb.compact_mutable_suffix_us.sum:2|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.pending_id

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.pending_id
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_128
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_128
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_128
tb.compact_mutable_suffix_us.sum:1|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_128
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_128
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_64
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_64
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_64
tb.compact_mutable_suffix_us.sum:4|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_64
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_64

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_32
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_32
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_32
tb.compact_mutable_suffix_us.sum:1|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_32
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.user_data_32
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.ledger
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.ledger
tb.compact_mutable_suffix_us.avg:129|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.ledger
tb.compact_mutable_suffix_us.sum:776|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.ledger
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.ledger
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.code
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.code

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.avg:140|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.code
tb.compact_mutable_suffix_us.sum:840|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.code
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.code
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.compact_mutable_suffix_us.avg:139|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.compact_mutable_suffix_us.sum:838|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.compact_mutable_suffix_us.sum:3|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.imported
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.imported
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.imported
tb.compact_mutable_suffix_us.sum:2|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.imported
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.imported
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.closing
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.closing
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.closing
tb.compact_mutable_suffix_us.sum:3|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.closing
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.closing
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.compact_mutable_suffix_us.avg:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.compact_mutable_suffix_us.sum:7|c|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.status
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.status
tb.compact_mutable_suffix_us.avg:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.status
tb.compact_mutable_suffix_us.sum:7|c|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.status
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.status
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.timestamp
tb.compact_mutable_suffix_us.max:2|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.timestamp

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.avg:244|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.timestamp
tb.compact_mutable_suffix_us.sum:1467|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.timestamp
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.timestamp
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.account_timestamp
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.account_timestamp
tb.compact_mutable_suffix_us.avg:17|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.account_timestamp
tb.compact_mutable_suffix_us.sum:103|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.account_timestamp
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.account_timestamp
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_status
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_status
tb.compact_mutable_suffix_us.avg:128|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_status

    if (!ok) unreachable; // assertion failure
             2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.sum:768|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_status
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_status
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.dr_account_id_expired
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.dr_account_id_expired
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.dr_account_id_expired
tb.compact_mutable_suffix_us.sum:2|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.dr_account_id_expired
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.dr_account_id_expired
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.cr_account_id_expired
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.cr_account_id_expired
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.cr_account_id_expired

^
2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.sum:1|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.cr_account_id_expired
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.cr_account_id_expired
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_id_expired
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_id_expired
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_id_expired
tb.compact_mutable_suffix_us.sum:1|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_id_expired
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.transfer_pending_id_expired
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.ledger_expired
tb.compact_mutable_suffix_us.max:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.ledger_expired
tb.compact_mutable_suffix_us.avg:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.ledger_expired

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.compact_mutable_suffix_us.sum:2|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.ledger_expired
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.ledger_expired
tb.compact_mutable_suffix_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.prunable
tb.compact_mutable_suffix_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.prunable
tb.compact_mutable_suffix_us.avg:72|g|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.prunable
tb.compact_mutable_suffix_us.sum:434|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.prunable
tb.compact_mutable_suffix_us.count:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:AccountEvents.prunable
tb.lookup_us.min:4|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.lookup_us.max:100|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.lookup_us.avg:92|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.lookup_us.sum:371|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.lookup_us.count:4|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp

2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=24
2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.lookup_us.min:6|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_us.max:98|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_us.avg:52|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_us.sum:105|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_us.count:2|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_us.min:5|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.lookup_us.max:5|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.lookup_us.avg:5|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.lookup_us.sum:5|c|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.lookup_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.lookup_worker_us.min:0|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.lookup_worker_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.lookup_worker_us.avg:2|g|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.lookup_worker_us.sum:9|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.lookup_worker_us.count:4|c|#cluster:00000000000000000000000000000000,replica:0,tree:Account.timestamp
tb.lookup_worker_us.min:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_worker_us.max:4|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_worker_us.avg:3|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_worker_us.sum:6|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_worker_us.count:2|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.timestamp
tb.lookup_worker_us.min:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.lookup_worker_us.max:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.lookup_worker_us.avg:1|g|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.lookup_worker_us.sum:1|c|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.lookup_worker_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,tree:TransferPending.timestamp
tb.scan_tree_us.min:60|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.scan_tree_us.max:60|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.scan_tree_us.avg:60|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.scan_tree_us.sum:60|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.scan_tree_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.scan_tree_level_us.min:43|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.scan_tree_level_us.max:44|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.scan_tree_level_us.avg:44|g|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.scan_tree_level_us.sum:314|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.scan_tree_level_us.count:7|c|#cluster:00000000000000000000000000000000,replica:0,tree:Transfer.expires_at
tb.storage_read_us.min:17|g|#cluster:00000000000000000000000000000000,replica:0,zone:superblock
tb.storage_read_us.max:17|g|#cluster:00000000000000000000000000000000,replica:0,zone:superblock
tb.storage_read_us.avg:43|g|#cluster:00000000000000000000000000000000,replica:0,zone:superblock
tb.storage_read_us.sum:525|c|#cluster:00000000000000000000000000000000,replica:0,zone:superblock

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.storage_read_us.count:12|c|#cluster:00000000000000000000000000000000,replica:0,zone:superblock
tb.storage_read_us.min:256|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_read_us.max:256|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_read_us.avg:256|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_read_us.sum:256|c|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_read_us.count:1|c|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_read_us.min:490|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_read_us.max:569|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_read_us.avg:38557|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_read_us.sum:39483268|c|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_read_us.count:1024|c|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_write_us.min:22|g|#cluster:00000000000000000000000000000000,replica:0,zone:superblock
tb.storage_write_us.max:32|g|#cluster:00000000000000000000000000000000,replica:0,zone:superblock
tb.storage_write_us.avg:317349|g|#cluster:00000000000000000000000000000000,replica:0,zone:superblock

2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.storage_write_us.sum:2538793|c|#cluster:00000000000000000000000000000000,replica:0,zone:superblock
tb.storage_write_us.count:8|c|#cluster:00000000000000000000000000000000,replica:0,zone:superblock
tb.storage_write_us.min:29|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_write_us.max:250|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_write_us.avg:129|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_write_us.sum:905|c|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_write_us.count:7|c|#cluster:00000000000000000000000000000000,replica:0,zone:wal_headers
tb.storage_write_us.min:156|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_write_us.max:579233|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_write_us.avg:88730|g|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_write_us.sum:621110|c|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_write_us.count:7|c|#cluster:00000000000000000000000000000000,replica:0,zone:wal_prepares
tb.storage_write_us.min:498|g|#cluster:00000000000000000000000000000000,replica:0,zone:client_replies

2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=25
2025-12-10 11:42:41.421Z debug(statsd): 0: statsd packet: tb.storage_write_us.max:578349|g|#cluster:00000000000000000000000000000000,replica:0,zone:client_replies
tb.storage_write_us.avg:119234|g|#cluster:00000000000000000000000000000000,replica:0,zone:client_replies
tb.storage_write_us.sum:596173|c|#cluster:00000000000000000000000000000000,replica:0,zone:client_replies
tb.storage_write_us.count:5|c|#cluster:00000000000000000000000000000000,replica:0,zone:client_replies

/root/tigerbeetle/working/release/src/testing/vortex/faulty_network.zig:88:15: 0x13ac7a7 in recv (vortex)
2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=26
        assert(!pipe.recv_inflight);
              2025-12-10 11:42:41.421Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
^
2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=27
/root/tigerbeetle/working/release/src/testing/vortex/faulty_network.zig:233:22: 0x13af827 in on_send (vortex)
2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=28
            pipe.recv();
                     ^
2025-12-10 11:42:41.421Z info(message_bus): 0: on_connect: connected to=1
/root/tigerbeetle/working/release/src/io/linux.zig:1856:25: 0x13af2bf in erased (vortex)
2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=29
                callback(ctx, completion, result.*);
                        ^
2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=30
/root/tigerbeetle/working/release/src/io/linux.zig:738:40: 0x1311823 in complete (vortex)
2025-12-10 11:42:41.421Z debug(journal): 1: recover_prepare: recovering slot=31
                    completion.callback(completion.context, completion, &result);
                                       ^
/root/tigerbeetle/working/release/src/io/linux.zig:194:49: 0x130fe6b in flush (vortex)
                .inactive => completion.complete(),
                                                ^
/root/tigerbeetle/working/release/src/io/linux.zig:149:27: 0x1312253 in run_for_ns (vortex)
            try self.flush(1, &timeouts, &etime);
                          ^
2025-12-10 11:42:41.422Z debug(journal): 1: recover_prepare: recovering slot=32
/root/tigerbeetle/working/release/src/testing/vortex/supervisor.zig:263:41: 0x1312c6f in run (vortex)
2025-12-10 11:42:41.422Z debug(journal): 1: recover_prepare: recovering slot=33
            try supervisor.io.run_for_ns(constants.vsr.tick_ms * std.time.ns_per_ms);
                                        ^
2025-12-10 11:42:41.422Z debug(journal): 1: recover_prepare: recovering slot=34
/root/tigerbeetle/working/release/src/testing/vortex/supervisor.zig2025-12-10 11:42:41.422Z debug(journal): 1: recover_prepare: recovering slot=35
2025-12-10 11:42:41.427Z debug(vsr): 2: journal_repair_timeout fired
2025-12-10 11:42:41.441Z debug(vsr): 0: journal_repair_budget_timeout fired
:2025-12-10 11:42:42.429Z debug(vsr): 2: journal_repair_timeout reset
2072025-12-10 11:42:42.429Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:42.429Z debug(journal): 1: recover_prepare: recovering slot=36
:23: 0x1316dc3 in main (vortex)
    try supervisor.run();
                      ^
/root/tigerbeetle/working/release/src/vortex.zig:61:61: 0x1328a13 in main (vortex)
        .supervisor => |supervisor_args| try Supervisor.main(allocator, supervisor_args),
                                                            ^
2025-12-10 11:42:42.429Z debug(journal): 1: recover_prepare: recovering slot=37
/root/tigerbeetle/zig/lib/std/start.zig:660:37: 0x132940f in main (vortex)
2025-12-10 11:42:42.429Z debug(journal): 1: recover_prepare: recovering slot=38
            const result = root.main() catch |err| {
                                    ^
2025-12-10 11:42:42.430Z debug(journal): 1: recover_prepare: recovering slot=39
2025-12-10 11:42:42.430Z debug(journal): 1: recover_prepare: recovering slot=40
2025-12-10 11:42:42.430Z debug(journal): 1: recover_prepare: recovering slot=41
2025-12-10 11:42:42.430Z debug(journal): 1: recover_prepare: recovering slot=42
/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c:95:7: 0x1546b67 in libc_start_main_stage2 (/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c)
2025-12-10 11:42:42.430Z debug(journal): 1: recover_prepare: recovering slot=43
 exit(main(argc, argv, envp));
      ^
2025-12-10 11:42:42.430Z debug(journal): 1: recover_prepare: recovering slot=44
2025-12-10 11:42:42.430Z debug(journal): 1: recover_prepare: recovering slot=45
2025-12-10 11:42:42.430Z debug(journal): 1: recover_prepare: recovering slot=46
2025-12-10 11:42:42.430Z debug(journal): 1: recover_prepare: recovering slot=47
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=48
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=49
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=50
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=51
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=52
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=53
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=54
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=55
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=56
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=57
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=58
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=59
2025-12-10 11:42:42.431Z debug(journal): 1: recover_prepare: recovering slot=60
2025-12-10 11:42:42.432Z debug(journal): 1: recover_prepare: recovering slot=61
2025-12-10 11:42:42.432Z debug(journal): 1: recover_prepare: recovering slot=62
2025-12-10 11:42:42.432Z debug(journal): 1: recover_prepare: recovering slot=63
2025-12-10 11:42:42.432Z debug(journal): 1: recover_prepare: recovering slot=64
2025-12-10 11:42:42.432Z debug(journal): 1: recover_prepare: recovering slot=65
2025-12-10 11:42:42.432Z debug(journal): 1: recover_prepare: recovering slot=66
2025-12-10 11:42:42.432Z debug(journal): 1: recover_prepare: recovering slot=67
2025-12-10 11:42:42.432Z debug(journal): 1: recover_prepare: recovering slot=68
2025-12-10 11:42:42.432Z debug(journal): 1: recover_prepare: recovering slot=69
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=70
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=71
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=72
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=73
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=74
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=75
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=76
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=77
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=78
2025-12-10 11:42:42.433Z debug(journal): 1: recover_prepare: recovering slot=79
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=80
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=81
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=82
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=83
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=84
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=85
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=86
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=87
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=88
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=89
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=90
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=91
2025-12-10 11:42:42.434Z debug(journal): 1: recover_prepare: recovering slot=92
2025-12-10 11:42:42.435Z debug(journal): 1: recover_prepare: recovering slot=93
2025-12-10 11:42:42.435Z debug(journal): 1: recover_prepare: recovering slot=94
2025-12-10 11:42:42.435Z debug(journal): 1: recover_prepare: recovering slot=95
2025-12-10 11:42:42.435Z debug(journal): 1: recover_prepare: recovering slot=96
2025-12-10 11:42:42.435Z debug(journal): 1: recover_prepare: recovering slot=97
2025-12-10 11:42:42.435Z debug(journal): 1: recover_prepare: recovering slot=98
2025-12-10 11:42:42.435Z debug(journal): 1: recover_prepare: recovering slot=99
2025-12-10 11:42:42.435Z debug(journal): 1: recover_prepare: recovering slot=100
2025-12-10 11:42:42.435Z debug(journal): 1: recover_prepare: recovering slot=101
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=102
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=103
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=104
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=105
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=106
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=107
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=108
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=109
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=110
2025-12-10 11:42:42.436Z debug(journal): 1: recover_prepare: recovering slot=111
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=112
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=113
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=114
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=115
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=116
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=117
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=118
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=119
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=120
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=121
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=122
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=123
2025-12-10 11:42:42.437Z debug(journal): 1: recover_prepare: recovering slot=124
2025-12-10 11:42:42.438Z debug(journal): 1: recover_prepare: recovering slot=125
2025-12-10 11:42:42.438Z debug(journal): 1: recover_prepare: recovering slot=126
2025-12-10 11:42:42.438Z debug(journal): 1: recover_prepare: recovering slot=127
2025-12-10 11:42:42.438Z debug(journal): 1: recover_prepare: recovering slot=128
2025-12-10 11:42:42.438Z debug(journal): 1: recover_prepare: recovering slot=129
2025-12-10 11:42:42.438Z debug(journal): 1: recover_prepare: recovering slot=130
2025-12-10 11:42:42.438Z debug(journal): 1: recover_prepare: recovering slot=131
2025-12-10 11:42:42.438Z debug(journal): 1: recover_prepare: recovering slot=132
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=133
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=134
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=135
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=136
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=137
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=138
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=139
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=140
2025-12-10 11:42:42.439Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:42.439Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=141
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=142
2025-12-10 11:42:42.439Z debug(journal): 1: recover_prepare: recovering slot=143
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=144
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=145
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=146
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=147
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=148
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=149
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=150
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=151
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=152
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=153
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=154
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=155
2025-12-10 11:42:42.440Z debug(journal): 1: recover_prepare: recovering slot=156
2025-12-10 11:42:42.441Z debug(journal): 1: recover_prepare: recovering slot=157
2025-12-10 11:42:42.441Z debug(journal): 1: recover_prepare: recovering slot=158
2025-12-10 11:42:42.441Z debug(journal): 1: recover_prepare: recovering slot=159
2025-12-10 11:42:42.441Z debug(journal): 1: recover_prepare: recovering slot=160
2025-12-10 11:42:42.441Z debug(journal): 1: recover_prepare: recovering slot=161
2025-12-10 11:42:42.441Z debug(journal): 1: recover_prepare: recovering slot=162
2025-12-10 11:42:42.441Z debug(journal): 1: recover_prepare: recovering slot=163
2025-12-10 11:42:42.441Z debug(journal): 1: recover_prepare: recovering slot=164
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=165
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=166
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=167
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=168
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=169
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=170
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=171
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=172
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=173
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=174
2025-12-10 11:42:42.442Z debug(journal): 1: recover_prepare: recovering slot=175
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=176
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=177
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=178
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=179
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=180
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=181
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=182
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=183
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=184
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=185
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=186
2025-12-10 11:42:42.443Z debug(journal): 1: recover_prepare: recovering slot=187
2025-12-10 11:42:42.444Z debug(journal): 1: recover_prepare: recovering slot=188
2025-12-10 11:42:42.444Z debug(journal): 1: recover_prepare: recovering slot=189
2025-12-10 11:42:42.444Z debug(journal): 1: recover_prepare: recovering slot=190
2025-12-10 11:42:42.444Z debug(journal): 1: recover_prepare: recovering slot=191
2025-12-10 11:42:42.444Z debug(journal): 1: recover_prepare: recovering slot=192
2025-12-10 11:42:42.444Z debug(journal): 1: recover_prepare: recovering slot=193
2025-12-10 11:42:42.444Z debug(journal): 1: recover_prepare: recovering slot=194
2025-12-10 11:42:42.444Z debug(journal): 1: recover_prepare: recovering slot=195
2025-12-10 11:42:42.444Z debug(journal): 1: recover_prepare: recovering slot=196
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=197
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=198
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=199
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=200
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=201
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=202
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=203
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=204
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=205
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=206
2025-12-10 11:42:42.445Z debug(journal): 1: recover_prepare: recovering slot=207
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=208
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=209
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=210
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=211
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=212
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=213
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=214
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=215
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=216
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=217
2025-12-10 11:42:42.446Z debug(journal): 1: recover_prepare: recovering slot=218
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=219
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=220
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=221
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=222
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=223
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=224
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=225
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=226
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=227
2025-12-10 11:42:42.447Z debug(journal): 1: recover_prepare: recovering slot=228
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=229
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=230
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=231
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=232
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=233
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=234
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=235
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=236
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=237
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=238
2025-12-10 11:42:42.448Z debug(journal): 1: recover_prepare: recovering slot=239
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=240
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=241
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=242
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=243
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=244
2025-12-10 11:42:42.449Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:42.449Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:42.449Z debug(vsr): 0: journal_repair_timeout fired
2025-12-10 11:42:42.449Z debug(vsr): 0: journal_repair_timeout reset
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=245
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=246
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=247
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=248
2025-12-10 11:42:42.449Z debug(journal): 1: recover_prepare: recovering slot=249
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=250
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=251
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=252
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=253
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=254
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=255
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=256
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=257
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=258
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=259
2025-12-10 11:42:42.450Z debug(journal): 1: recover_prepare: recovering slot=260
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=261
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=262
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=263
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=264
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=265
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=266
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=267
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=268
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=269
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=270
2025-12-10 11:42:42.451Z debug(journal): 1: recover_prepare: recovering slot=271
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=272
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=273
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=274
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=275
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=276
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=277
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=278
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=279
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=280
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=281
2025-12-10 11:42:42.452Z debug(journal): 1: recover_prepare: recovering slot=282
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=283
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=284
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=285
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=286
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=287
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=288
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=289
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=290
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=291
2025-12-10 11:42:42.453Z debug(journal): 1: recover_prepare: recovering slot=292
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=293
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=294
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=295
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=296
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=297
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=298
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=299
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=300
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=301
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=302
2025-12-10 11:42:42.454Z debug(journal): 1: recover_prepare: recovering slot=303
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=304
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=305
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=306
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=307
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=308
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=309
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=310
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=311
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=312
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=313
2025-12-10 11:42:42.455Z debug(journal): 1: recover_prepare: recovering slot=314
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=315
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=316
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=317
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=318
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=319
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=320
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=321
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=322
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=323
2025-12-10 11:42:42.456Z debug(journal): 1: recover_prepare: recovering slot=324
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=325
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=326
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=327
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=328
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=329
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=330
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=331
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=332
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=333
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=334
2025-12-10 11:42:42.457Z debug(journal): 1: recover_prepare: recovering slot=335
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=336
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=337
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=338
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=339
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=340
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=341
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=342
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=343
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=344
2025-12-10 11:42:42.458Z debug(journal): 1: recover_prepare: recovering slot=345
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=346
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=347
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=348
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=349
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=350
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=351
2025-12-10 11:42:42.459Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:42.459Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=352
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=353
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=354
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=355
2025-12-10 11:42:42.459Z debug(journal): 1: recover_prepare: recovering slot=356
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=357
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=358
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=359
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=360
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=361
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=362
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=363
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=364
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=365
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=366
2025-12-10 11:42:42.460Z debug(journal): 1: recover_prepare: recovering slot=367
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=368
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=369
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=370
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=371
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=372
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=373
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=374
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=375
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=376
2025-12-10 11:42:42.461Z debug(journal): 1: recover_prepare: recovering slot=377
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=378
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=379
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=380
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=381
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=382
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=383
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=384
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=385
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=386
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=387
2025-12-10 11:42:42.462Z debug(journal): 1: recover_prepare: recovering slot=388
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=389
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=390
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=391
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=392
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=393
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=394
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=395
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=396
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=397
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=398
2025-12-10 11:42:42.463Z debug(journal): 1: recover_prepare: recovering slot=399
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=400
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=401
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=402
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=403
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=404
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=405
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=406
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=407
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=408
2025-12-10 11:42:42.464Z debug(journal): 1: recover_prepare: recovering slot=409
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=410
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=411
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=412
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=413
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=414
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=415
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=416
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=417
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=418
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=419
2025-12-10 11:42:42.465Z debug(journal): 1: recover_prepare: recovering slot=420
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=421
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=422
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=423
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=424
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=425
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=426
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=427
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=428
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=429
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=430
2025-12-10 11:42:42.466Z debug(journal): 1: recover_prepare: recovering slot=431
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=432
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=433
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=434
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=435
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=436
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=437
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=438
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=439
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=440
2025-12-10 11:42:42.467Z debug(journal): 1: recover_prepare: recovering slot=441
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=442
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=443
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=444
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=445
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=446
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=447
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=448
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=449
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=450
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=451
2025-12-10 11:42:42.468Z debug(journal): 1: recover_prepare: recovering slot=452
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=453
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=454
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=455
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=456
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=457
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=458
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=459
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=460
2025-12-10 11:42:42.469Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-10 11:42:42.469Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=461
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=462
2025-12-10 11:42:42.469Z debug(journal): 1: recover_prepare: recovering slot=463
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=464
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=465
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=466
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=467
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=468
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=469
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=470
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=471
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=472
2025-12-10 11:42:42.470Z debug(journal): 1: recover_prepare: recovering slot=473
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=474
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=475
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=476
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=477
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=478
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=479
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=480
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=481
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=482
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=483
2025-12-10 11:42:42.471Z debug(journal): 1: recover_prepare: recovering slot=484
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=485
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=486
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=487
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=488
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=489
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=490
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=491
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=492
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=493
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=494
2025-12-10 11:42:42.472Z debug(journal): 1: recover_prepare: recovering slot=495
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=496
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=497
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=498
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=499
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=500
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=501
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=502
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=503
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=504
2025-12-10 11:42:42.473Z debug(journal): 1: recover_prepare: recovering slot=505
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=506
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=507
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=508
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=509
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=510
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=511
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=512
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=513
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=514
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=515
2025-12-10 11:42:42.474Z debug(journal): 1: recover_prepare: recovering slot=516
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=517
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=518
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=519
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=520
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=521
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=522
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=523
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=524
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=525
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=526
2025-12-10 11:42:42.475Z debug(journal): 1: recover_prepare: recovering slot=527
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=528
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=529
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=530
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=531
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=532
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=533
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=534
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=535
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=536
2025-12-10 11:42:42.476Z debug(journal): 1: recover_prepare: recovering slot=537
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=538
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=539
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=540
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=541
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=542
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=543
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=544
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=545
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=546
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=547
2025-12-10 11:42:42.477Z debug(journal): 1: recover_prepare: recovering slot=548
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=549
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=550
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=551
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=552
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=553
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=554
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=555
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=556
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=557
2025-12-10 11:42:42.478Z debug(journal): 1: recover_prepare: recovering slot=558
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=559
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=560
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=561
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=562
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=563
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=564
2025-12-10 11:42:42.479Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-10 11:42:42.479Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=565
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=566
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=567
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=568
2025-12-10 11:42:42.479Z debug(journal): 1: recover_prepare: recovering slot=569
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=570
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=571
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=572
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=573
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=574
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=575
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=576
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=577
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=578
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=579
2025-12-10 11:42:42.480Z debug(journal): 1: recover_prepare: recovering slot=580
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=581
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=582
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=583
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=584
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=585
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=586
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=587
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=588
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=589
2025-12-10 11:42:42.481Z debug(journal): 1: recover_prepare: recovering slot=590
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=591
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=592
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=593
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=594
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=595
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=596
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=597
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=598
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=599
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=600
2025-12-10 11:42:42.482Z debug(journal): 1: recover_prepare: recovering slot=601
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=602
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=603
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=604
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=605
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=606
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=607
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=608
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=609
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=610
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=611
2025-12-10 11:42:42.483Z debug(journal): 1: recover_prepare: recovering slot=612
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=613
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=614
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=615
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=616
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=617
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=618
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=619
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=620
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=621
2025-12-10 11:42:42.484Z debug(journal): 1: recover_prepare: recovering slot=622
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=623
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=624
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=625
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=626
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=627
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=628
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=629
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=630
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=631
2025-12-10 11:42:42.485Z debug(journal): 1: recover_prepare: recovering slot=632
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=633
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=634
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=635
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=636
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=637
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=638
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=639
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=640
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=641
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=642
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=643
2025-12-10 11:42:42.486Z debug(journal): 1: recover_prepare: recovering slot=644
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=645
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=646
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=647
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=648
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=649
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=650
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=651
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=652
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=653
2025-12-10 11:42:42.487Z debug(journal): 1: recover_prepare: recovering slot=654
2025-12-10 11:42:42.488Z debug(journal): 1: recover_prepare: recovering slot=655
2025-12-10 11:42:42.488Z debug(journal): 1: recover_prepare: recovering slot=656
2025-12-10 11:42:42.488Z debug(journal): 1: recover_prepare: recovering slot=657
2025-12-10 11:42:42.488Z debug(journal): 1: recover_prepare: recovering slot=658
2025-12-10 11:42:42.488Z debug(journal): 1: recover_prepare: recovering slot=659
2025-12-10 11:42:42.488Z debug(journal): 1: recover_prepare: recovering slot=660
2025-12-10 11:42:42.488Z debug(journal): 1: recover_prepare: recovering slot=661
2025-12-10 11:42:42.488Z debug(journal): 1: recover_prepare: recovering slot=662
2025-12-10 11:42:42.790Z info(unshare): sandboxed subprocesses exited with signal 11
