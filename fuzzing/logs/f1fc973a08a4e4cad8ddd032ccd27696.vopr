 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 316304874928711341619523639200027098988, .request_checksum_padding = 0, .context = 182627805709047186645454263464548736298, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 38, .commit = 38, .timestamp = 1765907127692915903, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:27.738Z debug(replica): 1N: sending reply to client 166665032907361157923918867912184359346: vsr.message_header.Header.Reply{ .checksum = 300765613555105084677363863277100035141, .checksum_padding = 0, .checksum_body = 186845314263036731535799113604972350619, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 9472, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 316304874928711341619523639200027098988, .request_checksum_padding = 0, .context = 182627805709047186645454263464548736298, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 38, .commit = 38, .timestamp = 1765907127692915903, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:27.738Z debug(forest): entering forest.compact() op=38 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:27.738Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 133169360988519056735023381331867147666, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 285058150302687635978780165248602918325, .parent_padding = 0, .prepare_checksum = 154002014584620810325586016426229830949, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 38, .commit_min = 37, .timestamp = 1765907127692915903, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:27.738Z debug(replica): 1N: on_prepare_ok: not preparing op=38 checksum=154002014584620810325586016426229830949
2025-12-16 17:45:27.739Z debug(client_replies): 1: write_reply: wrote (client=166665032907361157923918867912184359346 request=36)
2025-12-16 17:45:27.739Z info(workload): accounts created = 71, transfers = 17586, pending transfers = 0, commands run = 18
2025-12-16 17:45:27.743Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:27.743Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:27.750Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:27.750Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:27.750Z debug(replica): 1N: primary_pipeline_prepare: request checksum=240064173979705808641051079925877545461 client=166665032907361157923918867912184359346
2025-12-16 17:45:27.750Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:27.751Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-12-16 17:45:27.751Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:27.751Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:27.751Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:27.754Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=31010712456673645062526977245849357208 op=39
2025-12-16 17:45:27.754Z debug(vsr): 1: prepare_timeout started
2025-12-16 17:45:27.754Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-16 17:45:27.754Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:27.754Z debug(replica): 1N: replicate: replicating op=39 to replica 0
2025-12-16 17:45:27.758Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:27.758Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:27.771Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:27.754Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 31010712456673645062526977245849357208, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .request_checksum = 240064173979705808641051079925877545461, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:27.778Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:27.771Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:30.344Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:30.345Z debug(replica): 1N: replicate: replicating op=39 to replica 2
2025-12-16 17:45:30.345Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 31010712456673645062526977245849357208, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .request_checksum = 240064173979705808641051079925877545461, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.345Z debug(replica): 1N: on_prepare: advancing: op=38..39 checksum=154002014584620810325586016426229830949..31010712456673645062526977245849357208
2025-12-16 17:45:30.345Z debug(journal): 1: set_header_as_dirty: op=39 checksum=31010712456673645062526977245849357208
2025-12-16 17:45:30.345Z debug(replica): 1N: append: appending to journal op=39
2025-12-16 17:45:30.345Z debug(journal): 1: write: view=1 slot=39 op=39 len=700288: 31010712456673645062526977245849357208 starting
2025-12-16 17:45:30.345Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=700416 locked
2025-12-16 17:45:30.348Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.348Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-12-16 17:45:30.348Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.348Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-16 17:45:30.348Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.348Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.348Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.348Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:30.348Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:30.351Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 31010712456673645062526977245849357208, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .request_checksum = 240064173979705808641051079925877545461, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.351Z debug(replica): 0n: on_prepare: advancing commit_max=37..38
2025-12-16 17:45:30.351Z debug(replica): 0n: on_prepare: caching prepare.op=39 (commit_min=37 op=38 commit_max=38 prepare_max=1007)
2025-12-16 17:45:30.351Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 31010712456673645062526977245849357208, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .request_checksum = 240064173979705808641051079925877545461, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.351Z debug(replica): 2n: on_prepare: advancing commit_max=37..38
2025-12-16 17:45:30.351Z debug(replica): 0n: on_prepare: advancing: op=38..39 checksum=154002014584620810325586016426229830949..31010712456673645062526977245849357208
2025-12-16 17:45:30.351Z debug(replica): 2n: on_prepare: caching prepare.op=39 (commit_min=37 op=38 commit_max=38 prepare_max=1007)
2025-12-16 17:45:30.351Z debug(journal): 0: set_header_as_dirty: op=39 checksum=31010712456673645062526977245849357208
2025-12-16 17:45:30.351Z debug(replica): 0n: append: appending to journal op=39
2025-12-16 17:45:30.351Z debug(journal): 0: write: view=1 slot=39 op=39 len=700288: 31010712456673645062526977245849357208 starting
2025-12-16 17:45:30.351Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=700416 locked
2025-12-16 17:45:30.351Z debug(replica): 0n: commit_start_journal: cached prepare op=38 checksum=154002014584620810325586016426229830949
2025-12-16 17:45:30.351Z debug(replica): 2n: on_prepare: advancing: op=38..39 checksum=154002014584620810325586016426229830949..31010712456673645062526977245849357208
2025-12-16 17:45:30.351Z debug(journal): 2: set_header_as_dirty: op=39 checksum=31010712456673645062526977245849357208
2025-12-16 17:45:30.351Z debug(replica): 2n: append: appending to journal op=39
2025-12-16 17:45:30.351Z debug(journal): 2: write: view=1 slot=39 op=39 len=700288: 31010712456673645062526977245849357208 starting
2025-12-16 17:45:30.351Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=700416 locked
2025-12-16 17:45:30.351Z debug(replica): 2n: commit_start_journal: cached prepare op=38 checksum=154002014584620810325586016426229830949
2025-12-16 17:45:30.351Z debug(replica): 0n: repair_prepare: op=39 checksum=31010712456673645062526977245849357208 (already writing)
2025-12-16 17:45:30.351Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=37)
2025-12-16 17:45:30.351Z debug(replica): 2n: repair_prepare: op=39 checksum=31010712456673645062526977245849357208 (already writing)
2025-12-16 17:45:30.351Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.351Z debug(replica): 0n: execute_op: executing view=1 primary=false op=38 checksum=154002014584620810325586016426229830949 (lookup_accounts)
2025-12-16 17:45:30.351Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:30.351Z debug(replica): 0n: execute_op: commit_timestamp=1765907124696394364 prepare.header.timestamp=1765907127692915903
2025-12-16 17:45:30.351Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=37)
2025-12-16 17:45:30.351Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:30.352Z debug(replica): 2n: execute_op: executing view=1 primary=false op=38 checksum=154002014584620810325586016426229830949 (lookup_accounts)
2025-12-16 17:45:30.352Z debug(replica): 2n: execute_op: commit_timestamp=1765907124696394364 prepare.header.timestamp=1765907127692915903
2025-12-16 17:45:30.352Z debug(replica): 0n: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=36
2025-12-16 17:45:30.352Z debug(replica): 2n: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=36
2025-12-16 17:45:30.352Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 300765613555105084677363863277100035141, .checksum_padding = 0, .checksum_body = 186845314263036731535799113604972350619, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 9472, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 316304874928711341619523639200027098988, .request_checksum_padding = 0, .context = 182627805709047186645454263464548736298, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 38, .commit = 38, .timestamp = 1765907127692915903, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.352Z debug(forest): entering forest.compact() op=38 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:30.352Z debug(replica): 0n: sending reply to client 166665032907361157923918867912184359346: vsr.message_header.Header.Reply{ .checksum = 300765613555105084677363863277100035141, .checksum_padding = 0, .checksum_body = 186845314263036731535799113604972350619, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 9472, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 316304874928711341619523639200027098988, .request_checksum_padding = 0, .context = 182627805709047186645454263464548736298, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 38, .commit = 38, .timestamp = 1765907127692915903, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.352Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=700416 unlocked
2025-12-16 17:45:30.352Z debug(journal): 1: write_header: op=39 sectors[8192..12288]
2025-12-16 17:45:30.352Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:30.352Z debug(forest): entering forest.compact() op=38 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:30.352Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:30.352Z debug(journal): 1: write: view=1 slot=39 op=39 len=700288: 31010712456673645062526977245849357208 complete, marking clean
2025-12-16 17:45:30.352Z debug(replica): 1N: send_prepare_ok: op=39 checksum=31010712456673645062526977245849357208
2025-12-16 17:45:30.352Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 75742188195380043912781926482206033645, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .prepare_checksum = 31010712456673645062526977245849357208, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit_min = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.352Z debug(client_replies): 2: write_reply: wrote (client=166665032907361157923918867912184359346 request=36)
2025-12-16 17:45:30.352Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 75742188195380043912781926482206033645, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .prepare_checksum = 31010712456673645062526977245849357208, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit_min = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.352Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-16 17:45:30.352Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-16 17:45:30.352Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-16 17:45:30.352Z debug(client_replies): 0: write_reply: wrote (client=166665032907361157923918867912184359346 request=36)
2025-12-16 17:45:30.352Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=700416 unlocked
2025-12-16 17:45:30.352Z debug(journal): 2: write_header: op=39 sectors[8192..12288]
2025-12-16 17:45:30.352Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:30.352Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:30.352Z debug(journal): 2: write: view=1 slot=39 op=39 len=700288: 31010712456673645062526977245849357208 complete, marking clean
2025-12-16 17:45:30.352Z debug(replica): 2n: send_prepare_ok: op=39 checksum=31010712456673645062526977245849357208
2025-12-16 17:45:30.352Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 335475065479027402471034063427730136528, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .prepare_checksum = 31010712456673645062526977245849357208, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit_min = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.353Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=700416 unlocked
2025-12-16 17:45:30.353Z debug(journal): 0: write_header: op=39 sectors[8192..12288]
2025-12-16 17:45:30.353Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:30.353Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:30.353Z debug(journal): 0: write: view=1 slot=39 op=39 len=700288: 31010712456673645062526977245849357208 complete, marking clean
2025-12-16 17:45:30.353Z debug(replica): 0n: send_prepare_ok: op=39 checksum=31010712456673645062526977245849357208
2025-12-16 17:45:30.353Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 282977407791084966476803266418824199949, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .prepare_checksum = 31010712456673645062526977245849357208, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit_min = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.355Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.355Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:30.355Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:30.358Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.358Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:30.358Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:30.361Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 240064173979705808641051079925877545461, .checksum_padding = 0, .checksum_body = 22518253732913944279865949644731004587, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 700288, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 182627805709047186645454263464548736298, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 49507280, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.361Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:30.361Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:30.361Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 282977407791084966476803266418824199949, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .prepare_checksum = 31010712456673645062526977245849357208, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit_min = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.361Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-16 17:45:30.361Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-12-16 17:45:30.361Z debug(replica): 1N: on_prepare_ok: quorum received, context=31010712456673645062526977245849357208
2025-12-16 17:45:30.361Z debug(vsr): 1: prepare_timeout stopped
2025-12-16 17:45:30.361Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-12-16 17:45:30.363Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 335475065479027402471034063427730136528, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 154002014584620810325586016426229830949, .parent_padding = 0, .prepare_checksum = 31010712456673645062526977245849357208, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit_min = 38, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.363Z debug(replica): 1N: on_prepare_ok: 3 message(s)
2025-12-16 17:45:30.363Z debug(replica): 1N: on_prepare_ok: ignoring (quorum received already)
2025-12-16 17:45:30.364Z debug(replica): 1N: execute_op: executing view=1 primary=true op=39 checksum=31010712456673645062526977245849357208 (create_transfers)
2025-12-16 17:45:30.364Z debug(replica): 1N: execute_op: commit_timestamp=1765907127692915903 prepare.header.timestamp=1765907127750853234
2025-12-16 17:45:30.364Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:30.364Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:30.364Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:30.364Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:30.374Z debug(vsr): 2: journal_repair_timeout fired
2025-12-16 17:45:30.374Z debug(vsr): 2: journal_repair_timeout reset
2025-12-16 17:45:30.384Z debug(replica): 1N: execute_op: advancing commit_max=38..39
2025-12-16 17:45:30.384Z debug(replica): 1N: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=37
2025-12-16 17:45:30.384Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 6054803367465460167716837285540741624, .checksum_padding = 0, .checksum_body = 86846841502176747884143737702274868606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 296, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 240064173979705808641051079925877545461, .request_checksum_padding = 0, .context = 41371991270414184121217006489328048574, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit = 39, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.384Z debug(replica): 1N: sending reply to client 166665032907361157923918867912184359346: vsr.message_header.Header.Reply{ .checksum = 6054803367465460167716837285540741624, .checksum_padding = 0, .checksum_body = 86846841502176747884143737702274868606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 296, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 240064173979705808641051079925877545461, .request_checksum_padding = 0, .context = 41371991270414184121217006489328048574, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit = 39, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.384Z debug(forest): entering forest.compact() op=39 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 166665032907361157923918867912184359346: on_reply: slow request, request=37 op=39 size=700288 create_transfers time=2641ms
2025-12-16 17:45:30.384Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:30.384Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:30.385Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:30.385Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:30.394Z debug(client_replies): 1: write_reply: wrote (client=166665032907361157923918867912184359346 request=37)
2025-12-16 17:45:30.404Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:30.404Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:30.404Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:30.404Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:30.404Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 250314740824772360166326884854870837127, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 41371991270414184121217006489328048574, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 38, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2641463330, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.404Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:30.404Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 250314740824772360166326884854870837127, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 41371991270414184121217006489328048574, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 38, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2641463330, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.404Z debug(replica): 1N: primary_pipeline_prepare: request checksum=250314740824772360166326884854870837127 client=166665032907361157923918867912184359346
2025-12-16 17:45:30.404Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-12-16 17:45:30.404Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 250314740824772360166326884854870837127, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 41371991270414184121217006489328048574, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 38, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2641463330, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.404Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=321020414413213519586207344965930768827 op=40
2025-12-16 17:45:30.404Z debug(vsr): 1: prepare_timeout started
2025-12-16 17:45:30.404Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-16 17:45:30.404Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:30.404Z debug(replica): 1N: replicate: replicating op=40 to replica 0
2025-12-16 17:45:30.404Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 321020414413213519586207344965930768827, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .request_checksum = 250314740824772360166326884854870837127, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.404Z debug(replica): 1N: replicate: replicating op=40 to replica 2
2025-12-16 17:45:30.404Z debug(vsr): 0: start_view_change_message_timeout fired
2025-12-16 17:45:30.404Z debug(vsr): 0: start_view_change_message_timeout reset
2025-12-16 17:45:30.404Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:30.404Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 321020414413213519586207344965930768827, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .request_checksum = 250314740824772360166326884854870837127, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.404Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:30.405Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:30.405Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:30.405Z debug(replica): 1N: on_prepare: advancing: op=39..40 checksum=31010712456673645062526977245849357208..321020414413213519586207344965930768827
2025-12-16 17:45:30.405Z debug(journal): 1: set_header_as_dirty: op=40 checksum=321020414413213519586207344965930768827
2025-12-16 17:45:30.405Z debug(replica): 1N: append: appending to journal op=40
2025-12-16 17:45:30.405Z debug(journal): 1: write: view=1 slot=40 op=40 len=1408: 321020414413213519586207344965930768827 starting
2025-12-16 17:45:30.405Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 locked
2025-12-16 17:45:30.405Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 321020414413213519586207344965930768827, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .request_checksum = 250314740824772360166326884854870837127, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.405Z debug(replica): 2n: on_prepare: advancing commit_max=38..39
2025-12-16 17:45:30.405Z debug(replica): 2n: on_prepare: caching prepare.op=40 (commit_min=38 op=39 commit_max=39 prepare_max=1007)
2025-12-16 17:45:30.405Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-12-16 17:45:30.405Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-12-16 17:45:30.405Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 250314740824772360166326884854870837127, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 41371991270414184121217006489328048574, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 38, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2641463330, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.405Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:30.405Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:30.405Z debug(replica): 2n: on_prepare: advancing: op=39..40 checksum=31010712456673645062526977245849357208..321020414413213519586207344965930768827
2025-12-16 17:45:30.405Z debug(journal): 2: set_header_as_dirty: op=40 checksum=321020414413213519586207344965930768827
2025-12-16 17:45:30.405Z debug(replica): 2n: append: appending to journal op=40
2025-12-16 17:45:30.405Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 321020414413213519586207344965930768827, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .request_checksum = 250314740824772360166326884854870837127, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.405Z debug(journal): 2: write: view=1 slot=40 op=40 len=1408: 321020414413213519586207344965930768827 starting
2025-12-16 17:45:30.405Z debug(replica): 0n: on_prepare: advancing commit_max=38..39
2025-12-16 17:45:30.405Z debug(replica): 0n: on_prepare: caching prepare.op=40 (commit_min=38 op=39 commit_max=39 prepare_max=1007)
2025-12-16 17:45:30.405Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 locked
2025-12-16 17:45:30.405Z debug(replica): 2n: commit_start_journal: cached prepare op=39 checksum=31010712456673645062526977245849357208
2025-12-16 17:45:30.405Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 unlocked
2025-12-16 17:45:30.405Z debug(journal): 1: write_header: op=40 sectors[8192..12288]
2025-12-16 17:45:30.405Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:30.405Z debug(replica): 0n: on_prepare: advancing: op=39..40 checksum=31010712456673645062526977245849357208..321020414413213519586207344965930768827
2025-12-16 17:45:30.405Z debug(journal): 0: set_header_as_dirty: op=40 checksum=321020414413213519586207344965930768827
2025-12-16 17:45:30.405Z debug(replica): 0n: append: appending to journal op=40
2025-12-16 17:45:30.405Z debug(journal): 0: write: view=1 slot=40 op=40 len=1408: 321020414413213519586207344965930768827 starting
2025-12-16 17:45:30.405Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:30.405Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 locked
2025-12-16 17:45:30.405Z debug(journal): 1: write: view=1 slot=40 op=40 len=1408: 321020414413213519586207344965930768827 complete, marking clean
2025-12-16 17:45:30.405Z debug(replica): 0n: commit_start_journal: cached prepare op=39 checksum=31010712456673645062526977245849357208
2025-12-16 17:45:30.405Z debug(replica): 1N: send_prepare_ok: op=40 checksum=321020414413213519586207344965930768827
2025-12-16 17:45:30.405Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 111220855648642881643699584552141725161, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .prepare_checksum = 321020414413213519586207344965930768827, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit_min = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.405Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 111220855648642881643699584552141725161, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .prepare_checksum = 321020414413213519586207344965930768827, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit_min = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.405Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-16 17:45:30.405Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-16 17:45:30.405Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-16 17:45:30.406Z debug(replica): 2n: repair_prepare: op=40 checksum=321020414413213519586207344965930768827 (already writing)
2025-12-16 17:45:30.406Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=38)
2025-12-16 17:45:30.406Z debug(replica): 0n: repair_prepare: op=40 checksum=321020414413213519586207344965930768827 (already writing)
2025-12-16 17:45:30.406Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=38)
2025-12-16 17:45:30.408Z debug(replica): 2n: execute_op: executing view=1 primary=false op=39 checksum=31010712456673645062526977245849357208 (create_transfers)
2025-12-16 17:45:30.408Z debug(replica): 2n: execute_op: commit_timestamp=1765907127692915903 prepare.header.timestamp=1765907127750853234
2025-12-16 17:45:30.408Z debug(replica): 0n: execute_op: executing view=1 primary=false op=39 checksum=31010712456673645062526977245849357208 (create_transfers)
2025-12-16 17:45:30.408Z debug(replica): 0n: execute_op: commit_timestamp=1765907127692915903 prepare.header.timestamp=1765907127750853234
2025-12-16 17:45:30.424Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:30.424Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:30.428Z debug(replica): 0n: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=37
2025-12-16 17:45:30.428Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 6054803367465460167716837285540741624, .checksum_padding = 0, .checksum_body = 86846841502176747884143737702274868606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 296, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 240064173979705808641051079925877545461, .request_checksum_padding = 0, .context = 41371991270414184121217006489328048574, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit = 39, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.428Z debug(replica): 0n: sending reply to client 166665032907361157923918867912184359346: vsr.message_header.Header.Reply{ .checksum = 6054803367465460167716837285540741624, .checksum_padding = 0, .checksum_body = 86846841502176747884143737702274868606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 296, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 240064173979705808641051079925877545461, .request_checksum_padding = 0, .context = 41371991270414184121217006489328048574, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 39, .commit = 39, .timestamp = 1765907127750853234, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.428Z debug(forest): entering forest.compact() op=39 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:30.428Z debug(replica): 2n: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=37
2025-12-16 17:45:30.428Z debug(forest): entering forest.compact() op=39 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:30.439Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 unlocked
2025-12-16 17:45:30.439Z debug(journal): 0: write_header: op=40 sectors[8192..12288]
2025-12-16 17:45:30.439Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:30.439Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:30.439Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:30.439Z debug(client_replies): 0: write_reply: wrote (client=166665032907361157923918867912184359346 request=37)
2025-12-16 17:45:30.439Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 unlocked
2025-12-16 17:45:30.439Z debug(journal): 2: write_header: op=40 sectors[8192..12288]
2025-12-16 17:45:30.439Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:30.439Z debug(client_replies): 2: write_reply: wrote (client=166665032907361157923918867912184359346 request=37)
2025-12-16 17:45:30.439Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:30.439Z debug(journal): 0: write: view=1 slot=40 op=40 len=1408: 321020414413213519586207344965930768827 complete, marking clean
2025-12-16 17:45:30.439Z debug(replica): 0n: send_prepare_ok: op=40 checksum=321020414413213519586207344965930768827
2025-12-16 17:45:30.439Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:30.439Z debug(journal): 2: write: view=1 slot=40 op=40 len=1408: 321020414413213519586207344965930768827 complete, marking clean
2025-12-16 17:45:30.439Z debug(replica): 2n: send_prepare_ok: op=40 checksum=321020414413213519586207344965930768827
2025-12-16 17:45:30.439Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 123590404750049817365205463569558130885, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .prepare_checksum = 321020414413213519586207344965930768827, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit_min = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.439Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 247992029234241831177641513369794656321, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .prepare_checksum = 321020414413213519586207344965930768827, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit_min = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.440Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 247992029234241831177641513369794656321, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .prepare_checksum = 321020414413213519586207344965930768827, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit_min = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.440Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-16 17:45:30.440Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-12-16 17:45:30.440Z debug(replica): 1N: on_prepare_ok: quorum received, context=321020414413213519586207344965930768827
2025-12-16 17:45:30.440Z debug(vsr): 1: prepare_timeout stopped
2025-12-16 17:45:30.440Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-12-16 17:45:30.440Z debug(replica): 1N: execute_op: executing view=1 primary=true op=40 checksum=321020414413213519586207344965930768827 (lookup_accounts)
2025-12-16 17:45:30.440Z debug(replica): 1N: execute_op: commit_timestamp=1765907127750853234 prepare.header.timestamp=1765907130404872881
2025-12-16 17:45:30.440Z debug(replica): 1N: execute_op: advancing commit_max=39..40
2025-12-16 17:45:30.440Z debug(replica): 1N: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=38
2025-12-16 17:45:30.440Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 16414559185726883966910989568480080532, .checksum_padding = 0, .checksum_body = 101501480128577071138118802513456294189, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 9472, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250314740824772360166326884854870837127, .request_checksum_padding = 0, .context = 299075481701345039438015469887026119254, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit = 40, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.440Z debug(replica): 1N: sending reply to client 166665032907361157923918867912184359346: vsr.message_header.Header.Reply{ .checksum = 16414559185726883966910989568480080532, .checksum_padding = 0, .checksum_body = 101501480128577071138118802513456294189, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 9472, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250314740824772360166326884854870837127, .request_checksum_padding = 0, .context = 299075481701345039438015469887026119254, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit = 40, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.440Z debug(forest): entering forest.compact() op=40 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:30.440Z info(workload): accounts created = 71, transfers = 23054, pending transfers = 0, commands run = 19
2025-12-16 17:45:30.440Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 123590404750049817365205463569558130885, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31010712456673645062526977245849357208, .parent_padding = 0, .prepare_checksum = 321020414413213519586207344965930768827, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit_min = 39, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:30.440Z debug(replica): 1N: on_prepare_ok: not preparing op=40 checksum=321020414413213519586207344965930768827
2025-12-16 17:45:30.440Z debug(client_replies): 1: write_reply: wrote (client=166665032907361157923918867912184359346 request=38)
2025-12-16 17:45:30.444Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:30.444Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:30.449Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:30.449Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:30.469Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:30.469Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:30.489Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:30.489Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:30.509Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:30.509Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:30.529Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:30.451Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.451Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.143Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:33.143Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-12-16 17:45:33.144Z debug(replica): 1N: primary_pipeline_prepare: request checksum=295975068506590856544468880171937635338 client=166665032907361157923918867912184359346
2025-12-16 17:45:33.144Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:30.529Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:32.031Z info(supervisor): injecting network loss: testing.vortex.faulty_network.Faults{ .delay = null, .lose = 1/100, .corrupt = null }
2025-12-16 17:45:33.144Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:33.144Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:33.146Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=253372602743604193088292026934069770660 op=41
2025-12-16 17:45:33.146Z debug(vsr): 1: prepare_timeout started
2025-12-16 17:45:33.146Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-16 17:45:33.146Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:33.146Z debug(replica): 1N: replicate: replicating op=41 to replica 0
2025-12-16 17:45:33.146Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 253372602743604193088292026934069770660, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .request_checksum = 295975068506590856544468880171937635338, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.146Z debug(replica): 1N: replicate: replicating op=41 to replica 2
2025-12-16 17:45:33.146Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 253372602743604193088292026934069770660, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .request_checksum = 295975068506590856544468880171937635338, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.147Z debug(replica): 1N: on_prepare: advancing: op=40..41 checksum=321020414413213519586207344965930768827..253372602743604193088292026934069770660
2025-12-16 17:45:33.147Z debug(journal): 1: set_header_as_dirty: op=41 checksum=253372602743604193088292026934069770660
2025-12-16 17:45:33.147Z debug(replica): 1N: append: appending to journal op=41
2025-12-16 17:45:33.147Z debug(journal): 1: write: view=1 slot=41 op=41 len=628864: 253372602743604193088292026934069770660 starting
2025-12-16 17:45:33.147Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=630784 locked
2025-12-16 17:45:33.147Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.147Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-12-16 17:45:33.147Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.147Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.147Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-16 17:45:33.147Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.150Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.150Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:33.150Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:33.150Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 253372602743604193088292026934069770660, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .request_checksum = 295975068506590856544468880171937635338, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.150Z debug(replica): 2n: on_prepare: advancing commit_max=39..40
2025-12-16 17:45:33.150Z debug(replica): 2n: on_prepare: caching prepare.op=41 (commit_min=39 op=40 commit_max=40 prepare_max=1007)
2025-12-16 17:45:33.150Z debug(replica): 2n: on_prepare: advancing: op=40..41 checksum=321020414413213519586207344965930768827..253372602743604193088292026934069770660
2025-12-16 17:45:33.150Z debug(journal): 2: set_header_as_dirty: op=41 checksum=253372602743604193088292026934069770660
2025-12-16 17:45:33.150Z debug(replica): 2n: append: appending to journal op=41
2025-12-16 17:45:33.150Z debug(journal): 2: write: view=1 slot=41 op=41 len=628864: 253372602743604193088292026934069770660 starting
2025-12-16 17:45:33.150Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=630784 locked
2025-12-16 17:45:33.150Z debug(replica): 2n: commit_start_journal: cached prepare op=40 checksum=321020414413213519586207344965930768827
2025-12-16 17:45:33.150Z debug(replica): 2n: repair_prepare: op=41 checksum=253372602743604193088292026934069770660 (already writing)
2025-12-16 17:45:33.150Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=39)
2025-12-16 17:45:33.150Z debug(replica): 2n: execute_op: executing view=1 primary=false op=40 checksum=321020414413213519586207344965930768827 (lookup_accounts)
2025-12-16 17:45:33.150Z debug(replica): 2n: execute_op: commit_timestamp=1765907127750853234 prepare.header.timestamp=1765907130404872881
2025-12-16 17:45:33.150Z debug(replica): 2n: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=38
2025-12-16 17:45:33.150Z debug(forest): entering forest.compact() op=40 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:33.150Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 253372602743604193088292026934069770660, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .request_checksum = 295975068506590856544468880171937635338, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.150Z debug(replica): 0n: on_prepare: advancing commit_max=39..40
2025-12-16 17:45:33.150Z debug(replica): 0n: on_prepare: caching prepare.op=41 (commit_min=39 op=40 commit_max=40 prepare_max=1007)
2025-12-16 17:45:33.151Z debug(replica): 0n: on_prepare: advancing: op=40..41 checksum=321020414413213519586207344965930768827..253372602743604193088292026934069770660
2025-12-16 17:45:33.151Z debug(journal): 0: set_header_as_dirty: op=41 checksum=253372602743604193088292026934069770660
2025-12-16 17:45:33.151Z debug(replica): 0n: append: appending to journal op=41
2025-12-16 17:45:33.151Z debug(journal): 0: write: view=1 slot=41 op=41 len=628864: 253372602743604193088292026934069770660 starting
2025-12-16 17:45:33.151Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=630784 locked
2025-12-16 17:45:33.151Z debug(replica): 0n: commit_start_journal: cached prepare op=40 checksum=321020414413213519586207344965930768827
2025-12-16 17:45:33.151Z debug(replica): 0n: repair_prepare: op=41 checksum=253372602743604193088292026934069770660 (already writing)
2025-12-16 17:45:33.151Z debug(client_replies): 2: write_reply: wrote (client=166665032907361157923918867912184359346 request=38)
2025-12-16 17:45:33.151Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=39)
2025-12-16 17:45:33.151Z debug(replica): 0n: execute_op: executing view=1 primary=false op=40 checksum=321020414413213519586207344965930768827 (lookup_accounts)
2025-12-16 17:45:33.151Z debug(replica): 0n: execute_op: commit_timestamp=1765907127750853234 prepare.header.timestamp=1765907130404872881
2025-12-16 17:45:33.151Z debug(replica): 0n: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=38
2025-12-16 17:45:33.151Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 16414559185726883966910989568480080532, .checksum_padding = 0, .checksum_body = 101501480128577071138118802513456294189, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 9472, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250314740824772360166326884854870837127, .request_checksum_padding = 0, .context = 299075481701345039438015469887026119254, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit = 40, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.151Z debug(replica): 0n: sending reply to client 166665032907361157923918867912184359346: vsr.message_header.Header.Reply{ .checksum = 16414559185726883966910989568480080532, .checksum_padding = 0, .checksum_body = 101501480128577071138118802513456294189, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 9472, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250314740824772360166326884854870837127, .request_checksum_padding = 0, .context = 299075481701345039438015469887026119254, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 40, .commit = 40, .timestamp = 1765907130404872881, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.151Z debug(forest): entering forest.compact() op=40 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:33.151Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=630784 unlocked
2025-12-16 17:45:33.151Z debug(journal): 2: write_header: op=41 sectors[8192..12288]
2025-12-16 17:45:33.151Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:33.151Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:33.151Z debug(journal): 2: write: view=1 slot=41 op=41 len=628864: 253372602743604193088292026934069770660 complete, marking clean
2025-12-16 17:45:33.151Z debug(replica): 2n: send_prepare_ok: op=41 checksum=253372602743604193088292026934069770660
2025-12-16 17:45:33.151Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 311691425142307014068796110435240801260, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .prepare_checksum = 253372602743604193088292026934069770660, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit_min = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.151Z debug(client_replies): 0: write_reply: wrote (client=166665032907361157923918867912184359346 request=38)
2025-12-16 17:45:33.152Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=630784 unlocked
2025-12-16 17:45:33.152Z debug(journal): 0: write_header: op=41 sectors[8192..12288]
2025-12-16 17:45:33.152Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:33.152Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:33.152Z debug(journal): 0: write: view=1 slot=41 op=41 len=628864: 253372602743604193088292026934069770660 complete, marking clean
2025-12-16 17:45:33.152Z debug(replica): 0n: send_prepare_ok: op=41 checksum=253372602743604193088292026934069770660
2025-12-16 17:45:33.152Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 30733504284874819834682471764052730873, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .prepare_checksum = 253372602743604193088292026934069770660, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit_min = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.152Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.152Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:33.152Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:33.152Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=630784 unlocked
2025-12-16 17:45:33.152Z debug(journal): 1: write_header: op=41 sectors[8192..12288]
2025-12-16 17:45:33.152Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:33.153Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:33.153Z debug(journal): 1: write: view=1 slot=41 op=41 len=628864: 253372602743604193088292026934069770660 complete, marking clean
2025-12-16 17:45:33.153Z debug(replica): 1N: send_prepare_ok: op=41 checksum=253372602743604193088292026934069770660
2025-12-16 17:45:33.153Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 60655346423044089700436717558557837393, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .prepare_checksum = 253372602743604193088292026934069770660, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit_min = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.153Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 60655346423044089700436717558557837393, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .prepare_checksum = 253372602743604193088292026934069770660, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit_min = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.153Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-16 17:45:33.153Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-16 17:45:33.153Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-16 17:45:33.154Z info(supervisor): injecting network delays: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 147, .jitter_ms = 50 }, .lose = 1/100, .corrupt = null }
2025-12-16 17:45:33.155Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.155Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:33.155Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:33.158Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.158Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:33.158Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:33.158Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 311691425142307014068796110435240801260, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .prepare_checksum = 253372602743604193088292026934069770660, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit_min = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.158Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-16 17:45:33.158Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-12-16 17:45:33.158Z debug(replica): 1N: on_prepare_ok: quorum received, context=253372602743604193088292026934069770660
2025-12-16 17:45:33.158Z debug(vsr): 1: prepare_timeout stopped
2025-12-16 17:45:33.158Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-12-16 17:45:33.160Z debug(replica): 1N: execute_op: executing view=1 primary=true op=41 checksum=253372602743604193088292026934069770660 (create_transfers)
2025-12-16 17:45:33.160Z debug(replica): 1N: execute_op: commit_timestamp=1765907130404872881 prepare.header.timestamp=1765907133144006984
2025-12-16 17:45:33.161Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.161Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.164Z info(supervisor): healing network
2025-12-16 17:45:33.164Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.164Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.174Z info(supervisor): injecting network delays: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 226, .jitter_ms = 50 }, .lose = null, .corrupt = null }
2025-12-16 17:45:33.177Z debug(replica): 1N: execute_op: advancing commit_max=40..41
2025-12-16 17:45:33.177Z debug(replica): 1N: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=39
2025-12-16 17:45:33.177Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 9057551913469773668674784193700353001, .checksum_padding = 0, .checksum_body = 75181908575003886978804768033752215083, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 295975068506590856544468880171937635338, .request_checksum_padding = 0, .context = 234333283018793756472084259160332141173, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit = 41, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.177Z debug(replica): 1N: sending reply to client 166665032907361157923918867912184359346: vsr.message_header.Header.Reply{ .checksum = 9057551913469773668674784193700353001, .checksum_padding = 0, .checksum_body = 75181908575003886978804768033752215083, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 295975068506590856544468880171937635338, .request_checksum_padding = 0, .context = 234333283018793756472084259160332141173, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit = 41, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.177Z debug(forest): entering forest.compact() op=41 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:33.181Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.181Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.184Z info(supervisor): 2: pausing replica
2025-12-16 17:45:33.184Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.184Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.188Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 295975068506590856544468880171937635338, .checksum_padding = 0, .checksum_body = 285619473355957187824624642552406063019, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 628864, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 299075481701345039438015469887026119254, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 36251057, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.188Z debug(replica): 1N: on_request: replying to duplicate request
2025-12-16 17:45:33.188Z debug(replica): 1N: on_request: repeat reply (client=166665032907361157923918867912184359346 request=39)
2025-12-16 17:45:33.188Z debug(replica): 1N: sending reply to client 166665032907361157923918867912184359346: vsr.message_header.Header.Reply{ .checksum = 9057551913469773668674784193700353001, .checksum_padding = 0, .checksum_body = 75181908575003886978804768033752215083, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 295975068506590856544468880171937635338, .request_checksum_padding = 0, .context = 234333283018793756472084259160332141173, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit = 41, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.188Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 30733504284874819834682471764052730873, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 321020414413213519586207344965930768827, .parent_padding = 0, .prepare_checksum = 253372602743604193088292026934069770660, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit_min = 40, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:33.188Z debug(replica): 1N: on_prepare_ok: not preparing op=41 checksum=253372602743604193088292026934069770660
2025-12-16 17:45:33.188Z debug(client_replies): 1: write_reply: wrote (client=166665032907361157923918867912184359346 request=39)
2025-12-16 17:45:33.194Z info(supervisor): 2: unpausing replica
2025-12-16 17:45:33.198Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.198Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.204Z info(supervisor): healing network
2025-12-16 17:45:33.204Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.204Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.204Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.204Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.214Z info(supervisor): injecting network delays: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 426, .jitter_ms = 50 }, .lose = null, .corrupt = null }
2025-12-16 17:45:33.214Z debug(vsr): 2: journal_repair_timeout fired
2025-12-16 17:45:33.214Z debug(vsr): 2: journal_repair_timeout reset
2025-12-16 17:45:33.218Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.218Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.224Z info(supervisor): sleeping for 1.057s
2025-12-16 17:45:33.224Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.224Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.224Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.224Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.238Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.238Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.238Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:33.238Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:33.244Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.244Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.244Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:33.244Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:33.244Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.244Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.258Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.258Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.264Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.264Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.264Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.264Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.278Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.278Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.278Z debug(vsr): 1: pulse_timeout fired
2025-12-16 17:45:33.278Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:33.284Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.284Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.284Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.284Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.298Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.298Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.304Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.304Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.304Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.304Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.314Z debug(vsr): 2: journal_repair_timeout fired
2025-12-16 17:45:33.314Z debug(vsr): 2: journal_repair_timeout reset
2025-12-16 17:45:33.318Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.318Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.324Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.324Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.324Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.324Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.338Z debug(vsr): 1: commit_message_timeout fired
2025-12-16 17:45:33.338Z debug(vsr): 1: commit_message_timeout reset
2025-12-16 17:45:33.338Z debug(replica): 1N: sending commit to replica 0: vsr.message_header.Header.Commit{ .checksum = 82335869106793443309104364988880235970, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 253372602743604193088292026934069770660, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 41, .timestamp_monotonic = 38022115858273845, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.339Z debug(replica): 1N: sending commit to replica 2: vsr.message_header.Header.Commit{ .checksum = 82335869106793443309104364988880235970, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 253372602743604193088292026934069770660, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 41, .timestamp_monotonic = 38022115858273845, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.339Z debug(vsr): 1: start_view_change_message_timeout fired
2025-12-16 17:45:33.339Z debug(vsr): 1: start_view_change_message_timeout reset
2025-12-16 17:45:33.339Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.339Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.339Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:33.339Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:33.339Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-12-16 17:45:33.339Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-12-16 17:45:33.344Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.344Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.344Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:33.344Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:33.344Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.344Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.359Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.359Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.364Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.364Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.364Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.364Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.379Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.379Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.379Z debug(vsr): 1: pulse_timeout fired
2025-12-16 17:45:33.379Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:33.384Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.384Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.385Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.385Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.399Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.399Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.404Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.404Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.405Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.405Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.415Z debug(vsr): 2: ping_timeout fired
2025-12-16 17:45:33.415Z debug(vsr): 2: ping_timeout reset
2025-12-16 17:45:33.415Z debug(replica): 2n: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 33573433562150898417619105362271638467, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022115934416411, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.415Z debug(replica): 2n: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 33573433562150898417619105362271638467, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022115934416411, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.415Z debug(vsr): 2: start_view_change_message_timeout fired
2025-12-16 17:45:33.415Z debug(vsr): 2: start_view_change_message_timeout reset
2025-12-16 17:45:33.415Z debug(vsr): 2: journal_repair_timeout fired
2025-12-16 17:45:33.415Z debug(vsr): 2: journal_repair_timeout reset
2025-12-16 17:45:33.415Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-12-16 17:45:33.415Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-12-16 17:45:33.419Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.419Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.425Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.425Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.425Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.425Z debug(vsr): 2: journal_repair_budget_timeout reset
warning(client): 166665032907361157923918867912184359346: on_reply: slow request, request=39 op=41 size=628864 create_transfers time=2986ms
2025-12-16 17:45:33.439Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.439Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.439Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:33.439Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:33.445Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.445Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.445Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:33.445Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:33.445Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.445Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.459Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.459Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.465Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.465Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.465Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.465Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.479Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.479Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.479Z debug(vsr): 1: pulse_timeout fired
2025-12-16 17:45:33.479Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:33.485Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.485Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.485Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.485Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.499Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.499Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.505Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.505Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.505Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.505Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.515Z debug(vsr): 2: journal_repair_timeout fired
2025-12-16 17:45:33.515Z debug(vsr): 2: journal_repair_timeout reset
2025-12-16 17:45:33.519Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.519Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.525Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.525Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.525Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.525Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.540Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.540Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.540Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:33.540Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:33.545Z debug(vsr): 0: ping_timeout fired
2025-12-16 17:45:33.545Z debug(vsr): 0: ping_timeout reset
2025-12-16 17:45:33.545Z debug(replica): 0n: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 150535842826589715438522340789391929130, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022116064806389, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.545Z debug(replica): 0n: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 150535842826589715438522340789391929130, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022116064806389, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:33.545Z debug(vsr): 0: start_view_change_message_timeout fired
2025-12-16 17:45:33.545Z debug(vsr): 0: start_view_change_message_timeout reset
2025-12-16 17:45:33.545Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.545Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.545Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:33.545Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:33.545Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-12-16 17:45:33.545Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-12-16 17:45:33.545Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.545Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.560Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.560Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.565Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.565Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.565Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.566Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.580Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.580Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.580Z debug(vsr): 1: pulse_timeout fired
2025-12-16 17:45:33.580Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:33.585Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.585Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.586Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.586Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.600Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.600Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.605Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.605Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.606Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.606Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.616Z debug(vsr): 2: journal_repair_timeout fired
2025-12-16 17:45:33.616Z debug(vsr): 2: journal_repair_timeout reset
2025-12-16 17:45:33.620Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.620Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.625Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.625Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.626Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.626Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.640Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.640Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.640Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:33.640Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:33.646Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.646Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.646Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:33.646Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:33.646Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.646Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.660Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.660Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.666Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.666Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.666Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:33.666Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:33.680Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:33.680Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:33.680Z debug(vsr): 1: pulse_timeout fired
2025-12-16 17:45:33.686Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:33.686Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-16 17:45:36.121Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:33.680Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:36.121Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-16 17:45:34.287Z info(supervisor): injecting network loss: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 426, .jitter_ms = 50 }, .lose = 8/100, .corrupt = null }
2025-12-16 17:45:36.121Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 82335869106793443309104364988880235970, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 253372602743604193088292026934069770660, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 41, .timestamp_monotonic = 38022115858273845, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.121Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 82335869106793443309104364988880235970, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 253372602743604193088292026934069770660, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 41, .timestamp_monotonic = 38022115858273845, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.121Z debug(vsr): 0: normal_heartbeat_timeout reset
2025-12-16 17:45:36.121Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-12-16 17:45:36.121Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 33573433562150898417619105362271638467, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022115934416411, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.121Z debug(replica): 0n: on_commit: checksum verified
2025-12-16 17:45:36.121Z debug(replica): 2n: on_commit: checksum verified
2025-12-16 17:45:36.121Z debug(replica): 0n: on_commit: advancing commit_max=40..41
2025-12-16 17:45:36.121Z debug(replica): 2n: on_commit: advancing commit_max=40..41
2025-12-16 17:45:36.121Z debug(replica): 2n: commit_start_journal: cached prepare op=41 checksum=253372602743604193088292026934069770660
2025-12-16 17:45:36.121Z debug(replica): 0n: commit_start_journal: cached prepare op=41 checksum=253372602743604193088292026934069770660
2025-12-16 17:45:36.121Z debug(replica): 1N: sending pong to replica 2: vsr.message_header.Header.Pong{ .checksum = 304230044696041666570349766756769000137, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 38022115934416411, .pong_timestamp_wall = 1765907136121886933, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.122Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 309677453850700047477444493576958183194, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 234333283018793756472084259160332141173, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 40, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2986945393, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.122Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:36.122Z debug(replica): 1N: primary_pipeline_prepare: request checksum=309677453850700047477444493576958183194 client=166665032907361157923918867912184359346
2025-12-16 17:45:36.122Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=302117760904335068503740773793085087443 op=42
2025-12-16 17:45:36.122Z debug(vsr): 1: prepare_timeout started
2025-12-16 17:45:36.122Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-16 17:45:36.122Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:36.122Z debug(replica): 1N: replicate: replicating op=42 to replica 0
2025-12-16 17:45:36.122Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 302117760904335068503740773793085087443, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 253372602743604193088292026934069770660, .parent_padding = 0, .request_checksum = 309677453850700047477444493576958183194, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 42, .commit = 41, .timestamp = 1765907136122098684, .request = 40, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:36.122Z debug(replica): 1N: replicate: replicating op=42 to replica 2
2025-12-16 17:45:36.122Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 302117760904335068503740773793085087443, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 253372602743604193088292026934069770660, .parent_padding = 0, .request_checksum = 309677453850700047477444493576958183194, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 42, .commit = 41, .timestamp = 1765907136122098684, .request = 40, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:36.122Z debug(replica): 1N: on_prepare: advancing: op=41..42 checksum=253372602743604193088292026934069770660..302117760904335068503740773793085087443
2025-12-16 17:45:36.122Z debug(journal): 1: set_header_as_dirty: op=42 checksum=302117760904335068503740773793085087443
2025-12-16 17:45:36.122Z debug(replica): 1N: append: appending to journal op=42
2025-12-16 17:45:36.122Z debug(journal): 1: write: view=1 slot=42 op=42 len=1408: 302117760904335068503740773793085087443 starting
2025-12-16 17:45:36.122Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=44040192 len=4096 locked
2025-12-16 17:45:36.122Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 150535842826589715438522340789391929130, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022116064806389, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.122Z debug(replica): 1N: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 292224268742334052422835556563766196531, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 38022116064806389, .pong_timestamp_wall = 1765907136122334953, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.122Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 309677453850700047477444493576958183194, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 234333283018793756472084259160332141173, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 40, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2986945393, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.122Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:36.122Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:36.122Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 309677453850700047477444493576958183194, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 234333283018793756472084259160332141173, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 40, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2986945393, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.122Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-16 17:45:36.122Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=44040192 len=4096 unlocked
2025-12-16 17:45:36.123Z debug(journal): 1: write_header: op=42 sectors[8192..12288]
2025-12-16 17:45:36.122Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 309677453850700047477444493576958183194, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 234333283018793756472084259160332141173, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 40, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2986945393, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.123Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-16 17:45:36.123Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 150535842826589715438522340789391929130, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022116064806389, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.123Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-16 17:45:36.123Z debug(journal): 1: write: view=1 slot=42 op=42 len=1408: 302117760904335068503740773793085087443 complete, marking clean
2025-12-16 17:45:36.123Z debug(replica): 2n: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 191666246515146492881208400560093055951, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 38022116064806389, .pong_timestamp_wall = 1765907136123045001, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.123Z debug(replica): 1N: send_prepare_ok: op=42 checksum=302117760904335068503740773793085087443
2025-12-16 17:45:36.123Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 115120565402361246048089745653170589983, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 253372602743604193088292026934069770660, .parent_padding = 0, .prepare_checksum = 302117760904335068503740773793085087443, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 42, .commit_min = 41, .timestamp = 1765907136122098684, .request = 40, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:36.123Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 33573433562150898417619105362271638467, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022115934416411, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.123Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 115120565402361246048089745653170589983, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 253372602743604193088292026934069770660, .parent_padding = 0, .prepare_checksum = 302117760904335068503740773793085087443, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 42, .commit_min = 41, .timestamp = 1765907136122098684, .request = 40, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:36.123Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-16 17:45:36.123Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-16 17:45:36.123Z debug(replica): 0n: sending pong to replica 2: vsr.message_header.Header.Pong{ .checksum = 233038748108189849509412581834814506041, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 38022115934416411, .pong_timestamp_wall = 1765907136123114031, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.123Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-16 17:45:36.124Z debug(replica): 0n: execute_op: executing view=1 primary=false op=41 checksum=253372602743604193088292026934069770660 (create_transfers)
2025-12-16 17:45:36.124Z debug(replica): 0n: execute_op: commit_timestamp=1765907130404872881 prepare.header.timestamp=1765907133144006984
2025-12-16 17:45:36.124Z debug(replica): 2n: execute_op: executing view=1 primary=false op=41 checksum=253372602743604193088292026934069770660 (create_transfers)
2025-12-16 17:45:36.124Z debug(replica): 2n: execute_op: commit_timestamp=1765907130404872881 prepare.header.timestamp=1765907133144006984
2025-12-16 17:45:36.131Z info(supervisor): 2: terminating replica
2025-12-16 17:45:36.141Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.141Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.142Z info(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-12-16 17:45:36.143Z debug(replica): 0n: client_table_entry_update: client=166665032907361157923918867912184359346 session=2 request=39
2025-12-16 17:45:36.143Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 9057551913469773668674784193700353001, .checksum_padding = 0, .checksum_body = 75181908575003886978804768033752215083, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 295975068506590856544468880171937635338, .request_checksum_padding = 0, .context = 234333283018793756472084259160332141173, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit = 41, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.143Z debug(replica): 0n: sending reply to client 166665032907361157923918867912184359346: vsr.message_header.Header.Reply{ .checksum = 9057551913469773668674784193700353001, .checksum_padding = 0, .checksum_body = 75181908575003886978804768033752215083, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 336, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 295975068506590856544468880171937635338, .request_checksum_padding = 0, .context = 234333283018793756472084259160332141173, .context_padding = 0, .client = 166665032907361157923918867912184359346, .op = 41, .commit = 41, .timestamp = 1765907133144006984, .request = 39, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.143Z debug(forest): entering forest.compact() op=41 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-16 17:45:36.151Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=88ms
2025-12-16 17:45:36.152Z debug(client_replies): 0: write_reply: wrote (client=166665032907361157923918867912184359346 request=39)
2025-12-16 17:45:36.153Z info(supervisor): 2: starting replica
2025-12-16 17:45:36.153Z info(supervisor): going into 3m31s quiescence (no faults)
2025-12-16 17:45:36.155Z info(io): opening "0_2.tigerbeetle"...
2025-12-16 17:45:36.161Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.161Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.162Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.162Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.175Z info(main): multiversioning: upgrades disabled for development (0.0.1) release.
2025-12-16 17:45:36.175Z info(main): release=0.0.1
2025-12-16 17:45:36.175Z info(main): release_client_min=0.0.1
2025-12-16 17:45:36.175Z info(main): releases_bundled={ 0.0.1 }
2025-12-16 17:45:36.175Z info(main): git_commit=42b49563137e00922fbe44112e90a18fdcb21338
2025-12-16 17:45:36.175Z debug(superblock): null: open: started
2025-12-16 17:45:36.176Z debug(superblock): null: open: read_header: copy=0 size=8192 offset=0
2025-12-16 17:45:36.176Z debug(superblock): null: open: read_header: copy=1 size=8192 offset=24576
2025-12-16 17:45:36.176Z debug(superblock): null: open: read_header: copy=2 size=8192 offset=49152
2025-12-16 17:45:36.176Z debug(superblock): null: open: read_header: copy=3 size=8192 offset=73728
2025-12-16 17:45:36.176Z debug(superblock_quorums): copy: 0/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-16 17:45:36.176Z debug(superblock_quorums): copy: 1/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-16 17:45:36.176Z debug(superblock_quorums): copy: 2/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-16 17:45:36.176Z debug(superblock_quorums): copy: 3/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-16 17:45:36.176Z debug(superblock_quorums): quorum: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3 count=4 valid=true
2025-12-16 17:45:36.176Z debug(superblock): null: open: installed working superblock: checksum=7b32c8d46df0002da29939ed96939def sequence=3 release=0.0.1 cluster=00000000000000000000000000000000 replica_id=134075807420264837279280775697797238201 size=1141374976 free_set_blocks_acquired_size=0 free_set_blocks_released_size=0 client_sessions_size=0 checkpoint_id=f222e9ce156b309eaeb4af665242ac18 commit_min_checksum=108034676951432761169128540124443993015 commit_min=0 commit_max=0 log_view=1 view=1 sync_op_min=0 sync_op_max=0 manifest_oldest_checksum=0 manifest_oldest_address=0 manifest_newest_checksum=0 manifest_newest_address=0 manifest_block_count=0 snapshots_block_checksum=0 snapshots_block_address=0
2025-12-16 17:45:36.176Z debug(superblock): null: open: vsr_header: op=0 checksum=108034676951432761169128540124443993015
2025-12-16 17:45:36.176Z debug(superblock): null: open: complete
2025-12-16 17:45:36.177Z debug(journal): 2: slot_count=1024 size=1.000244140625GiB headers_size=256KiB prepares_size=1GiB
2025-12-16 17:45:36.182Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.182Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.182Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:36.182Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:36.182Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.182Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.202Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.202Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.202Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.202Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.202Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:36.202Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:36.207Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-12-16 17:45:36.222Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.222Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.222Z debug(vsr): 1: pulse_timeout fired
2025-12-16 17:45:36.222Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:36.222Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.222Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.239Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-12-16 17:45:36.240Z info(message_bus): 1: on_connect: connected to=2
2025-12-16 17:45:36.240Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-12-16 17:45:36.240Z info(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-12-16 17:45:36.242Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=55ms
2025-12-16 17:45:36.242Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.242Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.242Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.242Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.262Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.262Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.262Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.262Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.272Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-12-16 17:45:36.282Z debug(vsr): 1: ping_timeout fired
2025-12-16 17:45:36.282Z debug(vsr): 1: ping_timeout reset
2025-12-16 17:45:36.282Z debug(replica): 1N: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 187694898188176353628575934620633586899, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022118801795215, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.282Z debug(replica): 1N: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 187694898188176353628575934620633586899, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 38022118801795215, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.282Z debug(vsr): 1: commit_message_timeout fired
2025-12-16 17:45:36.282Z debug(vsr): 1: commit_message_timeout reset
2025-12-16 17:45:36.282Z debug(replica): 1N: sending commit to replica 0: vsr.message_header.Header.Commit{ .checksum = 305506195441551412376341436043117240759, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 253372602743604193088292026934069770660, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 41, .timestamp_monotonic = 38022118801910945, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.282Z debug(replica): 1N: sending commit to replica 2: vsr.message_header.Header.Commit{ .checksum = 305506195441551412376341436043117240759, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 253372602743604193088292026934069770660, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 41, .timestamp_monotonic = 38022118801910945, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.282Z debug(vsr): 1: start_view_change_message_timeout fired
2025-12-16 17:45:36.282Z debug(vsr): 1: start_view_change_message_timeout reset
2025-12-16 17:45:36.282Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.282Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.282Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:36.282Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:36.282Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-12-16 17:45:36.282Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-12-16 17:45:36.282Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.282Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.297Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-12-16 17:45:36.297Z info(message_bus): 1: on_connect: connected to=2
2025-12-16 17:45:36.297Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-12-16 17:45:36.297Z warning(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-12-16 17:45:36.302Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.PingClient{ .checksum = 169611741156070413950805526057953999228, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 166665032907361157923918867912184359346, .ping_timestamp_monotonic = 38022118821197136, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.302Z debug(replica): 0n: sending pong_client to client 166665032907361157923918867912184359346: vsr.message_header.Header.PongClient{ .checksum = 82756169144222234591609470784987372832, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 38022118821197136, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:36.302Z debug(manifest_log): 2: Manifest.Pace.half_bar_append_blocks_max = 1
2025-12-16 17:45:36.302Z debug(manifest_log): 2: Manifest.Pace.half_bar_compact_blocks_max = 2
2025-12-16 17:45:36.302Z debug(manifest_log): 2: Manifest.Pace.log_blocks_full_max = 586
2025-12-16 17:45:36.302Z debug(manifest_log): 2: Manifest.Pace.log_blocks_cycle_max = 1172
2025-12-16 17:45:36.302Z debug(manifest_log): 2: Manifest.Pace.log_blocks_max = 1466
2025-12-16 17:45:36.302Z debug(manifest_log): 2: Manifest.Pace.tables_max = 2396744
2025-12-16 17:45:36.302Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=88ms
2025-12-16 17:45:36.302Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.302Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.302Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.302Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.303Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:36.303Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:36.322Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.322Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.322Z debug(vsr): 1: pulse_timeout fired
2025-12-16 17:45:36.322Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:36.323Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.323Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.343Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.343Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.343Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.343Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.363Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.363Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.363Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.363Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.371Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
warning(message_bus): 166665032907361157923918867912184359346: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-12-16 17:45:36.373Z debug(vsr): 1: prepare_timeout fired
2025-12-16 17:45:36.373Z debug(vsr): 1: prepare_timeout backing off
2025-12-16 17:45:36.373Z debug(vsr): 1: prepare_timeout after=25..5 (rtt=1 min=1 max=1000 attempts=1)
2025-12-16 17:45:36.373Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-12-16 17:45:36.373Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-12-16 17:45:36.373Z debug(replica): 1N: on_prepare_timeout: replicating to replica 0
2025-12-16 17:45:36.373Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 302117760904335068503740773793085087443, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 253372602743604193088292026934069770660, .parent_padding = 0, .request_checksum = 309677453850700047477444493576958183194, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 42, .commit = 41, .timestamp = 1765907136122098684, .request = 40, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:36.383Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.383Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.383Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.383Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.383Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:36.383Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:36.390Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-12-16 17:45:36.390Z info(message_bus): 1: on_connect: connected to=2
2025-12-16 17:45:36.391Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-12-16 17:45:36.391Z info(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-12-16 17:45:36.393Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=59ms
2025-12-16 17:45:36.403Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.403Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.403Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:36.403Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:36.403Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.403Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.423Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.423Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.423Z debug(vsr): 1: prepare_timeout fired
2025-12-16 17:45:36.423Z debug(vsr): 1: prepare_timeout backing off
2025-12-16 17:45:36.423Z debug(vsr): 1: prepare_timeout after=5..5 (rtt=1 min=1 max=1000 attempts=2)
2025-12-16 17:45:36.423Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-12-16 17:45:36.423Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-12-16 17:45:36.423Z debug(replica): 1N: on_prepare_timeout: replicating to replica 2
2025-12-16 17:45:36.423Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 302117760904335068503740773793085087443, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 253372602743604193088292026934069770660, .parent_padding = 0, .request_checksum = 309677453850700047477444493576958183194, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 42, .commit = 41, .timestamp = 1765907136122098684, .request = 40, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:36.423Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.423Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.423Z debug(vsr): 1: pulse_timeout fired
2025-12-16 17:45:36.423Z debug(vsr): 1: pulse_timeout reset
2025-12-16 17:45:36.435Z debug(replica): 2r: init: replica_count=3 quorum_view_change=2 quorum_replication=2 release=0.0.1
2025-12-16 17:45:36.435Z info(replica): superblock release=0.0.1
2025-12-16 17:45:36.435Z debug(journal): 2: recover: recovering
2025-12-16 17:45:36.435Z debug(journal): 2: recover_headers: offset=0 size=262144 recovering
2025-12-16 17:45:36.435Z debug(journal): 2: recover_headers: offset=0 size=262144 recovered
2025-12-16 17:45:36.435Z debug(journal): 2: recover_headers: complete
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=0
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=1
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=2
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=3
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=4
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=5
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=6
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=7
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=8
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=9
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=10
2025-12-16 17:45:36.435Z debug(journal): 2: recover_prepare: recovering slot=11
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=12
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=13
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=14
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=15
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=16
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=17
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=18
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=19
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=20
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=21
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=22
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=23
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=24
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=25
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=26
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=27
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=28
2025-12-16 17:45:36.436Z debug(journal): 2: recover_prepare: recovering slot=29
2025-12-16 17:45:36.437Z debug(journal): 2: recover_prepare: recovering slot=30
2025-12-16 17:45:36.437Z debug(journal): 2: recover_prepare: recovering slot=31
2025-12-16 17:45:36.437Z debug(journal): 2: recover_prepare: recovering slot=32
2025-12-16 17:45:36.437Z debug(journal): 2: recover_prepare: recovering slot=33
2025-12-16 17:45:36.437Z debug(journal): 2: recover_prepare: recovering slot=34
2025-12-16 17:45:36.437Z debug(journal): 2: recover_prepare: recovering slot=35
2025-12-16 17:45:36.437Z debug(journal): 2: recover_prepare: recovering slot=36
2025-12-16 17:45:36.437Z debug(journal): 2: recover_prepare: recovering slot=37
2025-12-16 17:45:36.437Z debug(journal): 2: recover_prepare: recovering slot=38
2025-12-16 17:45:36.438Z debug(journal): 2: recover_prepare: recovering slot=39
2025-12-16 17:45:36.438Z debug(journal): 2: recover_prepare: recovering slot=40
2025-12-16 17:45:36.438Z debug(journal): 2: recover_prepare: recovering slot=41
2025-12-16 17:45:36.438Z debug(journal): 2: recover_prepare: recovering slot=42
2025-12-16 17:45:36.439Z debug(journal): 2: recover_prepare: recovering slot=43
2025-12-16 17:45:36.442Z debug(journal): 2: recover_prepare: recovering slot=44
2025-12-16 17:45:36.443Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:36.443Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:36.443Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.443Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:36.446Z debug(journal): 2: recover_prepare: recovering slot=45
2025-12-16 17:45:36.446Z debug(journal): 2: recover_prepare: recovering slot=46
2025-12-16 17:45:36.449Z debug(journal): 2: recover_prepare: recovering slot=47
2025-12-16 17:45:36.449Z debug(journal): 2: recover_prepare: recovering slot=48
2025-12-16 17:45:36.449Z debug(journal): 2: recover_prepare: recovering slot=49
2025-12-16 17:45:36.449Z debug(journal): 2: recover_prepare: recovering slot=50
2025-12-16 17:45:36.449Z debug(journal): 2: recover_prepare: recovering slot=51
2025-12-16 17:45:36.451Z debug(journal): 2: recover_prepare: recovering slot=52
2025-12-16 17:45:36.451Z debug(journal): 2: recover_prepare: recovering slot=53
2025-12-16 17:45:36.451Z debug(journal): 2: recover_prepare: recovering slot=54
2025-12-16 17:45:36.451Z debug(journal): 2: recover_prepare: recovering slot=55
2025-12-16 17:45:36.451Z debug(journal): 2: recover_prepare: recovering slot=56
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=57
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=58
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=59
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=60
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=61
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=62
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=63
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=64
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=65
2025-12-16 17:45:36.452Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=66
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=67
2025-12-16 17:45:36.452Z info(message_bus): 1: on_connect: connected to=2
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=68
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=69
2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=70
thread 1 panic: 2025-12-16 17:45:36.452Z debug(journal): 2: recover_prepare: recovering slot=71
2025-12-16 17:45:36.463Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:36.463Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:38.323Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:38.323Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:38.323Z debug(journal): 2: recover_prepare: recovering slot=72
2025-12-16 17:45:38.323Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 309677453850700047477444493576958183194, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 234333283018793756472084259160332141173, .parent_padding = 0, .client = 166665032907361157923918867912184359346, .session = 2, .timestamp = 0, .request = 40, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2986945393, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:38.323Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 191666246515146492881208400560093055951, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 38022116064806389, .pong_timestamp_wall = 1765907136123045001, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:38.323Z debug(clock): 0: learn: replica=2 m0=38022116064806389 t1=1765907136123045001 m2=38022120843044663 t2=1765907138323735458 one_way_delay=2389119137 asymmetric_delay=-2388886338 clock_offset=-2200457658
2025-12-16 17:45:38.323Z debug(replica): 1N: on_request: new request
2025-12-16 17:45:38.323Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-12-16 17:45:38.323Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
reached unreachable code
2025-12-16 17:45:38.323Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PingClient{ .checksum = 169611741156070413950805526057953999228, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 166665032907361157923918867912184359346, .ping_timestamp_monotonic = 38022118821197136, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:38.323Z debug(replica): 1N: sending pong_client to client 166665032907361157923918867912184359346: vsr.message_header.Header.PongClient{ .checksum = 252791517799443196944405338839429331267, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 38022118821197136, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-16 17:45:38.323Z debug(journal): 2: recover_prepare: recovering slot=73
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=74
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=75
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=76
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=77
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=78
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=79
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=80
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=81
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=82
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=83
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=84
2025-12-16 17:45:38.324Z debug(journal): 2: recover_prepare: recovering slot=85
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=86
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=87
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=88
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=89
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=90
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=91
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=92
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=93
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=94
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=95
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=96
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=97
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=98
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=99
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=100
2025-12-16 17:45:38.325Z debug(journal): 2: recover_prepare: recovering slot=101
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=102
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=103
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=104
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=105
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=106
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=107
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=108
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=109
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=110
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=111
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=112
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=113
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=114
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=115
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=116
2025-12-16 17:45:38.326Z debug(journal): 2: recover_prepare: recovering slot=117
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=118
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=119
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=120
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=121
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=122
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=123
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=124
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=125
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=126
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=127
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=128
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=129
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=130
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=131
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=132
2025-12-16 17:45:38.327Z debug(journal): 2: recover_prepare: recovering slot=133
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=134
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=135
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=136
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=137
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=138
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=139
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=140
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=141
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=142
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=143
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=144
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=145
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=146
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=147
2025-12-16 17:45:38.328Z debug(journal): 2: recover_prepare: recovering slot=148
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=149
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=150
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=151
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=152
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=153
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=154
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=155
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=156
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=157
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=158
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=159
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=160
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=161
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=162
2025-12-16 17:45:38.329Z debug(journal): 2: recover_prepare: recovering slot=163
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=164
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=165
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=166
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=167
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=168
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=169
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=170
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=171
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=172
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=173
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=174
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=175
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=176
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=177
2025-12-16 17:45:38.330Z debug(journal): 2: recover_prepare: recovering slot=178
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=179
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=180
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=181
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=182
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=183
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=184
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=185
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=186
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=187
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=188
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=189
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=190
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=191
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=192
2025-12-16 17:45:38.331Z debug(journal): 2: recover_prepare: recovering slot=193
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=194
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=195
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=196
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=197
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=198
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=199
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=200
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=201
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=202
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=203
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=204
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=205
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=206
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=207
2025-12-16 17:45:38.332Z debug(journal): 2: recover_prepare: recovering slot=208
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=209
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=210
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=211
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=212
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=213
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=214
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=215
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=216
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=217
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=218
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=219
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=220
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=221
2025-12-16 17:45:38.333Z debug(vsr): 1: prepare_timeout fired
2025-12-16 17:45:38.333Z debug(vsr): 1: prepare_timeout backing off
2025-12-16 17:45:38.333Z debug(clock): 0: synchronized: truechimers=2/3 clock_offset=0ns..0ns accuracy=0ns
2025-12-16 17:45:38.333Z debug(vsr): 1: prepare_timeout after=5..9 (rtt=1 min=1 max=1000 attempts=3)
2025-12-16 17:45:38.333Z debug(clock): 0: system time is 190ns behind
2025-12-16 17:45:38.333Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-12-16 17:45:38.333Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-12-16 17:45:38.333Z debug(replica): 1N: on_prepare_timeout: replicating to replica 0
2025-12-16 17:45:38.333Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 302117760904335068503740773793085087443, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 253372602743604193088292026934069770660, .parent_padding = 0, .request_checksum = 309677453850700047477444493576958183194, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 42, .commit = 41, .timestamp = 1765907136122098684, .request = 40, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:38.333Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=68ms
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=222
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=223
2025-12-16 17:45:38.333Z debug(journal): 2: recover_prepare: recovering slot=224
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=225
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=226
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=227
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=228
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=229
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=230
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=231
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=232
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=233
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=234
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=235
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=236
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=237
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=238
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=239
2025-12-16 17:45:38.334Z debug(journal): 2: recover_prepare: recovering slot=240
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=241
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=242
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=243
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=244
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=245
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=246
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=247
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=248
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=249
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=250
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=251
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=252
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=253
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=254
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=255
2025-12-16 17:45:38.335Z debug(journal): 2: recover_prepare: recovering slot=256
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=257
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=258
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=259
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=260
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=261
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=262
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=263
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=264
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=265
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=266
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=267
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=268
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=269
2025-12-16 17:45:38.336Z debug(journal): 2: recover_prepare: recovering slot=270
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=271
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=272
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=273
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=274
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=275
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=276
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=277
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=278
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=279
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=280
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=281
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=282
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=283
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=284
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=285
2025-12-16 17:45:38.337Z debug(journal): 2: recover_prepare: recovering slot=286
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=287
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=288
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=289
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=290
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=291
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=292
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=293
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=294
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=295
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=296
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=297
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=298
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=299
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=300
2025-12-16 17:45:38.338Z debug(journal): 2: recover_prepare: recovering slot=301
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=302
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=303
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=304
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=305
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=306
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=307
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=308
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=309
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=310
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=311
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=312
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=313
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=314
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=315
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=316
2025-12-16 17:45:38.339Z debug(journal): 2: recover_prepare: recovering slot=317
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=318
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=319
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=320
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=321
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=322
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=323
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=324
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=325
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=326
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=327
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=328
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=329
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=330
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=331
2025-12-16 17:45:38.340Z debug(journal): 2: recover_prepare: recovering slot=332
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=333
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=334
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=335
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=336
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=337
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=338
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=339
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=340
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=341
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=342
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=343
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=344
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=345
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=346
2025-12-16 17:45:38.341Z debug(journal): 2: recover_prepare: recovering slot=347
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=348
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=349
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=350
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=351
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=352
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=353
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=354
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=355
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=356
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=357
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=358
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=359
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=360
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=361
2025-12-16 17:45:38.342Z debug(journal): 2: recover_prepare: recovering slot=362
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=363
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=364
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=365
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=366
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=367
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=368
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=369
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=370
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=371
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=372
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=373
2025-12-16 17:45:38.343Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:38.343Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=374
2025-12-16 17:45:38.343Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:38.343Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:38.343Z debug(vsr): 1: journal_repair_timeout fired
2025-12-16 17:45:38.343Z debug(vsr): 1: journal_repair_timeout reset
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=375
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=376
2025-12-16 17:45:38.343Z debug(journal): 2: recover_prepare: recovering slot=377
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=378
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=379
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=380
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=381
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=382
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=383
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=384
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=385
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=386
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=387
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=388
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=389
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=390
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=391
2025-12-16 17:45:38.344Z debug(journal): 2: recover_prepare: recovering slot=392
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=393
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=394
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=395
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=396
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=397
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=398
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=399
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=400
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=401
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=402
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=403
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=404
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=405
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=406
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=407
2025-12-16 17:45:38.345Z debug(journal): 2: recover_prepare: recovering slot=408
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=409
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=410
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=411
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=412
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=413
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=414
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=415
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=416
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=417
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=418
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=419
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=420
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=421
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=422
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=423
2025-12-16 17:45:38.346Z debug(journal): 2: recover_prepare: recovering slot=424
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=425
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=426
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=427
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=428
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=429
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=430
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=431
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=432
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=433
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=434
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=435
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=436
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=437
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=438
2025-12-16 17:45:38.347Z debug(journal): 2: recover_prepare: recovering slot=439
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=440
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=441
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=442
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=443
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=444
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=445
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=446
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=447
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=448
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=449
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=450
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=451
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=452
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=453
2025-12-16 17:45:38.348Z debug(journal): 2: recover_prepare: recovering slot=454
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=455
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=456
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=457
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=458
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=459
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=460
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=461
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=462
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=463
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=464
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=465
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=466
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=467
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=468
2025-12-16 17:45:38.349Z debug(journal): 2: recover_prepare: recovering slot=469
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=470
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=471
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=472
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=473
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=474
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=475
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=476
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=477
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=478
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=479
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=480
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=481
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=482
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=483
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=484
2025-12-16 17:45:38.350Z debug(journal): 2: recover_prepare: recovering slot=485
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=486
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=487
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=488
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=489
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=490
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=491
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=492
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=493
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=494
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=495
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=496
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=497
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=498
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=499
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=500
2025-12-16 17:45:38.351Z debug(journal): 2: recover_prepare: recovering slot=501
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=502
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=503
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=504
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=505
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=506
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=507
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=508
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=509
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=510
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=511
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=512
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=513
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=514
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=515
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=516
2025-12-16 17:45:38.352Z debug(journal): 2: recover_prepare: recovering slot=517
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=518
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=519
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=520
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=521
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=522
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=523
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=524
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=525
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=526
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=527
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=528
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=529
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=530
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=531
2025-12-16 17:45:38.353Z debug(journal): 2: recover_prepare: recovering slot=532
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=533
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=534
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=535
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=536
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=537
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=538
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=539
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=540
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=541
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=542
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=543
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=544
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=545
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=546
2025-12-16 17:45:38.354Z debug(journal): 2: recover_prepare: recovering slot=547
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=548
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=549
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=550
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=551
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=552
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=553
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=554
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=555
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=556
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=557
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=558
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=559
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=560
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=561
2025-12-16 17:45:38.355Z debug(journal): 2: recover_prepare: recovering slot=562
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=563
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=564
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=565
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=566
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=567
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=568
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=569
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=570
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=571
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=572
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=573
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=574
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=575
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=576
2025-12-16 17:45:38.356Z debug(journal): 2: recover_prepare: recovering slot=577
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=578
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=579
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=580
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=581
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=582
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=583
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=584
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=585
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=586
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=587
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=588
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=589
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=590
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=591
2025-12-16 17:45:38.357Z debug(journal): 2: recover_prepare: recovering slot=592
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=593
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=594
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=595
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=596
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=597
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=598
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=599
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=600
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=601
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=602
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=603
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=604
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=605
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=606
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=607
2025-12-16 17:45:38.358Z debug(journal): 2: recover_prepare: recovering slot=608
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=609
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=610
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=611
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=612
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=613
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=614
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=615
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=616
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=617
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=618
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=619
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=620
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=621
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=622
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=623
2025-12-16 17:45:38.359Z debug(journal): 2: recover_prepare: recovering slot=624
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=625
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=626
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=627
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=628
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=629
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=630
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=631
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=632
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=633
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=634
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=635
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=636
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=637
2025-12-16 17:45:38.360Z debug(journal): 2: recover_prepare: recovering slot=638
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=639
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=640
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=641
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=642
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=643
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=644
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=645
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=646
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=647
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=648
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=649
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=650
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=651
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=652
2025-12-16 17:45:38.361Z debug(journal): 2: recover_prepare: recovering slot=653
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=654
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=655
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=656
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=657
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=658
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=659
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=660
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=661
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=662
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=663
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=664
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=665
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=666
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=667
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=668
2025-12-16 17:45:38.362Z debug(journal): 2: recover_prepare: recovering slot=669
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=670
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=671
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=672
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=673
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=674
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=675
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=676
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=677
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=678
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=679
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=680
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=681
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=682
2025-12-16 17:45:38.363Z debug(vsr): 0: start_view_change_message_timeout fired
2025-12-16 17:45:38.363Z debug(vsr): 0: start_view_change_message_timeout reset
2025-12-16 17:45:38.363Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:38.363Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:38.363Z debug(vsr): 0: journal_repair_timeout fired
2025-12-16 17:45:38.363Z debug(vsr): 0: journal_repair_timeout reset
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=683
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=684
2025-12-16 17:45:38.363Z debug(journal): 2: recover_prepare: recovering slot=685
2025-12-16 17:45:38.363Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-12-16 17:45:38.363Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-12-16 17:45:38.363Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:38.363Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=686
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=687
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=688
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=689
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=690
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=691
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=692
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=693
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=694
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=695
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=696
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=697
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=698
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=699
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=700
2025-12-16 17:45:38.364Z debug(journal): 2: recover_prepare: recovering slot=701
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=702
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=703
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=704
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=705
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=706
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=707
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=708
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=709
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=710
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=711
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=712
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=713
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=714
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=715
2025-12-16 17:45:38.365Z debug(journal): 2: recover_prepare: recovering slot=716
2025-12-16 17:45:41.074Z debug(journal): 2: recover_prepare: recovering slot=717
2025-12-16 17:45:38.383Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:38.383Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:41.074Z debug(vsr): 1: journal_repair_budget_timeout reset
/root/tigerbeetle/zig/lib/std/debug.zig2025-12-16 17:45:41.074Z debug(vsr): 1: pulse_timeout fired
2025-12-16 17:45:41.074Z debug(vsr): 1: pulse_timeout reset
:2025-12-16 17:45:41.074Z debug(vsr): 0: journal_repair_budget_timeout reset
550:14: 0x1244a0d in assert (vortex)
2025-12-16 17:45:41.074Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-12-16 17:45:41.074Z info(message_bus): 0: on_connect: connected to=2
    if (!ok) unreachable; // assertion failure
             ^
/root/tigerbeetle/working/main/src/testing/vortex/faulty_network.zig:88:15: 0x13a3974 in recv (vortex)
        assert(!pipe.recv_inflight);
              ^
/root/tigerbeetle/working/main/src/testing/vortex/faulty_network.zig:2332025-12-16 17:45:41.074Z debug(journal): 2: recover_prepare: recovering slot=718
:22: 0x13a68a2 in on_send (vortex)
            pipe.recv();
                     ^
2025-12-16 17:45:41.074Z debug(journal): 2: recover_prepare: recovering slot=719
/root/tigerbeetle/working/main/src/io/linux.zig:1856:25: 0x13a63f9 in erased (vortex)
2025-12-16 17:45:41.074Z debug(journal): 2: recover_prepare: recovering slot=720
                callback(ctx, completion, result.*);
                        ^
/root/tigerbeetle/working/main/src/io/linux.zig:738:40: 0x1306517 in complete (vortex)
2025-12-16 17:45:41.075Z debug(journal): 2: recover_prepare: recovering slot=721
2025-12-16 17:45:41.075Z debug(journal): 2: recover_prepare: recovering slot=722
                    completion.callback(completion.context, completion, &result);
                                       ^
2025-12-16 17:45:41.075Z debug(journal): 2: recover_prepare: recovering slot=723
/root/tigerbeetle/working/main/src/io/linux.zig:194:49: 0x1304980 in flush (vortex)
                .inactive => completion.complete(),
                                                ^
2025-12-16 17:45:41.075Z debug(journal): 2: recover_prepare: recovering slot=724
/root/tigerbeetle/working/main/src/io/linux.zig:149:27: 0x1307096 in run_for_ns (vortex)
            try self.flush(1, &timeouts, &etime);
                          ^
2025-12-16 17:45:41.075Z debug(journal): 2: recover_prepare: recovering slot=725
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:263:41: 0x1307a90 in run (vortex)
            try supervisor.io.run_for_ns(constants.vsr.tick_ms * std.time.ns_per_ms);
                                        ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:207:23: 0x130c4fb in main (vortex)
    try supervisor.run();
                      ^
/root/tigerbeetle/working/main/src/vortex.zig:61:61: 0x1321604 in main (vortex)
        .supervisor => |supervisor_args| try Supervisor.main(allocator, supervisor_args),
                                                            ^
/root/tigerbeetle/zig/lib/std/start.zig:660:37: 0x13220d7 in main (vortex)
2025-12-16 17:45:41.075Z debug(journal): 2: recover_prepare: recovering slot=726
            const result = root.main() catch |err| {
                                    ^
2025-12-16 17:45:41.075Z debug(journal): 2: recover_prepare: recovering slot=727
2025-12-16 17:45:41.075Z debug(journal): 2: recover_prepare: recovering slot=728
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=729
/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c:95:7: 0x154e698 in libc_start_main_stage2 (/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c)
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=730
 exit(main(argc, argv, envp));
      ^
Unwind error at address `exe:0x154e698` (error.AddressOutOfRange), trace may be incomplete

2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=731
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=732
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=733
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=734
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=735
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=736
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=737
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=738
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=739
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=740
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=741
2025-12-16 17:45:41.076Z debug(journal): 2: recover_prepare: recovering slot=742
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=743
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=744
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=745
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=746
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=747
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=748
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=749
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=750
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=751
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=752
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=753
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=754
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=755
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=756
2025-12-16 17:45:41.077Z debug(journal): 2: recover_prepare: recovering slot=757
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=758
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=759
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=760
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=761
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=762
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=763
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=764
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=765
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=766
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=767
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=768
2025-12-16 17:45:41.078Z debug(journal): 2: recover_prepare: recovering slot=769
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=770
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=771
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=772
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=773
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=774
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=775
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=776
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=777
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=778
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=779
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=780
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=781
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=782
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=783
2025-12-16 17:45:41.079Z debug(journal): 2: recover_prepare: recovering slot=784
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=785
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=786
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=787
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=788
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=789
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=790
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=791
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=792
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=793
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=794
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=795
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=796
2025-12-16 17:45:41.080Z debug(journal): 2: recover_prepare: recovering slot=797
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=798
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=799
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=800
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=801
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=802
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=803
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=804
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=805
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=806
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=807
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=808
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=809
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=810
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=811
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=812
2025-12-16 17:45:41.081Z debug(journal): 2: recover_prepare: recovering slot=813
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=814
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=815
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=816
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=817
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=818
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=819
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=820
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=821
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=822
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=823
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=824
2025-12-16 17:45:41.082Z debug(journal): 2: recover_prepare: recovering slot=825
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=826
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=827
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=828
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=829
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=830
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=831
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=832
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=833
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=834
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=835
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=836
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=837
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=838
2025-12-16 17:45:41.083Z debug(journal): 2: recover_prepare: recovering slot=839
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=840
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=841
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=842
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=843
2025-12-16 17:45:41.084Z warning(clock): 1: synchronization failed, partitioned (sources=1 samples=1)
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=844
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=845
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=846
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=847
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=848
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=849
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=850
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=851
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=852
2025-12-16 17:45:41.084Z debug(journal): 2: recover_prepare: recovering slot=853
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=854
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=855
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=856
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=857
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=858
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=859
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=860
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=861
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=862
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=863
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=864
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=865
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=866
2025-12-16 17:45:41.085Z debug(journal): 2: recover_prepare: recovering slot=867
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=868
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=869
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=870
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=871
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=872
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=873
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=874
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=875
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=876
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=877
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=878
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=879
2025-12-16 17:45:41.086Z debug(journal): 2: recover_prepare: recovering slot=880
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=881
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=882
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=883
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=884
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=885
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=886
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=887
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=888
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=889
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=890
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=891
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=892
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=893
2025-12-16 17:45:41.087Z debug(journal): 2: recover_prepare: recovering slot=894
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=895
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=896
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=897
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=898
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=899
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=900
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=901
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=902
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=903
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=904
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=905
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=906
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=907
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=908
2025-12-16 17:45:41.088Z debug(journal): 2: recover_prepare: recovering slot=909
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=910
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=911
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=912
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=913
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=914
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=915
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=916
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=917
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=918
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=919
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=920
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=921
2025-12-16 17:45:41.089Z debug(journal): 2: recover_prepare: recovering slot=922
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=923
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=924
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=925
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=926
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=927
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=928
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=929
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=930
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=931
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=932
2025-12-16 17:45:41.090Z debug(journal): 2: recover_prepare: recovering slot=933
2025-12-16 17:45:41.091Z debug(journal): 2: recover_prepare: recovering slot=934
2025-12-16 17:45:41.091Z debug(journal): 2: recover_prepare: recovering slot=935
2025-12-16 17:45:41.091Z debug(journal): 2: recover_prepare: recovering slot=936
2025-12-16 17:45:41.091Z debug(journal): 2: recover_prepare: recovering slot=937
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=938
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=939
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=940
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=941
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=942
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=943
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=944
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=945
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=946
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=947
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=948
2025-12-16 17:45:41.092Z debug(journal): 2: recover_prepare: recovering slot=949
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=950
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=951
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=952
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=953
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=954
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=955
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=956
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=957
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=958
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=959
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=960
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=961
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=962
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=963
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=964
2025-12-16 17:45:41.093Z debug(journal): 2: recover_prepare: recovering slot=965
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=966
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=967
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=968
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=969
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=970
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=971
2025-12-16 17:45:41.094Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:41.094Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=972
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=973
2025-12-16 17:45:41.094Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:41.094Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=974
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=975
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=976
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=977
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=978
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=979
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=980
2025-12-16 17:45:41.094Z debug(journal): 2: recover_prepare: recovering slot=981
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=982
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=983
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=984
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=985
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=986
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=987
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=988
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=989
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=990
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=991
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=992
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=993
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=994
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=995
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=996
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=997
2025-12-16 17:45:41.095Z debug(journal): 2: recover_prepare: recovering slot=998
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=999
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1000
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1001
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1002
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1003
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1004
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1005
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1006
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1007
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1008
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1009
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1010
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1011
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1012
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1013
2025-12-16 17:45:41.096Z debug(journal): 2: recover_prepare: recovering slot=1014
2025-12-16 17:45:41.097Z debug(journal): 2: recover_prepare: recovering slot=1015
2025-12-16 17:45:41.097Z debug(journal): 2: recover_prepare: recovering slot=1016
2025-12-16 17:45:41.097Z debug(journal): 2: recover_prepare: recovering slot=1017
2025-12-16 17:45:41.097Z debug(journal): 2: recover_prepare: recovering slot=1018
2025-12-16 17:45:41.097Z debug(journal): 2: recover_prepare: recovering slot=1019
2025-12-16 17:45:41.097Z debug(journal): 2: recover_prepare: recovering slot=1020
2025-12-16 17:45:41.097Z debug(journal): 2: recover_prepare: recovering slot=1021
2025-12-16 17:45:41.097Z debug(journal): 2: recover_prepare: recovering slot=1022
2025-12-16 17:45:41.097Z debug(journal): 2: recover_prepare: recovering slot=1023
2025-12-16 17:45:41.114Z debug(vsr): 1: prepare_timeout fired
2025-12-16 17:45:41.114Z debug(vsr): 1: prepare_timeout backing off
2025-12-16 17:45:41.114Z debug(vsr): 1: prepare_timeout after=9..12 (rtt=1 min=1 max=1000 attempts=4)
2025-12-16 17:45:41.114Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-12-16 17:45:41.114Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-12-16 17:45:41.114Z debug(replica): 1N: on_prepare_timeout: replicating to replica 2
2025-12-16 17:45:41.114Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 302117760904335068503740773793085087443, .checksum_padding = 0, .checksum_body = 289517183597369731284421098338909928381, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1408, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 253372602743604193088292026934069770660, .parent_padding = 0, .request_checksum = 309677453850700047477444493576958183194, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 166665032907361157923918867912184359346, .op = 42, .commit = 41, .timestamp = 1765907136122098684, .request = 40, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-16 17:45:41.114Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-16 17:45:41.114Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-16 17:45:41.114Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-16 17:45:41.114Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-16 17:45:41.118Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-12-16 17:45:41.118Z warning(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 0 } error.ConnectionResetByPeer
2025-12-16 17:45:41.118Z info(message_bus): 1: on_recv: from=vsr.Peer{ .client = 166665032907361157923918867912184359346 } orderly shutdown
2025-12-16 17:45:41.118Z warning(message_bus): 0: on_recv: from=vsr.Peer{ .client = 166665032907361157923918867912184359346 } error.ConnectionResetByPeer
warning(message_bus): 166665032907361157923918867912184359346: on_recv: from=vsr.Peer{ .replica = 0 } error.ConnectionResetByPeer
2025-12-16 17:45:41.137Z info(unshare): sandboxed subprocesses exited with signal 11
