ration(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2787741315, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:37.882Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 299750853884830205114464578929734716565, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36032554731698777, .pong_timestamp_wall = 1763916997861046330, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:37.882Z debug(clock): 1: learn: m0=36032554731698777 < window.monotonic=36032557855220109
2025-11-23 16:56:37.883Z debug(client_replies): 1: read_reply: done (client=79827400614827065512247365792344921511 reply=332102806798732276053694882938762732234)
2025-11-23 16:56:37.883Z debug(replica): 1N: on_request: repeat reply (client=79827400614827065512247365792344921511 request=7)
2025-11-23 16:56:37.883Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 332102806798732276053694882938762732234, .checksum_padding = 0, .checksum_body = 163142189146558185265411974777684875497, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 330609638300065542758982022284661833219, .request_checksum_padding = 0, .context = 53205478539481744848174969734668242257, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 9, .commit = 9, .timestamp = 1763916994738402756, .request = 7, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:37.883Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 216880858710212169449066413198821145617, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53205478539481744848174969734668242257, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 8, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3144743737, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:37.883Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:37.883Z debug(replica): 1N: primary_pipeline_prepare: request checksum=216880858710212169449066413198821145617 client=79827400614827065512247365792344921511
2025-11-23 16:56:37.883Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=107213288262357457903348596358977167665 op=10
2025-11-23 16:56:37.883Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:37.883Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:37.883Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:37.883Z debug(replica): 1N: replicate: replicating op=10 to replica 0
2025-11-23 16:56:37.883Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 107213288262357457903348596358977167665, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 289074983437917163251391064115920087310, .parent_padding = 0, .request_checksum = 216880858710212169449066413198821145617, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit = 9, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:37.883Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 107213288262357457903348596358977167665, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 289074983437917163251391064115920087310, .parent_padding = 0, .request_checksum = 216880858710212169449066413198821145617, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit = 9, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:37.883Z debug(replica): 1N: replicate: replicating op=10 to replica 2
2025-11-23 16:56:37.883Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 107213288262357457903348596358977167665, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 289074983437917163251391064115920087310, .parent_padding = 0, .request_checksum = 216880858710212169449066413198821145617, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit = 9, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:37.883Z debug(replica): 1N: on_prepare: advancing: op=9..10 checksum=289074983437917163251391064115920087310..107213288262357457903348596358977167665
2025-11-23 16:56:37.883Z debug(journal): 1: set_header_as_dirty: op=10 checksum=107213288262357457903348596358977167665
2025-11-23 16:56:37.883Z debug(replica): 1N: append: appending to journal op=10
2025-11-23 16:56:37.883Z debug(journal): 1: write: view=1 slot=10 op=10 len=272: 107213288262357457903348596358977167665 starting
2025-11-23 16:56:37.883Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 107213288262357457903348596358977167665, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 289074983437917163251391064115920087310, .parent_padding = 0, .request_checksum = 216880858710212169449066413198821145617, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit = 9, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:37.883Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=10485760 len=4096 locked
2025-11-23 16:56:37.883Z debug(replica): 2n: on_prepare: advancing commit_max=8..9
2025-11-23 16:56:37.883Z debug(replica): 2n: on_prepare: caching prepare.op=10 (commit_min=8 op=9 commit_max=9 prepare_max=1007)
2025-11-23 16:56:37.883Z debug(replica): 2n: on_prepare: advancing: op=9..10 checksum=289074983437917163251391064115920087310..107213288262357457903348596358977167665
2025-11-23 16:56:37.883Z debug(journal): 2: set_header_as_dirty: op=10 checksum=107213288262357457903348596358977167665
2025-11-23 16:56:37.883Z debug(replica): 2n: append: appending to journal op=10
2025-11-23 16:56:37.883Z debug(journal): 2: write: view=1 slot=10 op=10 len=272: 107213288262357457903348596358977167665 starting
2025-11-23 16:56:37.883Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=10485760 len=4096 locked
2025-11-23 16:56:37.883Z debug(replica): 2n: commit_start_journal: cached prepare op=9 checksum=289074983437917163251391064115920087310
2025-11-23 16:56:37.883Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 330609638300065542758982022284661833219, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 278768130874173969049619716301723781946, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2787741315, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:37.883Z debug(replica): 1N: on_request: replying to duplicate request
2025-11-23 16:56:37.883Z debug(client_replies): 1: read_reply: start (client=79827400614827065512247365792344921511 reply=332102806798732276053694882938762732234)
2025-11-23 16:56:37.883Z debug(replica): 2n: repair_prepare: op=10 checksum=107213288262357457903348596358977167665 (already writing)
2025-11-23 16:56:37.883Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=8)
2025-11-23 16:56:37.883Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 261915545299219889874885902868248050564, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 206832859411777617347150160996175741165, .parent_padding = 0, .prepare_checksum = 289074983437917163251391064115920087310, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 9, .commit_min = 8, .timestamp = 1763916994738402756, .request = 7, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:37.883Z debug(replica): 1N: on_prepare_ok: not preparing op=9 checksum=289074983437917163251391064115920087310
2025-11-23 16:56:37.883Z debug(replica): 2n: execute_op: executing view=1 primary=false op=9 checksum=289074983437917163251391064115920087310 (lookup_transfers)
2025-11-23 16:56:37.883Z debug(replica): 2n: execute_op: commit_timestamp=1763916991949944583 prepare.header.timestamp=1763916994738402756
2025-11-23 16:56:37.883Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=7
2025-11-23 16:56:37.883Z debug(forest): entering forest.compact() op=9 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:37.883Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 330609638300065542758982022284661833219, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 278768130874173969049619716301723781946, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2787741315, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:37.883Z debug(replica): 1N: on_request: replying to duplicate request
2025-11-23 16:56:37.883Z debug(client_replies): 1: read_reply: busy (client=79827400614827065512247365792344921511 reply=332102806798732276053694882938762732234)
2025-11-23 16:56:37.883Z debug(replica): 1N: on_request: ignoring (client_replies busy)
2025-11-23 16:56:37.884Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=10485760 len=4096 unlocked
2025-11-23 16:56:37.884Z debug(journal): 1: write_header: op=10 sectors[0..4096]
2025-11-23 16:56:37.884Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:37.884Z debug(client_replies): 1: read_reply: done (client=79827400614827065512247365792344921511 reply=332102806798732276053694882938762732234)
2025-11-23 16:56:37.884Z debug(replica): 1N: on_request: repeat reply (client=79827400614827065512247365792344921511 request=7)
2025-11-23 16:56:37.884Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 332102806798732276053694882938762732234, .checksum_padding = 0, .checksum_body = 163142189146558185265411974777684875497, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 330609638300065542758982022284661833219, .request_checksum_padding = 0, .context = 53205478539481744848174969734668242257, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 9, .commit = 9, .timestamp = 1763916994738402756, .request = 7, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:37.884Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=10485760 len=4096 unlocked
2025-11-23 16:56:37.884Z debug(journal): 2: write_header: op=10 sectors[0..4096]
2025-11-23 16:56:37.884Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:37.884Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=7)
2025-11-23 16:56:37.884Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:37.884Z debug(journal): 1: write: view=1 slot=10 op=10 len=272: 107213288262357457903348596358977167665 complete, marking clean
2025-11-23 16:56:37.884Z debug(replica): 1N: send_prepare_ok: op=10 checksum=107213288262357457903348596358977167665
2025-11-23 16:56:37.884Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 311480107841072268344134373804603522452, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 289074983437917163251391064115920087310, .parent_padding = 0, .prepare_checksum = 107213288262357457903348596358977167665, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit_min = 9, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:37.884Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 311480107841072268344134373804603522452, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 289074983437917163251391064115920087310, .parent_padding = 0, .prepare_checksum = 107213288262357457903348596358977167665, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit_min = 9, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:37.884Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:37.884Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:37.884Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:37.884Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:37.884Z debug(journal): 2: write: view=1 slot=10 op=10 len=272: 107213288262357457903348596358977167665 complete, marking clean
2025-11-23 16:56:37.884Z debug(replica): 2n: send_prepare_ok: op=10 checksum=107213288262357457903348596358977167665
2025-11-23 16:56:37.884Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 83076174683370144950540071517335758899, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 289074983437917163251391064115920087310, .parent_padding = 0, .prepare_checksum = 107213288262357457903348596358977167665, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit_min = 9, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:37.892Z info(supervisor): 2: pausing replica
2025-11-23 16:56:37.893Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:37.893Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:37.902Z info(supervisor): 2: unpausing replica
2025-11-23 16:56:37.902Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:37.902Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:37.912Z info(supervisor): injecting network corruption: testing.vortex.faulty_network.Faults{ .delay = null, .lose = null, .corrupt = 10/100 }
2025-11-23 16:56:37.913Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:37.913Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:37.922Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:37.922Z info(supervisor): sleeping for 1.33s
2025-11-23 16:56:37.922Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:37.922Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:37.922Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:37.927Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 83076174683370144950540071517335758899, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 289074983437917163251391064115920087310, .parent_padding = 0, .prepare_checksum = 107213288262357457903348596358977167665, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit_min = 9, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:37.927Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:37.927Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:37.927Z debug(replica): 1N: on_prepare_ok: quorum received, context=107213288262357457903348596358977167665
2025-11-23 16:56:37.927Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:37.927Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:37.927Z debug(replica): 1N: execute_op: executing view=1 primary=true op=10 checksum=107213288262357457903348596358977167665 (lookup_accounts)
2025-11-23 16:56:37.927Z debug(replica): 1N: execute_op: commit_timestamp=1763916994738402756 prepare.header.timestamp=1763916997883368822
2025-11-23 16:56:37.927Z debug(replica): 1N: execute_op: advancing commit_max=9..10
2025-11-23 16:56:37.927Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=8
2025-11-23 16:56:37.927Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 324573745146683980888868255695458131942, .checksum_padding = 0, .checksum_body = 163142189146558185265411974777684875497, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 216880858710212169449066413198821145617, .request_checksum_padding = 0, .context = 98619470505286076531248246704728814656, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit = 10, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:37.927Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 324573745146683980888868255695458131942, .checksum_padding = 0, .checksum_body = 163142189146558185265411974777684875497, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 216880858710212169449066413198821145617, .request_checksum_padding = 0, .context = 98619470505286076531248246704728814656, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 10, .commit = 10, .timestamp = 1763916997883368822, .request = 8, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:37.927Z debug(forest): entering forest.compact() op=10 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:37.927Z info(workload): accounts created = 0, transfers = 0, pending transfers = 0, commands run = 4
2025-11-23 16:56:37.927Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=8)
2025-11-23 16:56:37.976Z warning(faulty_network): connect failed (0,2): error.ConnectionRefused
2025-11-23 16:56:37.928Z warning(message_bus): 1: on_recv: from=vsr.Peer{ .client = 79827400614827065512247365792344921511 } terminating connection: invalid header_checksum
2025-11-23 16:56:37.928Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 73334104946274688763086079758588185450, .checksum_padding = 0, .checksum_body = 163050585907038219888646303150066310988, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 98619470505286076531248246704728814656, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 9, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 44755573, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:38.069Z warning(faulty_network): connect failed (0,3): error.ConnectionRefused
2025-11-23 16:56:40.724Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:40.724Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 73334104946274688763086079758588185450, .checksum_padding = 0, .checksum_body = 163050585907038219888646303150066310988, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 98619470505286076531248246704728814656, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 9, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 44755573, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:40.724Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.724Z debug(vsr): 1: journal_repair_budget_timeout reset
warning(message_bus): 79827400614827065512247365792344921511: on_recv: from=vsr.Peer{ .replica = 0 } error.ConnectionResetByPeer
2025-11-23 16:56:40.724Z info(supervisor): healing network
2025-11-23 16:56:40.724Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 73334104946274688763086079758588185450, .checksum_padding = 0, .checksum_body = 163050585907038219888646303150066310988, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 98619470505286076531248246704728814656, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 9, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 44755573, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:40.725Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:40.725Z debug(replica): 1N: primary_pipeline_prepare: request checksum=73334104946274688763086079758588185450 client=79827400614827065512247365792344921511
2025-11-23 16:56:40.725Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=191329181731758465889815703540679926747 op=11
2025-11-23 16:56:40.725Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:40.725Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:40.725Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:40.725Z debug(replica): 1N: replicate: replicating op=11 to replica 0
2025-11-23 16:56:40.725Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 191329181731758465889815703540679926747, .checksum_padding = 0, .checksum_body = 163050585907038219888646303150066310988, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107213288262357457903348596358977167665, .parent_padding = 0, .request_checksum = 73334104946274688763086079758588185450, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit = 10, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:40.725Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 191329181731758465889815703540679926747, .checksum_padding = 0, .checksum_body = 163050585907038219888646303150066310988, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107213288262357457903348596358977167665, .parent_padding = 0, .request_checksum = 73334104946274688763086079758588185450, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit = 10, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:40.725Z debug(replica): 1N: replicate: replicating op=11 to replica 2
2025-11-23 16:56:40.725Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 191329181731758465889815703540679926747, .checksum_padding = 0, .checksum_body = 163050585907038219888646303150066310988, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107213288262357457903348596358977167665, .parent_padding = 0, .request_checksum = 73334104946274688763086079758588185450, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit = 10, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:40.725Z debug(replica): 1N: on_prepare: advancing: op=10..11 checksum=107213288262357457903348596358977167665..191329181731758465889815703540679926747
2025-11-23 16:56:40.725Z debug(journal): 1: set_header_as_dirty: op=11 checksum=191329181731758465889815703540679926747
2025-11-23 16:56:40.725Z debug(replica): 1N: append: appending to journal op=11
2025-11-23 16:56:40.725Z debug(journal): 1: write: view=1 slot=11 op=11 len=3712: 191329181731758465889815703540679926747 starting
2025-11-23 16:56:40.725Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=11534336 len=4096 locked
2025-11-23 16:56:40.725Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 191329181731758465889815703540679926747, .checksum_padding = 0, .checksum_body = 163050585907038219888646303150066310988, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107213288262357457903348596358977167665, .parent_padding = 0, .request_checksum = 73334104946274688763086079758588185450, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit = 10, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:40.725Z debug(replica): 2n: on_prepare: advancing commit_max=9..10
2025-11-23 16:56:40.725Z debug(replica): 2n: on_prepare: caching prepare.op=11 (commit_min=9 op=10 commit_max=10 prepare_max=1007)
2025-11-23 16:56:40.725Z debug(replica): 2n: on_prepare: advancing: op=10..11 checksum=107213288262357457903348596358977167665..191329181731758465889815703540679926747
2025-11-23 16:56:40.725Z debug(journal): 2: set_header_as_dirty: op=11 checksum=191329181731758465889815703540679926747
2025-11-23 16:56:40.725Z debug(replica): 2n: append: appending to journal op=11
2025-11-23 16:56:40.725Z debug(journal): 2: write: view=1 slot=11 op=11 len=3712: 191329181731758465889815703540679926747 starting
2025-11-23 16:56:40.725Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=11534336 len=4096 locked
2025-11-23 16:56:40.725Z debug(replica): 2n: commit_start_journal: cached prepare op=10 checksum=107213288262357457903348596358977167665
2025-11-23 16:56:40.725Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=11534336 len=4096 unlocked
2025-11-23 16:56:40.725Z debug(journal): 1: write_header: op=11 sectors[0..4096]
2025-11-23 16:56:40.725Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:40.725Z debug(replica): 2n: repair_prepare: op=11 checksum=191329181731758465889815703540679926747 (already writing)
2025-11-23 16:56:40.725Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=9)
2025-11-23 16:56:40.725Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:40.725Z debug(journal): 1: write: view=1 slot=11 op=11 len=3712: 191329181731758465889815703540679926747 complete, marking clean
2025-11-23 16:56:40.725Z debug(replica): 1N: send_prepare_ok: op=11 checksum=191329181731758465889815703540679926747
2025-11-23 16:56:40.725Z debug(replica): 2n: execute_op: executing view=1 primary=false op=10 checksum=107213288262357457903348596358977167665 (lookup_accounts)
2025-11-23 16:56:40.725Z debug(replica): 2n: execute_op: commit_timestamp=1763916994738402756 prepare.header.timestamp=1763916997883368822
2025-11-23 16:56:40.725Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 30606832049591538602181494181063217129, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107213288262357457903348596358977167665, .parent_padding = 0, .prepare_checksum = 191329181731758465889815703540679926747, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit_min = 10, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:40.725Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=8
2025-11-23 16:56:40.725Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 30606832049591538602181494181063217129, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107213288262357457903348596358977167665, .parent_padding = 0, .prepare_checksum = 191329181731758465889815703540679926747, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit_min = 10, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:40.725Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:40.725Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:40.725Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:40.725Z debug(forest): entering forest.compact() op=10 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:40.725Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=11534336 len=4096 unlocked
2025-11-23 16:56:40.725Z debug(journal): 2: write_header: op=11 sectors[0..4096]
2025-11-23 16:56:40.725Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:40.725Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=8)
2025-11-23 16:56:40.725Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:40.725Z debug(journal): 2: write: view=1 slot=11 op=11 len=3712: 191329181731758465889815703540679926747 complete, marking clean
2025-11-23 16:56:40.725Z debug(replica): 2n: send_prepare_ok: op=11 checksum=191329181731758465889815703540679926747
2025-11-23 16:56:40.725Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 159707905685382348999584594909683865493, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107213288262357457903348596358977167665, .parent_padding = 0, .prepare_checksum = 191329181731758465889815703540679926747, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit_min = 10, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:40.725Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 159707905685382348999584594909683865493, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107213288262357457903348596358977167665, .parent_padding = 0, .prepare_checksum = 191329181731758465889815703540679926747, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit_min = 10, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:40.725Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:40.725Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:40.725Z debug(replica): 1N: on_prepare_ok: quorum received, context=191329181731758465889815703540679926747
2025-11-23 16:56:40.725Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:40.725Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:40.725Z debug(replica): 1N: execute_op: executing view=1 primary=true op=11 checksum=191329181731758465889815703540679926747 (create_accounts)
2025-11-23 16:56:40.725Z debug(replica): 1N: execute_op: commit_timestamp=1763916997883368822 prepare.header.timestamp=1763917000725045121
2025-11-23 16:56:40.726Z debug(replica): 1N: execute_op: advancing commit_max=10..11
2025-11-23 16:56:40.726Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=9
2025-11-23 16:56:40.726Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 135132086014832579472937347860675073250, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 73334104946274688763086079758588185450, .request_checksum_padding = 0, .context = 295010729512431714305147756685912634200, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit = 11, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:40.726Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 135132086014832579472937347860675073250, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 73334104946274688763086079758588185450, .request_checksum_padding = 0, .context = 295010729512431714305147756685912634200, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit = 11, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:40.726Z debug(message_bus): 1: send_message_to_client: no connection to=79827400614827065512247365792344921511
2025-11-23 16:56:40.726Z debug(forest): entering forest.compact() op=11 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:40.726Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=9)
2025-11-23 16:56:40.734Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.734Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.735Z info(supervisor): sleeping for 9.982s
2025-11-23 16:56:40.744Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.744Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.744Z debug(vsr): 1: journal_repair_timeout fired
2025-11-23 16:56:40.744Z debug(vsr): 1: journal_repair_timeout reset
2025-11-23 16:56:40.755Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.755Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.765Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.765Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.775Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.775Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.785Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.785Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.787Z warning(faulty_network): connect failed (0,4): error.ConnectionRefused
2025-11-23 16:56:40.795Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.795Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.805Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.805Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.815Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.815Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.815Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:40.815Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:40.825Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.825Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.825Z debug(vsr): 1: pulse_timeout fired
2025-11-23 16:56:40.825Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:40.835Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.835Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.845Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.845Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.845Z debug(vsr): 1: journal_repair_timeout fired
2025-11-23 16:56:40.845Z debug(vsr): 1: journal_repair_timeout reset
2025-11-23 16:56:40.855Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.855Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.865Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.865Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.875Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.875Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.881Z warning(faulty_network): connect failed (0,5): error.ConnectionRefused
2025-11-23 16:56:40.885Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.885Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.895Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.895Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.905Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.905Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.915Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.915Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.915Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:40.915Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:40.925Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.925Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.925Z debug(vsr): 1: pulse_timeout fired
2025-11-23 16:56:40.925Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:40.935Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.935Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.945Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.945Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.945Z debug(vsr): 1: journal_repair_timeout fired
2025-11-23 16:56:40.945Z debug(vsr): 1: journal_repair_timeout reset
2025-11-23 16:56:40.955Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.955Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.965Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.965Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.975Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.975Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:40.985Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:40.985Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:40.990Z warning(faulty_network): connect failed (0,6): error.ConnectionRefused
2025-11-23 16:56:40.995Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:40.995Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.005Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.005Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.015Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.015Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.015Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:41.015Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:41.026Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.026Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.026Z debug(vsr): 1: pulse_timeout fired
2025-11-23 16:56:41.026Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:41.036Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.036Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.046Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.046Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.046Z debug(vsr): 1: journal_repair_timeout fired
2025-11-23 16:56:41.046Z debug(vsr): 1: journal_repair_timeout reset
2025-11-23 16:56:41.056Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.056Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.066Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.066Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.076Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.076Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.086Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.086Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.090Z warning(faulty_network): connect failed (0,7): error.ConnectionRefused
2025-11-23 16:56:41.096Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.096Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.106Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.106Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.116Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-23 16:56:41.116Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-23 16:56:41.116Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.116Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.116Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:41.116Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:41.116Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-23 16:56:41.116Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-23 16:56:41.126Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.126Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.126Z debug(vsr): 1: pulse_timeout fired
2025-11-23 16:56:41.126Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:41.136Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.136Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.146Z debug(vsr): 1: commit_message_timeout fired
2025-11-23 16:56:41.146Z debug(vsr): 1: commit_message_timeout reset
2025-11-23 16:56:41.146Z debug(replica): 1N: sending commit to replica 0: vsr.message_header.Header.Commit{ .checksum = 63101351234978545187642672994484312750, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 191329181731758465889815703540679926747, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 11, .timestamp_monotonic = 36032561139127201, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.146Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Commit{ .checksum = 63101351234978545187642672994484312750, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 191329181731758465889815703540679926747, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 11, .timestamp_monotonic = 36032561139127201, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.146Z debug(replica): 1N: sending commit to replica 2: vsr.message_header.Header.Commit{ .checksum = 63101351234978545187642672994484312750, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 191329181731758465889815703540679926747, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 11, .timestamp_monotonic = 36032561139127201, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.146Z debug(vsr): 1: start_view_change_message_timeout fired
2025-11-23 16:56:41.146Z debug(vsr): 1: start_view_change_message_timeout reset
2025-11-23 16:56:41.146Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.146Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.146Z debug(vsr): 1: journal_repair_timeout fired
2025-11-23 16:56:41.146Z debug(vsr): 1: journal_repair_timeout reset
2025-11-23 16:56:41.146Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 63101351234978545187642672994484312750, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 191329181731758465889815703540679926747, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 11, .timestamp_monotonic = 36032561139127201, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.146Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-11-23 16:56:41.146Z debug(replica): 2n: on_commit: checksum verified
2025-11-23 16:56:41.146Z debug(replica): 2n: on_commit: advancing commit_max=10..11
2025-11-23 16:56:41.146Z debug(replica): 2n: commit_start_journal: cached prepare op=11 checksum=191329181731758465889815703540679926747
2025-11-23 16:56:41.146Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-11-23 16:56:41.146Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-11-23 16:56:41.146Z debug(replica): 2n: execute_op: executing view=1 primary=false op=11 checksum=191329181731758465889815703540679926747 (create_accounts)
2025-11-23 16:56:41.146Z debug(replica): 2n: execute_op: commit_timestamp=1763916997883368822 prepare.header.timestamp=1763917000725045121
2025-11-23 16:56:41.146Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=9
2025-11-23 16:56:41.146Z debug(forest): entering forest.compact() op=11 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:41.147Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=9)
2025-11-23 16:56:41.156Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.156Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.166Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.166Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.176Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.176Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.180Z warning(faulty_network): connect failed (0,8): error.ConnectionRefused
2025-11-23 16:56:41.186Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.186Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.196Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.196Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.206Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.206Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.216Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.216Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.216Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:41.216Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:41.226Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.226Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.226Z debug(vsr): 1: pulse_timeout fired
2025-11-23 16:56:41.226Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:41.236Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.236Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.246Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.246Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.246Z debug(vsr): 1: journal_repair_timeout fired
2025-11-23 16:56:41.246Z debug(vsr): 1: journal_repair_timeout reset
2025-11-23 16:56:41.256Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.256Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.267Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.267Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.272Z warning(faulty_network): connect failed (0,9): error.ConnectionRefused
2025-11-23 16:56:41.276Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.276Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.287Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.287Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.296Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.296Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.307Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.307Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.316Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.316Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.316Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:41.316Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:41.327Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.327Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.327Z debug(vsr): 1: pulse_timeout fired
2025-11-23 16:56:41.327Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:41.337Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.337Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.343Z info(message_bus): 1: set_and_verify_peer connection from client_likely=79827400614827065512247365792344921511
2025-11-23 16:56:41.343Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 73334104946274688763086079758588185450, .checksum_padding = 0, .checksum_body = 163050585907038219888646303150066310988, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 98619470505286076531248246704728814656, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 9, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 44755573, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.343Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 73334104946274688763086079758588185450, .checksum_padding = 0, .checksum_body = 163050585907038219888646303150066310988, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 98619470505286076531248246704728814656, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 9, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 44755573, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.343Z debug(replica): 2n: on_request: replying to duplicate request
2025-11-23 16:56:41.343Z debug(replica): 1N: on_request: replying to duplicate request
2025-11-23 16:56:41.343Z debug(client_replies): 2: read_reply: start (client=79827400614827065512247365792344921511 reply=135132086014832579472937347860675073250)
2025-11-23 16:56:41.343Z debug(client_replies): 1: read_reply: start (client=79827400614827065512247365792344921511 reply=135132086014832579472937347860675073250)
2025-11-23 16:56:41.343Z debug(client_replies): 2: read_reply: done (client=79827400614827065512247365792344921511 reply=135132086014832579472937347860675073250)
2025-11-23 16:56:41.343Z debug(client_replies): 1: read_reply: done (client=79827400614827065512247365792344921511 reply=135132086014832579472937347860675073250)
2025-11-23 16:56:41.343Z debug(replica): 1N: on_request: repeat reply (client=79827400614827065512247365792344921511 request=9)
2025-11-23 16:56:41.343Z debug(replica): 2n: on_request: repeat reply (client=79827400614827065512247365792344921511 request=9)
2025-11-23 16:56:41.343Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 135132086014832579472937347860675073250, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 73334104946274688763086079758588185450, .request_checksum_padding = 0, .context = 295010729512431714305147756685912634200, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit = 11, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.343Z debug(replica): 2n: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 135132086014832579472937347860675073250, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 73334104946274688763086079758588185450, .request_checksum_padding = 0, .context = 295010729512431714305147756685912634200, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 11, .commit = 11, .timestamp = 1763917000725045121, .request = 9, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
warning(client): 79827400614827065512247365792344921511: on_reply: slow request, request=9 op=11 size=3712 create_accounts time=3415ms
2025-11-23 16:56:41.343Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 226549277384696763202932487195724810478, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 295010729512431714305147756685912634200, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 10, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3415571408, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.343Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:41.343Z debug(replica): 1N: primary_pipeline_prepare: request checksum=226549277384696763202932487195724810478 client=79827400614827065512247365792344921511
2025-11-23 16:56:41.343Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=318391902149817689374666382019463237796 op=12
2025-11-23 16:56:41.343Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:41.344Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:41.344Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:41.344Z debug(replica): 1N: replicate: replicating op=12 to replica 0
2025-11-23 16:56:41.344Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 318391902149817689374666382019463237796, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 191329181731758465889815703540679926747, .parent_padding = 0, .request_checksum = 226549277384696763202932487195724810478, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit = 11, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 318391902149817689374666382019463237796, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 191329181731758465889815703540679926747, .parent_padding = 0, .request_checksum = 226549277384696763202932487195724810478, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit = 11, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(replica): 1N: replicate: replicating op=12 to replica 2
2025-11-23 16:56:41.344Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 318391902149817689374666382019463237796, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 191329181731758465889815703540679926747, .parent_padding = 0, .request_checksum = 226549277384696763202932487195724810478, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit = 11, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(replica): 1N: on_prepare: advancing: op=11..12 checksum=191329181731758465889815703540679926747..318391902149817689374666382019463237796
2025-11-23 16:56:41.344Z debug(journal): 1: set_header_as_dirty: op=12 checksum=318391902149817689374666382019463237796
2025-11-23 16:56:41.344Z debug(replica): 1N: append: appending to journal op=12
2025-11-23 16:56:41.344Z debug(journal): 1: write: view=1 slot=12 op=12 len=688: 318391902149817689374666382019463237796 starting
2025-11-23 16:56:41.344Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 318391902149817689374666382019463237796, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 191329181731758465889815703540679926747, .parent_padding = 0, .request_checksum = 226549277384696763202932487195724810478, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit = 11, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=12582912 len=4096 locked
2025-11-23 16:56:41.344Z debug(replica): 2n: on_prepare: caching prepare.op=12 (commit_min=11 op=11 commit_max=11 prepare_max=1007)
2025-11-23 16:56:41.344Z debug(replica): 2n: on_prepare: advancing: op=11..12 checksum=191329181731758465889815703540679926747..318391902149817689374666382019463237796
2025-11-23 16:56:41.344Z debug(journal): 2: set_header_as_dirty: op=12 checksum=318391902149817689374666382019463237796
2025-11-23 16:56:41.344Z debug(replica): 2n: append: appending to journal op=12
2025-11-23 16:56:41.344Z debug(journal): 2: write: view=1 slot=12 op=12 len=688: 318391902149817689374666382019463237796 starting
2025-11-23 16:56:41.344Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=12582912 len=4096 locked
2025-11-23 16:56:41.344Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=12582912 len=4096 unlocked
2025-11-23 16:56:41.344Z debug(journal): 1: write_header: op=12 sectors[0..4096]
2025-11-23 16:56:41.344Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:41.344Z debug(replica): 2n: repair_prepare: op=12 checksum=318391902149817689374666382019463237796 (already writing)
2025-11-23 16:56:41.344Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:41.344Z debug(journal): 1: write: view=1 slot=12 op=12 len=688: 318391902149817689374666382019463237796 complete, marking clean
2025-11-23 16:56:41.344Z debug(replica): 1N: send_prepare_ok: op=12 checksum=318391902149817689374666382019463237796
2025-11-23 16:56:41.344Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 299906699077366046258168951056739778201, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 191329181731758465889815703540679926747, .parent_padding = 0, .prepare_checksum = 318391902149817689374666382019463237796, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit_min = 11, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 299906699077366046258168951056739778201, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 191329181731758465889815703540679926747, .parent_padding = 0, .prepare_checksum = 318391902149817689374666382019463237796, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit_min = 11, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:41.344Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:41.344Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:41.344Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=12582912 len=4096 unlocked
2025-11-23 16:56:41.344Z debug(journal): 2: write_header: op=12 sectors[0..4096]
2025-11-23 16:56:41.344Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:41.344Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:41.344Z debug(journal): 2: write: view=1 slot=12 op=12 len=688: 318391902149817689374666382019463237796 complete, marking clean
2025-11-23 16:56:41.344Z debug(replica): 2n: send_prepare_ok: op=12 checksum=318391902149817689374666382019463237796
2025-11-23 16:56:41.344Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 251842373480950558070692502766824188982, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 191329181731758465889815703540679926747, .parent_padding = 0, .prepare_checksum = 318391902149817689374666382019463237796, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit_min = 11, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 251842373480950558070692502766824188982, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 191329181731758465889815703540679926747, .parent_padding = 0, .prepare_checksum = 318391902149817689374666382019463237796, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit_min = 11, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:41.344Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:41.344Z debug(replica): 1N: on_prepare_ok: quorum received, context=318391902149817689374666382019463237796
2025-11-23 16:56:41.344Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:41.344Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:41.344Z debug(replica): 1N: execute_op: executing view=1 primary=true op=12 checksum=318391902149817689374666382019463237796 (lookup_accounts)
2025-11-23 16:56:41.344Z debug(replica): 1N: execute_op: commit_timestamp=1763917000725045121 prepare.header.timestamp=1763917001343962403
2025-11-23 16:56:41.344Z debug(replica): 1N: execute_op: advancing commit_max=11..12
2025-11-23 16:56:41.344Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=10
2025-11-23 16:56:41.344Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 267050775468870833531321895583992909217, .checksum_padding = 0, .checksum_body = 20541706246256818811409072326049836037, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 226549277384696763202932487195724810478, .request_checksum_padding = 0, .context = 35820065804276933886142937392769689780, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit = 12, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 267050775468870833531321895583992909217, .checksum_padding = 0, .checksum_body = 20541706246256818811409072326049836037, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 226549277384696763202932487195724810478, .request_checksum_padding = 0, .context = 35820065804276933886142937392769689780, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 12, .commit = 12, .timestamp = 1763917001343962403, .request = 10, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.344Z debug(forest): entering forest.compact() op=12 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:41.344Z info(workload): accounts created = 26, transfers = 0, pending transfers = 0, commands run = 5
2025-11-23 16:56:41.344Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=10)
2025-11-23 16:56:41.347Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:41.347Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:41.347Z debug(vsr): 1: journal_repair_timeout fired
2025-11-23 16:56:41.347Z debug(vsr): 1: journal_repair_timeout reset
2025-11-23 16:56:41.349Z warning(faulty_network): connect failed (0,0): error.ConnectionRefused
warning(message_bus): 79827400614827065512247365792344921511: on_recv: from=vsr.Peer{ .replica = 0 } error.ConnectionResetByPeer
2025-11-23 16:56:41.357Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.357Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.360Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 134204384934572690367255001858076400497, .checksum_padding = 0, .checksum_body = 183067559298728259899308167143862203326, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 35820065804276933886142937392769689780, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 11, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1032257, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:41.360Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:41.360Z debug(replica): 1N: primary_pipeline_prepare: request checksum=134204384934572690367255001858076400497 client=79827400614827065512247365792344921511
2025-11-23 16:56:41.364Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=200488294139296143516296810083299492231 op=13
2025-11-23 16:56:41.364Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:41.364Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:41.364Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:41.364Z debug(replica): 1N: replicate: replicating op=13 to replica 0
2025-11-23 16:56:41.377Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.377Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.397Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.397Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.417Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:41.417Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:41.364Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 200488294139296143516296810083299492231, .checksum_padding = 0, .checksum_body = 183067559298728259899308167143862203326, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 318391902149817689374666382019463237796, .parent_padding = 0, .request_checksum = 134204384934572690367255001858076400497, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit = 12, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:41.417Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:41.446Z warning(faulty_network): connect failed (0,1): error.ConnectionRefused
2025-11-23 16:56:44.286Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:44.286Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 200488294139296143516296810083299492231, .checksum_padding = 0, .checksum_body = 183067559298728259899308167143862203326, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 318391902149817689374666382019463237796, .parent_padding = 0, .request_checksum = 134204384934572690367255001858076400497, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit = 12, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.286Z debug(replica): 1N: replicate: replicating op=13 to replica 2
2025-11-23 16:56:44.286Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 200488294139296143516296810083299492231, .checksum_padding = 0, .checksum_body = 183067559298728259899308167143862203326, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 318391902149817689374666382019463237796, .parent_padding = 0, .request_checksum = 134204384934572690367255001858076400497, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit = 12, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
warning(message_bus): 79827400614827065512247365792344921511: on_recv: from=vsr.Peer{ .replica = 0 } error.ConnectionResetByPeer
2025-11-23 16:56:44.286Z debug(replica): 1N: on_prepare: advancing: op=12..13 checksum=318391902149817689374666382019463237796..200488294139296143516296810083299492231
2025-11-23 16:56:44.286Z debug(journal): 1: set_header_as_dirty: op=13 checksum=200488294139296143516296810083299492231
2025-11-23 16:56:44.286Z debug(replica): 1N: append: appending to journal op=13
2025-11-23 16:56:44.286Z debug(journal): 1: write: view=1 slot=13 op=13 len=963840: 200488294139296143516296810083299492231 starting
2025-11-23 16:56:44.286Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=13631488 len=966656 locked
2025-11-23 16:56:44.291Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 134204384934572690367255001858076400497, .checksum_padding = 0, .checksum_body = 183067559298728259899308167143862203326, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 35820065804276933886142937392769689780, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 11, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1032257, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.291Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:44.291Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:44.291Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=13631488 len=966656 unlocked
2025-11-23 16:56:44.291Z debug(journal): 1: write_header: op=13 sectors[0..4096]
2025-11-23 16:56:44.291Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:44.291Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:44.291Z debug(journal): 1: write: view=1 slot=13 op=13 len=963840: 200488294139296143516296810083299492231 complete, marking clean
2025-11-23 16:56:44.291Z debug(replica): 1N: send_prepare_ok: op=13 checksum=200488294139296143516296810083299492231
2025-11-23 16:56:44.291Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 281669527041020065256442308898449757545, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 318391902149817689374666382019463237796, .parent_padding = 0, .prepare_checksum = 200488294139296143516296810083299492231, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit_min = 12, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.292Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 281669527041020065256442308898449757545, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 318391902149817689374666382019463237796, .parent_padding = 0, .prepare_checksum = 200488294139296143516296810083299492231, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit_min = 12, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.292Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:44.292Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:44.292Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:44.292Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 200488294139296143516296810083299492231, .checksum_padding = 0, .checksum_body = 183067559298728259899308167143862203326, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 318391902149817689374666382019463237796, .parent_padding = 0, .request_checksum = 134204384934572690367255001858076400497, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit = 12, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.293Z debug(replica): 2n: on_prepare: advancing commit_max=11..12
2025-11-23 16:56:44.293Z debug(replica): 2n: on_prepare: caching prepare.op=13 (commit_min=11 op=12 commit_max=12 prepare_max=1007)
2025-11-23 16:56:44.293Z debug(replica): 2n: on_prepare: advancing: op=12..13 checksum=318391902149817689374666382019463237796..200488294139296143516296810083299492231
2025-11-23 16:56:44.293Z debug(journal): 2: set_header_as_dirty: op=13 checksum=200488294139296143516296810083299492231
2025-11-23 16:56:44.293Z debug(replica): 2n: append: appending to journal op=13
2025-11-23 16:56:44.293Z debug(journal): 2: write: view=1 slot=13 op=13 len=963840: 200488294139296143516296810083299492231 starting
2025-11-23 16:56:44.293Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=13631488 len=966656 locked
2025-11-23 16:56:44.293Z debug(replica): 2n: commit_start_journal: cached prepare op=12 checksum=318391902149817689374666382019463237796
2025-11-23 16:56:44.293Z debug(replica): 2n: repair_prepare: op=13 checksum=200488294139296143516296810083299492231 (already writing)
2025-11-23 16:56:44.293Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=11)
2025-11-23 16:56:44.293Z debug(replica): 2n: execute_op: executing view=1 primary=false op=12 checksum=318391902149817689374666382019463237796 (lookup_accounts)
2025-11-23 16:56:44.293Z debug(replica): 2n: execute_op: commit_timestamp=1763917000725045121 prepare.header.timestamp=1763917001343962403
2025-11-23 16:56:44.293Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=10
2025-11-23 16:56:44.293Z debug(forest): entering forest.compact() op=12 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:44.293Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=10)
2025-11-23 16:56:44.294Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=13631488 len=966656 unlocked
2025-11-23 16:56:44.294Z debug(journal): 2: write_header: op=13 sectors[0..4096]
2025-11-23 16:56:44.294Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:44.294Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:44.294Z debug(journal): 2: write: view=1 slot=13 op=13 len=963840: 200488294139296143516296810083299492231 complete, marking clean
2025-11-23 16:56:44.294Z debug(replica): 2n: send_prepare_ok: op=13 checksum=200488294139296143516296810083299492231
2025-11-23 16:56:44.294Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 173374071504036549468028379834882650341, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 318391902149817689374666382019463237796, .parent_padding = 0, .prepare_checksum = 200488294139296143516296810083299492231, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit_min = 12, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.294Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 173374071504036549468028379834882650341, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 318391902149817689374666382019463237796, .parent_padding = 0, .prepare_checksum = 200488294139296143516296810083299492231, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit_min = 12, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.294Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:44.294Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:44.294Z debug(replica): 1N: on_prepare_ok: quorum received, context=200488294139296143516296810083299492231
2025-11-23 16:56:44.294Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:44.294Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:44.296Z debug(replica): 1N: execute_op: executing view=1 primary=true op=13 checksum=200488294139296143516296810083299492231 (create_transfers)
2025-11-23 16:56:44.297Z debug(replica): 1N: execute_op: commit_timestamp=1763917001343962403 prepare.header.timestamp=1763917001360304088
2025-11-23 16:56:44.306Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:44.306Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:44.325Z debug(replica): 1N: execute_op: advancing commit_max=12..13
2025-11-23 16:56:44.325Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=11
2025-11-23 16:56:44.325Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 147282530351311309831261452813110803967, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 134204384934572690367255001858076400497, .request_checksum_padding = 0, .context = 187918461279966917446867478810167905727, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit = 13, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.325Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 147282530351311309831261452813110803967, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 134204384934572690367255001858076400497, .request_checksum_padding = 0, .context = 187918461279966917446867478810167905727, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 13, .commit = 13, .timestamp = 1763917001360304088, .request = 11, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.325Z debug(forest): entering forest.compact() op=13 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 79827400614827065512247365792344921511: on_reply: slow request, request=11 op=13 size=963840 create_transfers time=2974ms
2025-11-23 16:56:44.326Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:44.326Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:44.346Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:44.346Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:44.347Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:44.347Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:44.347Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=11)
2025-11-23 16:56:44.352Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 308212526909558574116337559820601340568, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 187918461279966917446867478810167905727, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 12, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2975019183, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.352Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 308212526909558574116337559820601340568, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 187918461279966917446867478810167905727, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 12, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2975019183, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.352Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:44.352Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:44.352Z debug(replica): 1N: primary_pipeline_prepare: request checksum=308212526909558574116337559820601340568 client=79827400614827065512247365792344921511
2025-11-23 16:56:44.352Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 308212526909558574116337559820601340568, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 187918461279966917446867478810167905727, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 12, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2975019183, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.352Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=89508451443221647386279004801039311926 op=14
2025-11-23 16:56:44.352Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:44.352Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:44.352Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:44.352Z debug(replica): 1N: replicate: replicating op=14 to replica 0
2025-11-23 16:56:44.352Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 89508451443221647386279004801039311926, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 200488294139296143516296810083299492231, .parent_padding = 0, .request_checksum = 308212526909558574116337559820601340568, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit = 13, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.352Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 89508451443221647386279004801039311926, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 200488294139296143516296810083299492231, .parent_padding = 0, .request_checksum = 308212526909558574116337559820601340568, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit = 13, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.352Z debug(replica): 1N: replicate: replicating op=14 to replica 2
2025-11-23 16:56:44.352Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 89508451443221647386279004801039311926, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 200488294139296143516296810083299492231, .parent_padding = 0, .request_checksum = 308212526909558574116337559820601340568, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit = 13, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.352Z debug(replica): 1N: on_prepare: advancing: op=13..14 checksum=200488294139296143516296810083299492231..89508451443221647386279004801039311926
2025-11-23 16:56:44.352Z debug(journal): 1: set_header_as_dirty: op=14 checksum=89508451443221647386279004801039311926
2025-11-23 16:56:44.352Z debug(replica): 1N: append: appending to journal op=14
2025-11-23 16:56:44.352Z debug(journal): 1: write: view=1 slot=14 op=14 len=688: 89508451443221647386279004801039311926 starting
2025-11-23 16:56:44.352Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=14680064 len=4096 locked
2025-11-23 16:56:44.352Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 89508451443221647386279004801039311926, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 200488294139296143516296810083299492231, .parent_padding = 0, .request_checksum = 308212526909558574116337559820601340568, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit = 13, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.352Z debug(replica): 2n: on_prepare: advancing commit_max=12..13
2025-11-23 16:56:44.352Z debug(replica): 2n: on_prepare: caching prepare.op=14 (commit_min=12 op=13 commit_max=13 prepare_max=1007)
2025-11-23 16:56:44.352Z debug(replica): 2n: on_prepare: advancing: op=13..14 checksum=200488294139296143516296810083299492231..89508451443221647386279004801039311926
2025-11-23 16:56:44.352Z debug(journal): 2: set_header_as_dirty: op=14 checksum=89508451443221647386279004801039311926
2025-11-23 16:56:44.352Z debug(replica): 2n: append: appending to journal op=14
2025-11-23 16:56:44.352Z debug(journal): 2: write: view=1 slot=14 op=14 len=688: 89508451443221647386279004801039311926 starting
2025-11-23 16:56:44.352Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=14680064 len=4096 locked
2025-11-23 16:56:44.352Z debug(replica): 2n: commit_start_journal: cached prepare op=13 checksum=200488294139296143516296810083299492231
2025-11-23 16:56:44.353Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 308212526909558574116337559820601340568, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 187918461279966917446867478810167905727, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 12, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2975019183, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.353Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:44.353Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:44.353Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=14680064 len=4096 unlocked
2025-11-23 16:56:44.353Z debug(journal): 1: write_header: op=14 sectors[0..4096]
2025-11-23 16:56:44.353Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:44.353Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:44.353Z debug(journal): 1: write: view=1 slot=14 op=14 len=688: 89508451443221647386279004801039311926 complete, marking clean
2025-11-23 16:56:44.353Z debug(replica): 1N: send_prepare_ok: op=14 checksum=89508451443221647386279004801039311926
2025-11-23 16:56:44.353Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 170657404372035814283762781741148165513, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 200488294139296143516296810083299492231, .parent_padding = 0, .prepare_checksum = 89508451443221647386279004801039311926, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit_min = 13, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.353Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 170657404372035814283762781741148165513, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 200488294139296143516296810083299492231, .parent_padding = 0, .prepare_checksum = 89508451443221647386279004801039311926, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit_min = 13, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.353Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:44.353Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:44.353Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:44.353Z debug(replica): 2n: repair_prepare: op=14 checksum=89508451443221647386279004801039311926 (already writing)
2025-11-23 16:56:44.353Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=12)
2025-11-23 16:56:44.355Z debug(replica): 2n: execute_op: executing view=1 primary=false op=13 checksum=200488294139296143516296810083299492231 (create_transfers)
2025-11-23 16:56:44.355Z debug(replica): 2n: execute_op: commit_timestamp=1763917001343962403 prepare.header.timestamp=1763917001360304088
2025-11-23 16:56:44.367Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:44.367Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:44.370Z warning(faulty_network): connect failed (0,2): error.ConnectionRefused
2025-11-23 16:56:44.387Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:44.387Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:44.390Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=11
2025-11-23 16:56:44.390Z debug(forest): entering forest.compact() op=13 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:44.407Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:44.407Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:44.410Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=14680064 len=4096 unlocked
2025-11-23 16:56:44.410Z debug(journal): 2: write_header: op=14 sectors[0..4096]
2025-11-23 16:56:44.410Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:44.410Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=11)
2025-11-23 16:56:44.410Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:44.410Z debug(journal): 2: write: view=1 slot=14 op=14 len=688: 89508451443221647386279004801039311926 complete, marking clean
2025-11-23 16:56:44.410Z debug(replica): 2n: send_prepare_ok: op=14 checksum=89508451443221647386279004801039311926
2025-11-23 16:56:44.410Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 209619127640015410674188942456698180615, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 200488294139296143516296810083299492231, .parent_padding = 0, .prepare_checksum = 89508451443221647386279004801039311926, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit_min = 13, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.410Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 209619127640015410674188942456698180615, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 200488294139296143516296810083299492231, .parent_padding = 0, .prepare_checksum = 89508451443221647386279004801039311926, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit_min = 13, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.410Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:44.410Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:44.410Z debug(replica): 1N: on_prepare_ok: quorum received, context=89508451443221647386279004801039311926
2025-11-23 16:56:44.410Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:44.410Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:44.410Z debug(replica): 1N: execute_op: executing view=1 primary=true op=14 checksum=89508451443221647386279004801039311926 (lookup_accounts)
2025-11-23 16:56:44.410Z debug(replica): 1N: execute_op: commit_timestamp=1763917001360304088 prepare.header.timestamp=1763917004352666888
2025-11-23 16:56:44.410Z debug(replica): 1N: execute_op: advancing commit_max=13..14
2025-11-23 16:56:44.410Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=12
2025-11-23 16:56:44.410Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 97268344357161574169864628416411762558, .checksum_padding = 0, .checksum_body = 143259974717236660172203379141579371893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 308212526909558574116337559820601340568, .request_checksum_padding = 0, .context = 221562246498020220251397622221884106750, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit = 14, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.410Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 97268344357161574169864628416411762558, .checksum_padding = 0, .checksum_body = 143259974717236660172203379141579371893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 308212526909558574116337559820601340568, .request_checksum_padding = 0, .context = 221562246498020220251397622221884106750, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit = 14, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.410Z debug(forest): entering forest.compact() op=14 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:44.411Z info(workload): accounts created = 26, transfers = 7527, pending transfers = 0, commands run = 6
2025-11-23 16:56:44.411Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=12)
2025-11-23 16:56:44.412Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 250335557400676643353415851648062793587, .checksum_padding = 0, .checksum_body = 172674999538414551823768947348369402292, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 221562246498020220251397622221884106750, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 13, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 58751176, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.412Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:44.412Z debug(replica): 1N: primary_pipeline_prepare: request checksum=250335557400676643353415851648062793587 client=79827400614827065512247365792344921511
2025-11-23 16:56:44.413Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 250335557400676643353415851648062793587, .checksum_padding = 0, .checksum_body = 172674999538414551823768947348369402292, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 221562246498020220251397622221884106750, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 13, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 58751176, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.413Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:44.413Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 250335557400676643353415851648062793587, .checksum_padding = 0, .checksum_body = 172674999538414551823768947348369402292, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 221562246498020220251397622221884106750, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 13, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 58751176, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.413Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=83159850127810897820449594900790233753 op=15
2025-11-23 16:56:44.413Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:44.413Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:44.413Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:44.413Z debug(replica): 1N: replicate: replicating op=15 to replica 0
2025-11-23 16:56:44.413Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 83159850127810897820449594900790233753, .checksum_padding = 0, .checksum_body = 172674999538414551823768947348369402292, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 89508451443221647386279004801039311926, .parent_padding = 0, .request_checksum = 250335557400676643353415851648062793587, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit = 14, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.413Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 83159850127810897820449594900790233753, .checksum_padding = 0, .checksum_body = 172674999538414551823768947348369402292, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 89508451443221647386279004801039311926, .parent_padding = 0, .request_checksum = 250335557400676643353415851648062793587, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit = 14, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.413Z debug(replica): 1N: replicate: replicating op=15 to replica 2
2025-11-23 16:56:44.413Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 83159850127810897820449594900790233753, .checksum_padding = 0, .checksum_body = 172674999538414551823768947348369402292, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 89508451443221647386279004801039311926, .parent_padding = 0, .request_checksum = 250335557400676643353415851648062793587, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit = 14, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.413Z debug(replica): 1N: on_prepare: advancing: op=14..15 checksum=89508451443221647386279004801039311926..83159850127810897820449594900790233753
2025-11-23 16:56:44.413Z debug(journal): 1: set_header_as_dirty: op=15 checksum=83159850127810897820449594900790233753
2025-11-23 16:56:44.413Z debug(replica): 1N: append: appending to journal op=15
2025-11-23 16:56:44.413Z debug(journal): 1: write: view=1 slot=15 op=15 len=120704: 83159850127810897820449594900790233753 starting
2025-11-23 16:56:44.413Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=15728640 len=122880 locked
2025-11-23 16:56:44.414Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 83159850127810897820449594900790233753, .checksum_padding = 0, .checksum_body = 172674999538414551823768947348369402292, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 89508451443221647386279004801039311926, .parent_padding = 0, .request_checksum = 250335557400676643353415851648062793587, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit = 14, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.414Z debug(replica): 2n: on_prepare: advancing commit_max=13..14
2025-11-23 16:56:44.414Z debug(replica): 2n: on_prepare: caching prepare.op=15 (commit_min=13 op=14 commit_max=14 prepare_max=1007)
2025-11-23 16:56:44.414Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 250335557400676643353415851648062793587, .checksum_padding = 0, .checksum_body = 172674999538414551823768947348369402292, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 221562246498020220251397622221884106750, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 13, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 58751176, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.414Z debug(replica): 2n: on_prepare: advancing: op=14..15 checksum=89508451443221647386279004801039311926..83159850127810897820449594900790233753
2025-11-23 16:56:44.414Z debug(journal): 2: set_header_as_dirty: op=15 checksum=83159850127810897820449594900790233753
2025-11-23 16:56:44.414Z debug(replica): 2n: append: appending to journal op=15
2025-11-23 16:56:44.414Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:44.414Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:44.414Z debug(journal): 2: write: view=1 slot=15 op=15 len=120704: 83159850127810897820449594900790233753 starting
2025-11-23 16:56:44.414Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=15728640 len=122880 locked
2025-11-23 16:56:44.414Z debug(replica): 2n: commit_start_journal: cached prepare op=14 checksum=89508451443221647386279004801039311926
2025-11-23 16:56:44.414Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=15728640 len=122880 unlocked
2025-11-23 16:56:44.414Z debug(journal): 1: write_header: op=15 sectors[0..4096]
2025-11-23 16:56:44.414Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:44.414Z debug(replica): 2n: repair_prepare: op=15 checksum=83159850127810897820449594900790233753 (already writing)
2025-11-23 16:56:44.414Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:44.414Z debug(journal): 1: write: view=1 slot=15 op=15 len=120704: 83159850127810897820449594900790233753 complete, marking clean
2025-11-23 16:56:44.414Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=13)
2025-11-23 16:56:44.414Z debug(replica): 1N: send_prepare_ok: op=15 checksum=83159850127810897820449594900790233753
2025-11-23 16:56:44.414Z debug(replica): 2n: execute_op: executing view=1 primary=false op=14 checksum=89508451443221647386279004801039311926 (lookup_accounts)
2025-11-23 16:56:44.414Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 42200400511097736318239686576510648285, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 89508451443221647386279004801039311926, .parent_padding = 0, .prepare_checksum = 83159850127810897820449594900790233753, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit_min = 14, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.414Z debug(replica): 2n: execute_op: commit_timestamp=1763917001360304088 prepare.header.timestamp=1763917004352666888
2025-11-23 16:56:44.414Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 42200400511097736318239686576510648285, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 89508451443221647386279004801039311926, .parent_padding = 0, .prepare_checksum = 83159850127810897820449594900790233753, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit_min = 14, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.414Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:44.414Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:44.414Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:44.414Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=12
2025-11-23 16:56:44.414Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 97268344357161574169864628416411762558, .checksum_padding = 0, .checksum_body = 143259974717236660172203379141579371893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 308212526909558574116337559820601340568, .request_checksum_padding = 0, .context = 221562246498020220251397622221884106750, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit = 14, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.414Z debug(replica): 2n: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 97268344357161574169864628416411762558, .checksum_padding = 0, .checksum_body = 143259974717236660172203379141579371893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 308212526909558574116337559820601340568, .request_checksum_padding = 0, .context = 221562246498020220251397622221884106750, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 14, .commit = 14, .timestamp = 1763917004352666888, .request = 12, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.414Z debug(forest): entering forest.compact() op=14 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:44.414Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=12)
2025-11-23 16:56:44.414Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=15728640 len=122880 unlocked
2025-11-23 16:56:44.414Z debug(journal): 2: write_header: op=15 sectors[0..4096]
2025-11-23 16:56:44.414Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-23 16:56:44.415Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-23 16:56:44.415Z debug(journal): 2: write: view=1 slot=15 op=15 len=120704: 83159850127810897820449594900790233753 complete, marking clean
2025-11-23 16:56:44.415Z debug(replica): 2n: send_prepare_ok: op=15 checksum=83159850127810897820449594900790233753
2025-11-23 16:56:44.415Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 55291652236214014883121112495761700036, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 89508451443221647386279004801039311926, .parent_padding = 0, .prepare_checksum = 83159850127810897820449594900790233753, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit_min = 14, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.420Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:44.420Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:44.427Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:44.427Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:44.427Z debug(vsr): 1: journal_repair_timeout fired
2025-11-23 16:56:44.427Z debug(vsr): 1: journal_repair_timeout reset
2025-11-23 16:56:44.440Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:44.440Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:44.440Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:44.440Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:44.447Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:44.447Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:44.449Z warning(faulty_network): connect failed (0,3): error.ConnectionRefused
2025-11-23 16:56:44.455Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 55291652236214014883121112495761700036, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 89508451443221647386279004801039311926, .parent_padding = 0, .prepare_checksum = 83159850127810897820449594900790233753, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit_min = 14, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.455Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:44.455Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:44.455Z debug(replica): 1N: on_prepare_ok: quorum received, context=83159850127810897820449594900790233753
2025-11-23 16:56:44.455Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:44.455Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:44.457Z debug(replica): 1N: execute_op: executing view=1 primary=true op=15 checksum=83159850127810897820449594900790233753 (lookup_transfers)
2025-11-23 16:56:44.457Z debug(replica): 1N: execute_op: commit_timestamp=1763917004352666888 prepare.header.timestamp=1763917004412972210
2025-11-23 16:56:44.459Z debug(replica): 1N: execute_op: advancing commit_max=14..15
2025-11-23 16:56:44.460Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:44.460Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:44.463Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=13
2025-11-23 16:56:44.464Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 914088184692947594557837993776503648, .checksum_padding = 0, .checksum_body = 208486678968286707441746449029185031679, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250335557400676643353415851648062793587, .request_checksum_padding = 0, .context = 31310629744925657700309551784629365974, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit = 15, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.464Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 914088184692947594557837993776503648, .checksum_padding = 0, .checksum_body = 208486678968286707441746449029185031679, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250335557400676643353415851648062793587, .request_checksum_padding = 0, .context = 31310629744925657700309551784629365974, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit = 15, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.464Z debug(forest): entering forest.compact() op=15 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=true half_beat=false last_beat=false
2025-11-23 16:56:44.465Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=13)
2025-11-23 16:56:44.474Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 87962295106070363320047649100660650180, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31310629744925657700309551784629365974, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 14, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 61663779, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.474Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 87962295106070363320047649100660650180, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31310629744925657700309551784629365974, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 14, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 61663779, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.474Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:44.474Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:44.474Z debug(replica): 1N: primary_pipeline_prepare: request checksum=87962295106070363320047649100660650180 client=79827400614827065512247365792344921511
2025-11-23 16:56:44.474Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 87962295106070363320047649100660650180, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31310629744925657700309551784629365974, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 14, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 61663779, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:44.474Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=179114731084636408487381337565132930226 op=16
2025-11-23 16:56:44.474Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:44.474Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:44.474Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:44.474Z debug(replica): 1N: replicate: replicating op=16 to replica 0
2025-11-23 16:56:44.480Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:44.480Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:44.500Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:44.500Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:44.520Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:44.520Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:44.540Z debug(vsr): 2: ping_timeout fired
2025-11-23 16:56:44.474Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 179114731084636408487381337565132930226, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 83159850127810897820449594900790233753, .parent_padding = 0, .request_checksum = 87962295106070363320047649100660650180, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit = 15, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:44.540Z debug(vsr): 2: ping_timeout reset
2025-11-23 16:56:44.547Z warning(faulty_network): connect failed (0,4): error.ConnectionRefused
2025-11-23 16:56:46.933Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 179114731084636408487381337565132930226, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 83159850127810897820449594900790233753, .parent_padding = 0, .request_checksum = 87962295106070363320047649100660650180, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit = 15, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.933Z debug(replica): 1N: replicate: replicating op=16 to replica 2
2025-11-23 16:56:46.933Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 179114731084636408487381337565132930226, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 83159850127810897820449594900790233753, .parent_padding = 0, .request_checksum = 87962295106070363320047649100660650180, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit = 15, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.933Z debug(replica): 2n: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 72097076019039024122878700572209086097, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36032566926616691, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.933Z debug(message_bus): 2: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Ping{ .checksum = 72097076019039024122878700572209086097, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36032566926616691, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.934Z debug(replica): 1N: on_prepare: advancing: op=15..16 checksum=83159850127810897820449594900790233753..179114731084636408487381337565132930226
2025-11-23 16:56:46.934Z debug(journal): 1: set_header_as_dirty: op=16 checksum=179114731084636408487381337565132930226
2025-11-23 16:56:46.934Z debug(replica): 1N: append: appending to journal op=16
2025-11-23 16:56:46.934Z debug(replica): 2n: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 72097076019039024122878700572209086097, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36032566926616691, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.934Z debug(journal): 1: write: view=1 slot=16 op=16 len=688: 179114731084636408487381337565132930226 starting
2025-11-23 16:56:46.934Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=16777216 len=4096 locked
2025-11-23 16:56:46.934Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-23 16:56:46.934Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-23 16:56:46.934Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:46.934Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:46.934Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:46.934Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:46.934Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-23 16:56:46.934Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-23 16:56:46.934Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 87962295106070363320047649100660650180, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31310629744925657700309551784629365974, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 14, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 61663779, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.934Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 179114731084636408487381337565132930226, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 83159850127810897820449594900790233753, .parent_padding = 0, .request_checksum = 87962295106070363320047649100660650180, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit = 15, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.934Z debug(replica): 2n: on_prepare: advancing commit_max=14..15
2025-11-23 16:56:46.934Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:46.934Z debug(replica): 2n: on_prepare: caching prepare.op=16 (commit_min=14 op=15 commit_max=15 prepare_max=1007)
2025-11-23 16:56:46.934Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:46.934Z debug(replica): 2n: on_prepare: advancing: op=15..16 checksum=83159850127810897820449594900790233753..179114731084636408487381337565132930226
2025-11-23 16:56:46.934Z debug(journal): 2: set_header_as_dirty: op=16 checksum=179114731084636408487381337565132930226
2025-11-23 16:56:46.934Z debug(replica): 2n: append: appending to journal op=16
2025-11-23 16:56:46.934Z debug(journal): 2: write: view=1 slot=16 op=16 len=688: 179114731084636408487381337565132930226 starting
2025-11-23 16:56:46.934Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=16777216 len=4096 locked
2025-11-23 16:56:46.934Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 87962295106070363320047649100660650180, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31310629744925657700309551784629365974, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 14, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 61663779, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.934Z debug(replica): 2n: commit_start_journal: cached prepare op=15 checksum=83159850127810897820449594900790233753
2025-11-23 16:56:46.934Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:46.934Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:46.934Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=16777216 len=4096 unlocked
2025-11-23 16:56:46.934Z debug(journal): 1: write_header: op=16 sectors[4096..8192]
2025-11-23 16:56:46.934Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:46.934Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 72097076019039024122878700572209086097, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36032566926616691, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.934Z debug(replica): 1N: sending pong to replica 2: vsr.message_header.Header.Pong{ .checksum = 54856910279408527002455547411499503340, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36032566926616691, .pong_timestamp_wall = 1763917006934449567, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.934Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:46.934Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:46.934Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:46.934Z debug(journal): 1: write: view=1 slot=16 op=16 len=688: 179114731084636408487381337565132930226 complete, marking clean
2025-11-23 16:56:46.934Z debug(replica): 1N: send_prepare_ok: op=16 checksum=179114731084636408487381337565132930226
2025-11-23 16:56:46.934Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 143281228568560032318616267763641515499, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 83159850127810897820449594900790233753, .parent_padding = 0, .prepare_checksum = 179114731084636408487381337565132930226, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit_min = 15, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.934Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 143281228568560032318616267763641515499, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 83159850127810897820449594900790233753, .parent_padding = 0, .prepare_checksum = 179114731084636408487381337565132930226, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit_min = 15, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.934Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:46.934Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:46.934Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:46.936Z debug(replica): 2n: repair_prepare: op=16 checksum=179114731084636408487381337565132930226 (already writing)
2025-11-23 16:56:46.936Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=14)
2025-11-23 16:56:46.936Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 87962295106070363320047649100660650180, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31310629744925657700309551784629365974, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 14, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 61663779, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.936Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:46.936Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 87962295106070363320047649100660650180, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31310629744925657700309551784629365974, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 14, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 61663779, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.936Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 87962295106070363320047649100660650180, .checksum_padding = 0, .checksum_body = 157997281711300960983866016433994989046, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 688, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 31310629744925657700309551784629365974, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 14, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 61663779, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.936Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:46.936Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:46.936Z debug(replica): 2n: execute_op: executing view=1 primary=false op=15 checksum=83159850127810897820449594900790233753 (lookup_transfers)
2025-11-23 16:56:46.936Z debug(replica): 2n: execute_op: commit_timestamp=1763917004352666888 prepare.header.timestamp=1763917004412972210
2025-11-23 16:56:46.942Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=13
2025-11-23 16:56:46.942Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 914088184692947594557837993776503648, .checksum_padding = 0, .checksum_body = 208486678968286707441746449029185031679, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250335557400676643353415851648062793587, .request_checksum_padding = 0, .context = 31310629744925657700309551784629365974, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit = 15, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.942Z debug(replica): 2n: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 914088184692947594557837993776503648, .checksum_padding = 0, .checksum_body = 208486678968286707441746449029185031679, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 963840, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250335557400676643353415851648062793587, .request_checksum_padding = 0, .context = 31310629744925657700309551784629365974, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 15, .commit = 15, .timestamp = 1763917004412972210, .request = 13, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.942Z debug(forest): entering forest.compact() op=15 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=true half_beat=false last_beat=false
2025-11-23 16:56:46.943Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Pong{ .checksum = 54856910279408527002455547411499503340, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36032566926616691, .pong_timestamp_wall = 1763917006934449567, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.943Z debug(clock): 2: learn: replica=1 m0=36032566926616691 t1=1763917006934449567 m2=36032566935899847 t2=1763917006943227045 one_way_delay=4641578 asymmetric_delay=0 clock_offset=-4135900
2025-11-23 16:56:46.943Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=16777216 len=4096 unlocked
2025-11-23 16:56:46.943Z debug(journal): 2: write_header: op=16 sectors[4096..8192]
2025-11-23 16:56:46.943Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:46.943Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=13)
2025-11-23 16:56:46.943Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:46.943Z debug(journal): 2: write: view=1 slot=16 op=16 len=688: 179114731084636408487381337565132930226 complete, marking clean
2025-11-23 16:56:46.943Z debug(replica): 2n: send_prepare_ok: op=16 checksum=179114731084636408487381337565132930226
2025-11-23 16:56:46.943Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 164313150265783282988636189936289785497, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 83159850127810897820449594900790233753, .parent_padding = 0, .prepare_checksum = 179114731084636408487381337565132930226, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit_min = 15, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.944Z debug(clock): 2: synchronized: truechimers=2/3 clock_offset=0ns..0ns accuracy=0ns
2025-11-23 16:56:46.944Z debug(clock): 2: system time is 441ns behind
2025-11-23 16:56:46.954Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:46.954Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:46.954Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:46.954Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:46.974Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:46.974Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:46.974Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:46.974Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:46.983Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 164313150265783282988636189936289785497, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 83159850127810897820449594900790233753, .parent_padding = 0, .prepare_checksum = 179114731084636408487381337565132930226, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit_min = 15, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.983Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:46.983Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:46.983Z debug(replica): 1N: on_prepare_ok: quorum received, context=179114731084636408487381337565132930226
2025-11-23 16:56:46.983Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:46.983Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:46.983Z debug(replica): 1N: execute_op: executing view=1 primary=true op=16 checksum=179114731084636408487381337565132930226 (lookup_accounts)
2025-11-23 16:56:46.983Z debug(replica): 1N: execute_op: commit_timestamp=1763917004412972210 prepare.header.timestamp=1763917004474507750
2025-11-23 16:56:46.983Z debug(replica): 1N: execute_op: advancing commit_max=15..16
2025-11-23 16:56:46.983Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=14
2025-11-23 16:56:46.983Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 279202013423942051401261121159698967269, .checksum_padding = 0, .checksum_body = 143259974717236660172203379141579371893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 87962295106070363320047649100660650180, .request_checksum_padding = 0, .context = 260725015149624319639359368747795691048, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit = 16, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.983Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 279202013423942051401261121159698967269, .checksum_padding = 0, .checksum_body = 143259974717236660172203379141579371893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 87962295106070363320047649100660650180, .request_checksum_padding = 0, .context = 260725015149624319639359368747795691048, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit = 16, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.983Z debug(forest): entering forest.compact() op=16 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=true last_beat=false
warning(client): 79827400614827065512247365792344921511: on_reply: slow request, request=14 op=16 size=688 lookup_accounts time=2509ms
2025-11-23 16:56:46.983Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=14)
2025-11-23 16:56:46.984Z info(workload): accounts created = 26, transfers = 7527, pending transfers = 0, commands run = 7
2025-11-23 16:56:46.984Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 60286435923466136774376777941627219779, .checksum_padding = 0, .checksum_body = 40017154806398019268978434823162390870, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2816, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 260725015149624319639359368747795691048, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 15, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2509632467, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.984Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:46.984Z debug(replica): 1N: primary_pipeline_prepare: request checksum=60286435923466136774376777941627219779 client=79827400614827065512247365792344921511
2025-11-23 16:56:46.984Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=10489267836669721394379356864799458902 op=17
2025-11-23 16:56:46.984Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:46.984Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:46.984Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:46.984Z debug(replica): 1N: replicate: replicating op=17 to replica 0
2025-11-23 16:56:46.984Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 10489267836669721394379356864799458902, .checksum_padding = 0, .checksum_body = 40017154806398019268978434823162390870, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2816, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 179114731084636408487381337565132930226, .parent_padding = 0, .request_checksum = 60286435923466136774376777941627219779, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit = 16, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.984Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 10489267836669721394379356864799458902, .checksum_padding = 0, .checksum_body = 40017154806398019268978434823162390870, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2816, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 179114731084636408487381337565132930226, .parent_padding = 0, .request_checksum = 60286435923466136774376777941627219779, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit = 16, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.984Z debug(replica): 1N: replicate: replicating op=17 to replica 2
2025-11-23 16:56:46.984Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 10489267836669721394379356864799458902, .checksum_padding = 0, .checksum_body = 40017154806398019268978434823162390870, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2816, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 179114731084636408487381337565132930226, .parent_padding = 0, .request_checksum = 60286435923466136774376777941627219779, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit = 16, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.984Z debug(replica): 1N: on_prepare: advancing: op=16..17 checksum=179114731084636408487381337565132930226..10489267836669721394379356864799458902
2025-11-23 16:56:46.984Z debug(journal): 1: set_header_as_dirty: op=17 checksum=10489267836669721394379356864799458902
2025-11-23 16:56:46.984Z debug(replica): 1N: append: appending to journal op=17
2025-11-23 16:56:46.984Z debug(journal): 1: write: view=1 slot=17 op=17 len=2816: 10489267836669721394379356864799458902 starting
2025-11-23 16:56:46.984Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=17825792 len=4096 locked
2025-11-23 16:56:46.984Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=17825792 len=4096 unlocked
2025-11-23 16:56:46.984Z debug(journal): 1: write_header: op=17 sectors[4096..8192]
2025-11-23 16:56:46.984Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:46.984Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:46.984Z debug(journal): 1: write: view=1 slot=17 op=17 len=2816: 10489267836669721394379356864799458902 complete, marking clean
2025-11-23 16:56:46.984Z debug(replica): 1N: send_prepare_ok: op=17 checksum=10489267836669721394379356864799458902
2025-11-23 16:56:46.984Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 32224710845313225593166352291863401135, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 179114731084636408487381337565132930226, .parent_padding = 0, .prepare_checksum = 10489267836669721394379356864799458902, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit_min = 16, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.984Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 32224710845313225593166352291863401135, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 179114731084636408487381337565132930226, .parent_padding = 0, .prepare_checksum = 10489267836669721394379356864799458902, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit_min = 16, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.984Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:46.984Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:46.984Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:46.984Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 10489267836669721394379356864799458902, .checksum_padding = 0, .checksum_body = 40017154806398019268978434823162390870, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2816, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 179114731084636408487381337565132930226, .parent_padding = 0, .request_checksum = 60286435923466136774376777941627219779, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit = 16, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.984Z debug(replica): 2n: on_prepare: advancing commit_max=15..16
2025-11-23 16:56:46.984Z debug(replica): 2n: on_prepare: caching prepare.op=17 (commit_min=15 op=16 commit_max=16 prepare_max=1007)
2025-11-23 16:56:46.984Z debug(replica): 2n: on_prepare: advancing: op=16..17 checksum=179114731084636408487381337565132930226..10489267836669721394379356864799458902
2025-11-23 16:56:46.984Z debug(journal): 2: set_header_as_dirty: op=17 checksum=10489267836669721394379356864799458902
2025-11-23 16:56:46.984Z debug(replica): 2n: append: appending to journal op=17
2025-11-23 16:56:46.984Z debug(journal): 2: write: view=1 slot=17 op=17 len=2816: 10489267836669721394379356864799458902 starting
2025-11-23 16:56:46.984Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=17825792 len=4096 locked
2025-11-23 16:56:46.984Z debug(replica): 2n: commit_start_journal: cached prepare op=16 checksum=179114731084636408487381337565132930226
2025-11-23 16:56:46.984Z debug(replica): 2n: repair_prepare: op=17 checksum=10489267836669721394379356864799458902 (already writing)
2025-11-23 16:56:46.985Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=15)
2025-11-23 16:56:46.985Z debug(replica): 2n: execute_op: executing view=1 primary=false op=16 checksum=179114731084636408487381337565132930226 (lookup_accounts)
2025-11-23 16:56:46.985Z debug(replica): 2n: execute_op: commit_timestamp=1763917004412972210 prepare.header.timestamp=1763917004474507750
2025-11-23 16:56:46.985Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=14
2025-11-23 16:56:46.985Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 279202013423942051401261121159698967269, .checksum_padding = 0, .checksum_body = 143259974717236660172203379141579371893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 87962295106070363320047649100660650180, .request_checksum_padding = 0, .context = 260725015149624319639359368747795691048, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit = 16, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.985Z debug(replica): 2n: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 279202013423942051401261121159698967269, .checksum_padding = 0, .checksum_body = 143259974717236660172203379141579371893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 3712, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 87962295106070363320047649100660650180, .request_checksum_padding = 0, .context = 260725015149624319639359368747795691048, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 16, .commit = 16, .timestamp = 1763917004474507750, .request = 14, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.985Z debug(forest): entering forest.compact() op=16 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=true last_beat=false
2025-11-23 16:56:46.985Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=17825792 len=4096 unlocked
2025-11-23 16:56:46.985Z debug(journal): 2: write_header: op=17 sectors[4096..8192]
2025-11-23 16:56:46.985Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:46.985Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=14)
2025-11-23 16:56:46.985Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:46.985Z debug(journal): 2: write: view=1 slot=17 op=17 len=2816: 10489267836669721394379356864799458902 complete, marking clean
2025-11-23 16:56:46.985Z debug(replica): 2n: send_prepare_ok: op=17 checksum=10489267836669721394379356864799458902
2025-11-23 16:56:46.985Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 5907224608233051033102779681344994564, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 179114731084636408487381337565132930226, .parent_padding = 0, .prepare_checksum = 10489267836669721394379356864799458902, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit_min = 16, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.985Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 5907224608233051033102779681344994564, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 179114731084636408487381337565132930226, .parent_padding = 0, .prepare_checksum = 10489267836669721394379356864799458902, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit_min = 16, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.985Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:46.985Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:46.985Z debug(replica): 1N: on_prepare_ok: quorum received, context=10489267836669721394379356864799458902
2025-11-23 16:56:46.985Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:46.985Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:46.985Z debug(replica): 1N: execute_op: executing view=1 primary=true op=17 checksum=10489267836669721394379356864799458902 (create_accounts)
2025-11-23 16:56:46.985Z debug(replica): 1N: execute_op: commit_timestamp=1763917004474507750 prepare.header.timestamp=1763917006984343366
2025-11-23 16:56:46.985Z debug(replica): 1N: execute_op: advancing commit_max=16..17
2025-11-23 16:56:46.985Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=15
2025-11-23 16:56:46.985Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 119872786320773969330691089304969102847, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 60286435923466136774376777941627219779, .request_checksum_padding = 0, .context = 1122221954865865024259456597832090968, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit = 17, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.985Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 119872786320773969330691089304969102847, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 60286435923466136774376777941627219779, .request_checksum_padding = 0, .context = 1122221954865865024259456597832090968, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit = 17, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.985Z debug(forest): entering forest.compact() op=17 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:46.985Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=15)
2025-11-23 16:56:46.986Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 2503818590129678907945532305632594236, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 1122221954865865024259456597832090968, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 16, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1637916, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.986Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:46.986Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 2503818590129678907945532305632594236, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 1122221954865865024259456597832090968, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 16, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1637916, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.986Z debug(replica): 1N: primary_pipeline_prepare: request checksum=2503818590129678907945532305632594236 client=79827400614827065512247365792344921511
2025-11-23 16:56:46.986Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:46.986Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=82395064191550750709630479547427776524 op=18
2025-11-23 16:56:46.986Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 2503818590129678907945532305632594236, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 1122221954865865024259456597832090968, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 16, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1637916, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.986Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:46.986Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:46.986Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:46.986Z debug(replica): 1N: replicate: replicating op=18 to replica 0
2025-11-23 16:56:46.986Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 82395064191550750709630479547427776524, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 10489267836669721394379356864799458902, .parent_padding = 0, .request_checksum = 2503818590129678907945532305632594236, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit = 17, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.986Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 82395064191550750709630479547427776524, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 10489267836669721394379356864799458902, .parent_padding = 0, .request_checksum = 2503818590129678907945532305632594236, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit = 17, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.986Z debug(replica): 1N: replicate: replicating op=18 to replica 2
2025-11-23 16:56:46.986Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 82395064191550750709630479547427776524, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 10489267836669721394379356864799458902, .parent_padding = 0, .request_checksum = 2503818590129678907945532305632594236, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit = 17, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.986Z debug(replica): 1N: on_prepare: advancing: op=17..18 checksum=10489267836669721394379356864799458902..82395064191550750709630479547427776524
2025-11-23 16:56:46.986Z debug(journal): 1: set_header_as_dirty: op=18 checksum=82395064191550750709630479547427776524
2025-11-23 16:56:46.986Z debug(replica): 1N: append: appending to journal op=18
2025-11-23 16:56:46.986Z debug(journal): 1: write: view=1 slot=18 op=18 len=992: 82395064191550750709630479547427776524 starting
2025-11-23 16:56:46.986Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 82395064191550750709630479547427776524, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 10489267836669721394379356864799458902, .parent_padding = 0, .request_checksum = 2503818590129678907945532305632594236, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit = 17, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.986Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=18874368 len=4096 locked
2025-11-23 16:56:46.986Z debug(replica): 2n: on_prepare: advancing commit_max=16..17
2025-11-23 16:56:46.986Z debug(replica): 2n: on_prepare: caching prepare.op=18 (commit_min=16 op=17 commit_max=17 prepare_max=1007)
2025-11-23 16:56:46.986Z debug(replica): 2n: on_prepare: advancing: op=17..18 checksum=10489267836669721394379356864799458902..82395064191550750709630479547427776524
2025-11-23 16:56:46.986Z debug(journal): 2: set_header_as_dirty: op=18 checksum=82395064191550750709630479547427776524
2025-11-23 16:56:46.986Z debug(replica): 2n: append: appending to journal op=18
2025-11-23 16:56:46.986Z debug(journal): 2: write: view=1 slot=18 op=18 len=992: 82395064191550750709630479547427776524 starting
2025-11-23 16:56:46.986Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=18874368 len=4096 locked
2025-11-23 16:56:46.986Z debug(replica): 2n: commit_start_journal: cached prepare op=17 checksum=10489267836669721394379356864799458902
2025-11-23 16:56:46.986Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 2503818590129678907945532305632594236, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 1122221954865865024259456597832090968, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 16, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1637916, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:46.986Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:46.986Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:46.986Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=18874368 len=4096 unlocked
2025-11-23 16:56:46.986Z debug(journal): 1: write_header: op=18 sectors[4096..8192]
2025-11-23 16:56:46.986Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:46.986Z debug(replica): 2n: repair_prepare: op=18 checksum=82395064191550750709630479547427776524 (already writing)
2025-11-23 16:56:46.986Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=16)
2025-11-23 16:56:46.986Z debug(replica): 2n: execute_op: executing view=1 primary=false op=17 checksum=10489267836669721394379356864799458902 (create_accounts)
2025-11-23 16:56:46.986Z debug(replica): 2n: execute_op: commit_timestamp=1763917004474507750 prepare.header.timestamp=1763917006984343366
2025-11-23 16:56:46.986Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:46.986Z debug(journal): 1: write: view=1 slot=18 op=18 len=992: 82395064191550750709630479547427776524 complete, marking clean
2025-11-23 16:56:46.986Z debug(replica): 1N: send_prepare_ok: op=18 checksum=82395064191550750709630479547427776524
2025-11-23 16:56:46.986Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=15
2025-11-23 16:56:47.039Z warning(faulty_network): connect failed (0,5): error.ConnectionRefused
warning(message_bus): 79827400614827065512247365792344921511: on_recv: from=vsr.Peer{ .replica = 0 } error.ConnectionResetByPeer
2025-11-23 16:56:47.116Z warning(faulty_network): connect failed (0,6): error.ConnectionRefused
2025-11-23 16:56:47.195Z warning(faulty_network): connect failed (0,7): error.ConnectionRefused
2025-11-23 16:56:47.253Z warning(faulty_network): connect failed (0,8): error.ConnectionRefused
2025-11-23 16:56:46.986Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 326595550019166254095735684930219203442, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 10489267836669721394379356864799458902, .parent_padding = 0, .prepare_checksum = 82395064191550750709630479547427776524, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit_min = 17, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:46.986Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 119872786320773969330691089304969102847, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 60286435923466136774376777941627219779, .request_checksum_padding = 0, .context = 1122221954865865024259456597832090968, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit = 17, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:47.346Z warning(faulty_network): connect failed (0,9): error.ConnectionRefused
2025-11-23 16:56:49.842Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 326595550019166254095735684930219203442, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 10489267836669721394379356864799458902, .parent_padding = 0, .prepare_checksum = 82395064191550750709630479547427776524, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit_min = 17, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.842Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:49.842Z debug(replica): 2n: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 119872786320773969330691089304969102847, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 60286435923466136774376777941627219779, .request_checksum_padding = 0, .context = 1122221954865865024259456597832090968, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 17, .commit = 17, .timestamp = 1763917006984343366, .request = 15, .operation = vsr.Operation(138), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.842Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:49.842Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:49.842Z debug(forest): entering forest.compact() op=17 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:49.842Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:49.842Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:49.842Z debug(vsr): 1: journal_repair_timeout fired
2025-11-23 16:56:49.842Z debug(vsr): 1: journal_repair_timeout reset
2025-11-23 16:56:49.842Z warning(replica): 2n: commit_dispatch: slow request, request=15 size=2816 create_accounts time=2856ms
2025-11-23 16:56:49.842Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 2503818590129678907945532305632594236, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 1122221954865865024259456597832090968, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 16, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1637916, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.842Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:49.842Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:49.842Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 2503818590129678907945532305632594236, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 1122221954865865024259456597832090968, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 16, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1637916, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.842Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:49.842Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 2503818590129678907945532305632594236, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 1122221954865865024259456597832090968, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 16, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1637916, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.842Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:49.842Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:49.842Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 2503818590129678907945532305632594236, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 1122221954865865024259456597832090968, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 16, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1637916, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.842Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:49.842Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=18874368 len=4096 unlocked
2025-11-23 16:56:49.842Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:49.842Z debug(journal): 2: write_header: op=18 sectors[4096..8192]
2025-11-23 16:56:49.842Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:49.842Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=15)
2025-11-23 16:56:49.842Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:49.842Z debug(journal): 2: write: view=1 slot=18 op=18 len=992: 82395064191550750709630479547427776524 complete, marking clean
2025-11-23 16:56:49.842Z debug(replica): 2n: send_prepare_ok: op=18 checksum=82395064191550750709630479547427776524
2025-11-23 16:56:49.842Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 333118987131435569822148847326884578851, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 10489267836669721394379356864799458902, .parent_padding = 0, .prepare_checksum = 82395064191550750709630479547427776524, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit_min = 17, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.843Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 333118987131435569822148847326884578851, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 10489267836669721394379356864799458902, .parent_padding = 0, .prepare_checksum = 82395064191550750709630479547427776524, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit_min = 17, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.843Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:49.843Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:49.843Z debug(replica): 1N: on_prepare_ok: quorum received, context=82395064191550750709630479547427776524
2025-11-23 16:56:49.843Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:49.843Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:49.843Z debug(replica): 1N: execute_op: executing view=1 primary=true op=18 checksum=82395064191550750709630479547427776524 (lookup_accounts)
2025-11-23 16:56:49.843Z debug(replica): 1N: execute_op: commit_timestamp=1763917006984343366 prepare.header.timestamp=1763917006986079722
2025-11-23 16:56:49.843Z debug(replica): 1N: execute_op: advancing commit_max=17..18
2025-11-23 16:56:49.843Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=16
2025-11-23 16:56:49.843Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 59243789126604879216968159618869275124, .checksum_padding = 0, .checksum_body = 8741230178737984251481288993886729428, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6144, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 2503818590129678907945532305632594236, .request_checksum_padding = 0, .context = 92357314863631872240181264541742814050, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit = 18, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.843Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 59243789126604879216968159618869275124, .checksum_padding = 0, .checksum_body = 8741230178737984251481288993886729428, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6144, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 2503818590129678907945532305632594236, .request_checksum_padding = 0, .context = 92357314863631872240181264541742814050, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit = 18, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.843Z debug(forest): entering forest.compact() op=18 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 79827400614827065512247365792344921511: on_reply: slow request, request=16 op=18 size=992 lookup_accounts time=2857ms
2025-11-23 16:56:49.843Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=16)
2025-11-23 16:56:49.843Z info(workload): accounts created = 45, transfers = 7527, pending transfers = 0, commands run = 8
2025-11-23 16:56:49.847Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 9535321764616651542623787573095955677, .checksum_padding = 0, .checksum_body = 325751320088309704533442737908049459658, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 245376, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 92357314863631872240181264541742814050, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 17, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2857530738, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.847Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:49.847Z debug(replica): 1N: primary_pipeline_prepare: request checksum=9535321764616651542623787573095955677 client=79827400614827065512247365792344921511
2025-11-23 16:56:49.847Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 9535321764616651542623787573095955677, .checksum_padding = 0, .checksum_body = 325751320088309704533442737908049459658, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 245376, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 92357314863631872240181264541742814050, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 17, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2857530738, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.847Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:49.847Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 9535321764616651542623787573095955677, .checksum_padding = 0, .checksum_body = 325751320088309704533442737908049459658, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 245376, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 92357314863631872240181264541742814050, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 17, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2857530738, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.848Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=57114187066127147925107258441335472935 op=19
2025-11-23 16:56:49.848Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:49.848Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:49.848Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:49.848Z debug(replica): 1N: replicate: replicating op=19 to replica 0
2025-11-23 16:56:49.848Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 57114187066127147925107258441335472935, .checksum_padding = 0, .checksum_body = 325751320088309704533442737908049459658, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 245376, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 82395064191550750709630479547427776524, .parent_padding = 0, .request_checksum = 9535321764616651542623787573095955677, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit = 18, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.848Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 57114187066127147925107258441335472935, .checksum_padding = 0, .checksum_body = 325751320088309704533442737908049459658, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 245376, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 82395064191550750709630479547427776524, .parent_padding = 0, .request_checksum = 9535321764616651542623787573095955677, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit = 18, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.848Z debug(replica): 1N: replicate: replicating op=19 to replica 2
2025-11-23 16:56:49.848Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 57114187066127147925107258441335472935, .checksum_padding = 0, .checksum_body = 325751320088309704533442737908049459658, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 245376, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 82395064191550750709630479547427776524, .parent_padding = 0, .request_checksum = 9535321764616651542623787573095955677, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit = 18, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.848Z debug(replica): 1N: on_prepare: advancing: op=18..19 checksum=82395064191550750709630479547427776524..57114187066127147925107258441335472935
2025-11-23 16:56:49.848Z debug(journal): 1: set_header_as_dirty: op=19 checksum=57114187066127147925107258441335472935
2025-11-23 16:56:49.848Z debug(replica): 1N: append: appending to journal op=19
2025-11-23 16:56:49.848Z debug(journal): 1: write: view=1 slot=19 op=19 len=245376: 57114187066127147925107258441335472935 starting
2025-11-23 16:56:49.848Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=19922944 len=245760 locked
2025-11-23 16:56:49.849Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 9535321764616651542623787573095955677, .checksum_padding = 0, .checksum_body = 325751320088309704533442737908049459658, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 245376, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 92357314863631872240181264541742814050, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 17, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2857530738, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.849Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 57114187066127147925107258441335472935, .checksum_padding = 0, .checksum_body = 325751320088309704533442737908049459658, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 245376, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 82395064191550750709630479547427776524, .parent_padding = 0, .request_checksum = 9535321764616651542623787573095955677, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit = 18, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.849Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:49.849Z debug(replica): 2n: on_prepare: advancing commit_max=17..18
2025-11-23 16:56:49.849Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:49.849Z debug(replica): 2n: on_prepare: caching prepare.op=19 (commit_min=17 op=18 commit_max=18 prepare_max=1007)
2025-11-23 16:56:49.849Z debug(replica): 2n: on_prepare: advancing: op=18..19 checksum=82395064191550750709630479547427776524..57114187066127147925107258441335472935
2025-11-23 16:56:49.849Z debug(journal): 2: set_header_as_dirty: op=19 checksum=57114187066127147925107258441335472935
2025-11-23 16:56:49.849Z debug(replica): 2n: append: appending to journal op=19
2025-11-23 16:56:49.849Z debug(journal): 2: write: view=1 slot=19 op=19 len=245376: 57114187066127147925107258441335472935 starting
2025-11-23 16:56:49.849Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=19922944 len=245760 locked
2025-11-23 16:56:49.849Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=19922944 len=245760 unlocked
2025-11-23 16:56:49.849Z debug(replica): 2n: commit_start_journal: cached prepare op=18 checksum=82395064191550750709630479547427776524
2025-11-23 16:56:49.849Z debug(journal): 1: write_header: op=19 sectors[4096..8192]
2025-11-23 16:56:49.849Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:49.849Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:49.849Z debug(journal): 1: write: view=1 slot=19 op=19 len=245376: 57114187066127147925107258441335472935 complete, marking clean
2025-11-23 16:56:49.849Z debug(replica): 1N: send_prepare_ok: op=19 checksum=57114187066127147925107258441335472935
2025-11-23 16:56:49.849Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 271474811298914156155514764351197710871, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 82395064191550750709630479547427776524, .parent_padding = 0, .prepare_checksum = 57114187066127147925107258441335472935, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit_min = 18, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.849Z debug(replica): 2n: repair_prepare: op=19 checksum=57114187066127147925107258441335472935 (already writing)
2025-11-23 16:56:49.849Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 271474811298914156155514764351197710871, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 82395064191550750709630479547427776524, .parent_padding = 0, .prepare_checksum = 57114187066127147925107258441335472935, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit_min = 18, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.849Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:49.849Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:49.849Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:49.849Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=17)
2025-11-23 16:56:49.849Z debug(replica): 2n: execute_op: executing view=1 primary=false op=18 checksum=82395064191550750709630479547427776524 (lookup_accounts)
2025-11-23 16:56:49.849Z debug(replica): 2n: execute_op: commit_timestamp=1763917006984343366 prepare.header.timestamp=1763917006986079722
2025-11-23 16:56:49.849Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=16
2025-11-23 16:56:49.849Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 59243789126604879216968159618869275124, .checksum_padding = 0, .checksum_body = 8741230178737984251481288993886729428, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6144, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 2503818590129678907945532305632594236, .request_checksum_padding = 0, .context = 92357314863631872240181264541742814050, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit = 18, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.850Z debug(replica): 2n: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 59243789126604879216968159618869275124, .checksum_padding = 0, .checksum_body = 8741230178737984251481288993886729428, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6144, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 2503818590129678907945532305632594236, .request_checksum_padding = 0, .context = 92357314863631872240181264541742814050, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 18, .commit = 18, .timestamp = 1763917006986079722, .request = 16, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.850Z debug(forest): entering forest.compact() op=18 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:49.850Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=16)
2025-11-23 16:56:49.850Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=19922944 len=245760 unlocked
2025-11-23 16:56:49.850Z debug(journal): 2: write_header: op=19 sectors[4096..8192]
2025-11-23 16:56:49.850Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:49.850Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:49.850Z debug(journal): 2: write: view=1 slot=19 op=19 len=245376: 57114187066127147925107258441335472935 complete, marking clean
2025-11-23 16:56:49.850Z debug(replica): 2n: send_prepare_ok: op=19 checksum=57114187066127147925107258441335472935
2025-11-23 16:56:49.850Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 36593137681412493039549356592354655644, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 82395064191550750709630479547427776524, .parent_padding = 0, .prepare_checksum = 57114187066127147925107258441335472935, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit_min = 18, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.850Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 36593137681412493039549356592354655644, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 82395064191550750709630479547427776524, .parent_padding = 0, .prepare_checksum = 57114187066127147925107258441335472935, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit_min = 18, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.850Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:49.850Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:49.850Z debug(replica): 1N: on_prepare_ok: quorum received, context=57114187066127147925107258441335472935
2025-11-23 16:56:49.850Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:49.850Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:49.851Z debug(replica): 1N: execute_op: executing view=1 primary=true op=19 checksum=57114187066127147925107258441335472935 (create_transfers)
2025-11-23 16:56:49.851Z debug(replica): 1N: execute_op: commit_timestamp=1763917006986079722 prepare.header.timestamp=1763917009847365054
2025-11-23 16:56:49.858Z debug(replica): 1N: execute_op: advancing commit_max=18..19
2025-11-23 16:56:49.858Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=17
2025-11-23 16:56:49.858Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 111446887656639704423295527216799019563, .checksum_padding = 0, .checksum_body = 249020043531968545210554095928307112010, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 416, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 9535321764616651542623787573095955677, .request_checksum_padding = 0, .context = 32621366845905016819009801221737400605, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit = 19, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.858Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 111446887656639704423295527216799019563, .checksum_padding = 0, .checksum_body = 249020043531968545210554095928307112010, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 416, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 9535321764616651542623787573095955677, .request_checksum_padding = 0, .context = 32621366845905016819009801221737400605, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit = 19, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.858Z debug(forest): entering forest.compact() op=19 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:49.862Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=17)
2025-11-23 16:56:49.862Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:49.862Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:49.865Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 174718081208970959641946877402323707365, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 32621366845905016819009801221737400605, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 18, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 13877634, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:49.865Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 174718081208970959641946877402323707365, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 32621366845905016819009801221737400605, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 18, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 13877634, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(replica): 1N: primary_pipeline_prepare: request checksum=174718081208970959641946877402323707365 client=79827400614827065512247365792344921511
2025-11-23 16:56:49.865Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:49.865Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=247189217696710613613111057033255154628 op=20
2025-11-23 16:56:49.865Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 174718081208970959641946877402323707365, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 32621366845905016819009801221737400605, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 18, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 13877634, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(vsr): 1: prepare_timeout started
2025-11-23 16:56:49.865Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-23 16:56:49.865Z debug(vsr): 1: pulse_timeout reset
2025-11-23 16:56:49.865Z debug(replica): 1N: replicate: replicating op=20 to replica 0
2025-11-23 16:56:49.865Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 247189217696710613613111057033255154628, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 57114187066127147925107258441335472935, .parent_padding = 0, .request_checksum = 174718081208970959641946877402323707365, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit = 19, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(message_bus): 1: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Prepare{ .checksum = 247189217696710613613111057033255154628, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 57114187066127147925107258441335472935, .parent_padding = 0, .request_checksum = 174718081208970959641946877402323707365, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit = 19, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(replica): 1N: replicate: replicating op=20 to replica 2
2025-11-23 16:56:49.865Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 247189217696710613613111057033255154628, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 57114187066127147925107258441335472935, .parent_padding = 0, .request_checksum = 174718081208970959641946877402323707365, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit = 19, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(replica): 1N: on_prepare: advancing: op=19..20 checksum=57114187066127147925107258441335472935..247189217696710613613111057033255154628
2025-11-23 16:56:49.865Z debug(journal): 1: set_header_as_dirty: op=20 checksum=247189217696710613613111057033255154628
2025-11-23 16:56:49.865Z debug(replica): 1N: append: appending to journal op=20
2025-11-23 16:56:49.865Z debug(journal): 1: write: view=1 slot=20 op=20 len=992: 247189217696710613613111057033255154628 starting
2025-11-23 16:56:49.865Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 247189217696710613613111057033255154628, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 57114187066127147925107258441335472935, .parent_padding = 0, .request_checksum = 174718081208970959641946877402323707365, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit = 19, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=20971520 len=4096 locked
2025-11-23 16:56:49.865Z debug(replica): 2n: on_prepare: advancing commit_max=18..19
2025-11-23 16:56:49.865Z debug(replica): 2n: on_prepare: caching prepare.op=20 (commit_min=18 op=19 commit_max=19 prepare_max=1007)
2025-11-23 16:56:49.865Z debug(replica): 2n: on_prepare: advancing: op=19..20 checksum=57114187066127147925107258441335472935..247189217696710613613111057033255154628
2025-11-23 16:56:49.865Z debug(journal): 2: set_header_as_dirty: op=20 checksum=247189217696710613613111057033255154628
2025-11-23 16:56:49.865Z debug(replica): 2n: append: appending to journal op=20
2025-11-23 16:56:49.865Z debug(journal): 2: write: view=1 slot=20 op=20 len=992: 247189217696710613613111057033255154628 starting
2025-11-23 16:56:49.865Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=20971520 len=4096 locked
2025-11-23 16:56:49.865Z debug(replica): 2n: commit_start_journal: cached prepare op=19 checksum=57114187066127147925107258441335472935
2025-11-23 16:56:49.865Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 174718081208970959641946877402323707365, .checksum_padding = 0, .checksum_body = 299292792860401176567473861645164879780, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 992, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 32621366845905016819009801221737400605, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 18, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 13877634, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(replica): 1N: on_request: new request
2025-11-23 16:56:49.865Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-23 16:56:49.865Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=20971520 len=4096 unlocked
2025-11-23 16:56:49.865Z debug(journal): 1: write_header: op=20 sectors[4096..8192]
2025-11-23 16:56:49.865Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:49.865Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:49.865Z debug(journal): 1: write: view=1 slot=20 op=20 len=992: 247189217696710613613111057033255154628 complete, marking clean
2025-11-23 16:56:49.865Z debug(replica): 1N: send_prepare_ok: op=20 checksum=247189217696710613613111057033255154628
2025-11-23 16:56:49.865Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 306848136073976608075038102159641307880, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 57114187066127147925107258441335472935, .parent_padding = 0, .prepare_checksum = 247189217696710613613111057033255154628, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit_min = 19, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 306848136073976608075038102159641307880, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 57114187066127147925107258441335472935, .parent_padding = 0, .prepare_checksum = 247189217696710613613111057033255154628, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit_min = 19, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.865Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:49.865Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-23 16:56:49.865Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-23 16:56:49.866Z debug(replica): 2n: repair_prepare: op=20 checksum=247189217696710613613111057033255154628 (already writing)
2025-11-23 16:56:49.866Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=18)
2025-11-23 16:56:49.866Z debug(replica): 2n: execute_op: executing view=1 primary=false op=19 checksum=57114187066127147925107258441335472935 (create_transfers)
2025-11-23 16:56:49.866Z debug(replica): 2n: execute_op: commit_timestamp=1763917006986079722 prepare.header.timestamp=1763917009847365054
2025-11-23 16:56:49.872Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:49.872Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:49.873Z debug(replica): 2n: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=17
2025-11-23 16:56:49.873Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 111446887656639704423295527216799019563, .checksum_padding = 0, .checksum_body = 249020043531968545210554095928307112010, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 416, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 9535321764616651542623787573095955677, .request_checksum_padding = 0, .context = 32621366845905016819009801221737400605, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit = 19, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.873Z debug(replica): 2n: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 111446887656639704423295527216799019563, .checksum_padding = 0, .checksum_body = 249020043531968545210554095928307112010, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 416, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 9535321764616651542623787573095955677, .request_checksum_padding = 0, .context = 32621366845905016819009801221737400605, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 19, .commit = 19, .timestamp = 1763917009847365054, .request = 17, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.873Z debug(forest): entering forest.compact() op=19 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:49.877Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=20971520 len=4096 unlocked
2025-11-23 16:56:49.877Z debug(journal): 2: write_header: op=20 sectors[4096..8192]
2025-11-23 16:56:49.877Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 locked
2025-11-23 16:56:49.877Z debug(client_replies): 2: write_reply: wrote (client=79827400614827065512247365792344921511 request=17)
2025-11-23 16:56:49.877Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=4096 len=4096 unlocked
2025-11-23 16:56:49.877Z debug(journal): 2: write: view=1 slot=20 op=20 len=992: 247189217696710613613111057033255154628 complete, marking clean
2025-11-23 16:56:49.877Z debug(replica): 2n: send_prepare_ok: op=20 checksum=247189217696710613613111057033255154628
2025-11-23 16:56:49.877Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 132688325324311157106499802006711050801, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 57114187066127147925107258441335472935, .parent_padding = 0, .prepare_checksum = 247189217696710613613111057033255154628, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit_min = 19, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.887Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:49.887Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:49.887Z debug(vsr): 2: journal_repair_timeout fired
2025-11-23 16:56:49.887Z debug(vsr): 2: journal_repair_timeout reset
2025-11-23 16:56:49.892Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:49.892Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:49.905Z warning(faulty_network): connect failed (0,0): error.ConnectionRefused
2025-11-23 16:56:49.907Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-23 16:56:49.907Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-23 16:56:49.911Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 132688325324311157106499802006711050801, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 57114187066127147925107258441335472935, .parent_padding = 0, .prepare_checksum = 247189217696710613613111057033255154628, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit_min = 19, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-23 16:56:49.911Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-23 16:56:49.911Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-23 16:56:49.911Z debug(replica): 1N: on_prepare_ok: quorum received, context=247189217696710613613111057033255154628
2025-11-23 16:56:49.911Z debug(vsr): 1: prepare_timeout stopped
2025-11-23 16:56:49.911Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-23 16:56:49.911Z debug(replica): 1N: execute_op: executing view=1 primary=true op=20 checksum=247189217696710613613111057033255154628 (lookup_accounts)
2025-11-23 16:56:49.911Z debug(replica): 1N: execute_op: commit_timestamp=1763917009847365054 prepare.header.timestamp=1763917009865338624
2025-11-23 16:56:49.911Z debug(replica): 1N: execute_op: advancing commit_max=19..20
2025-11-23 16:56:49.911Z debug(replica): 1N: client_table_entry_update: client=79827400614827065512247365792344921511 session=2 request=18
2025-11-23 16:56:49.912Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 168461605913069629535053882555084907154, .checksum_padding = 0, .checksum_body = 296940610592001246735714005676352606446, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6144, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 174718081208970959641946877402323707365, .request_checksum_padding = 0, .context = 122749131913851688901775559250006715727, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit = 20, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.912Z debug(replica): 1N: sending reply to client 79827400614827065512247365792344921511: vsr.message_header.Header.Reply{ .checksum = 168461605913069629535053882555084907154, .checksum_padding = 0, .checksum_body = 296940610592001246735714005676352606446, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6144, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 174718081208970959641946877402323707365, .request_checksum_padding = 0, .context = 122749131913851688901775559250006715727, .context_padding = 0, .client = 79827400614827065512247365792344921511, .op = 20, .commit = 20, .timestamp = 1763917009865338624, .request = 18, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.912Z debug(forest): entering forest.compact() op=20 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-23 16:56:49.912Z debug(client_replies): 1: write_reply: wrote (client=79827400614827065512247365792344921511 request=18)
2025-11-23 16:56:49.912Z info(workload): accounts created = 45, transfers = 9441, pending transfers = 0, commands run = 9
2025-11-23 16:56:49.912Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-23 16:56:49.912Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-23 16:56:49.914Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 51665523932760051760139650279650332971, .checksum_padding = 0, .checksum_body = 109108298103151637825311733087932017135, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 122749131913851688901775559250006715727, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 19, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 47070067, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-23 16:56:49.914Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-23 16:56:50.005Z warning(faulty_network): connect failed (0,1): error.ConnectionRefused
2025-11-23 16:56:50.107Z warning(faulty_network): connect failed (0,2): error.ConnectionRefused
2025-11-23 16:56:50.189Z warning(faulty_network): connect failed (0,3): error.ConnectionRefused
2025-11-23 16:56:50.266Z warning(faulty_network): connect failed (0,4): error.ConnectionRefused
2025-11-23 16:56:50.347Z warning(faulty_network): connect failed (0,5): error.ConnectionRefused
2025-11-23 16:56:50.408Z warning(faulty_network): connect failed (0,6): error.ConnectionRefused
2025-11-23 16:56:50.477Z warning(faulty_network): connect failed (0,7): error.ConnectionRefused
2025-11-23 16:56:49.914Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 51665523932760051760139650279650332971, .checksum_padding = 0, .checksum_body = 109108298103151637825311733087932017135, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 120704, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 122749131913851688901775559250006715727, .parent_padding = 0, .client = 79827400614827065512247365792344921511, .session = 2, .timestamp = 0, .request = 19, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 47070067, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
