
2025-11-24 15:06:47.708Z debug(replica): 2n: repair_prepare: op=198 checksum=146589306596125555213875987360645679016 (already writing)
2025-11-24 15:06:47.708Z debug(replica): 1n: repair_prepare: op=198 checksum=146589306596125555213875987360645679016 (already writing)
2025-11-24 15:06:50.464Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 224317478839755971601512456121300703255, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 168590351263038195227348124973002214649, .parent_padding = 0, .prepare_checksum = 146589306596125555213875987360645679016, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit_min = 197, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.464Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=196)
2025-11-24 15:06:50.464Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 224317478839755971601512456121300703255, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 168590351263038195227348124973002214649, .parent_padding = 0, .prepare_checksum = 146589306596125555213875987360645679016, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit_min = 197, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.464Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 15:06:50.464Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=196)
2025-11-24 15:06:50.464Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 15:06:50.464Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 15:06:50.464Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.464Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.464Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.464Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.464Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.464Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.464Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.464Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.464Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.464Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.465Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.465Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.465Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.465Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.465Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.465Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:06:50.465Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:06:50.465Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:06:50.465Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:06:50.465Z debug(replica): 2n: execute_op: executing view=3 primary=false op=197 checksum=168590351263038195227348124973002214649 (create_transfers)
2025-11-24 15:06:50.465Z debug(replica): 2n: execute_op: commit_timestamp=1763996804738376402 prepare.header.timestamp=1763996804786721726
2025-11-24 15:06:50.465Z debug(replica): 1n: execute_op: executing view=3 primary=false op=197 checksum=168590351263038195227348124973002214649 (create_transfers)
2025-11-24 15:06:50.465Z debug(replica): 1n: execute_op: commit_timestamp=1763996804738376402 prepare.header.timestamp=1763996804786721726
2025-11-24 15:06:50.471Z debug(replica): 1n: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=195
2025-11-24 15:06:50.471Z debug(replica): 1n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 249839469441175731857691156239344026562, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 120060822418748360250686598126280402258, .request_checksum_padding = 0, .context = 300876949271790774005669849093240057453, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 197, .commit = 197, .timestamp = 1763996804786721726, .request = 195, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.471Z debug(replica): 1n: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 249839469441175731857691156239344026562, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 120060822418748360250686598126280402258, .request_checksum_padding = 0, .context = 300876949271790774005669849093240057453, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 197, .commit = 197, .timestamp = 1763996804786721726, .request = 195, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.471Z debug(forest): entering forest.compact() op=197 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 15:06:50.475Z warning(replica): 1n: commit_dispatch: slow request, request=195 size=239104 create_transfers time=2767ms
2025-11-24 15:06:50.475Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.475Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:50.475Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.475Z error(clock): 1: no agreement on cluster time (partitioned or too many clock faults)
2025-11-24 15:06:50.475Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=207618048 len=4096 unlocked
2025-11-24 15:06:50.475Z debug(journal): 1: write_header: op=198 sectors[49152..53248]
2025-11-24 15:06:50.475Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:06:50.475Z debug(client_replies): 1: write_reply: wrote (client=255374662039738724781684357862070504472 request=195)
2025-11-24 15:06:50.475Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.475Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.475Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.475Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:06:50.475Z debug(journal): 1: write: view=3 slot=198 op=198 len=2320: 146589306596125555213875987360645679016 complete, marking clean
2025-11-24 15:06:50.475Z debug(replica): 1n: send_prepare_ok: op=198 checksum=146589306596125555213875987360645679016
2025-11-24 15:06:50.475Z debug(replica): 1n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 36007449450217024195149819164779099442, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 168590351263038195227348124973002214649, .parent_padding = 0, .prepare_checksum = 146589306596125555213875987360645679016, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit_min = 197, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.476Z debug(replica): 2n: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=195
2025-11-24 15:06:50.476Z debug(forest): entering forest.compact() op=197 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 15:06:50.482Z warning(replica): 2n: commit_dispatch: slow request, request=195 size=239104 create_transfers time=2774ms
2025-11-24 15:06:50.482Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.482Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:50.482Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.482Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:50.482Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:50.482Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.482Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=207618048 len=4096 unlocked
2025-11-24 15:06:50.482Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.482Z debug(journal): 2: write_header: op=198 sectors[49152..53248]
2025-11-24 15:06:50.482Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.482Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:06:50.482Z debug(client_replies): 2: write_reply: wrote (client=255374662039738724781684357862070504472 request=195)
2025-11-24 15:06:50.482Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.482Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:50.482Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.482Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.482Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.482Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:50.482Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.482Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.482Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.483Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.483Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.483Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.483Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.483Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:50.483Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.483Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 24685561199782465795555761651789107145, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 300876949271790774005669849093240057453, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 196, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2913382142, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.483Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.483Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.483Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:06:50.483Z debug(journal): 2: write: view=3 slot=198 op=198 len=2320: 146589306596125555213875987360645679016 complete, marking clean
2025-11-24 15:06:50.483Z debug(replica): 2n: send_prepare_ok: op=198 checksum=146589306596125555213875987360645679016
2025-11-24 15:06:50.483Z debug(replica): 2n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 8609340709445860846545873893814219774, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 168590351263038195227348124973002214649, .parent_padding = 0, .prepare_checksum = 146589306596125555213875987360645679016, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit_min = 197, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.483Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 8609340709445860846545873893814219774, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 168590351263038195227348124973002214649, .parent_padding = 0, .prepare_checksum = 146589306596125555213875987360645679016, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit_min = 197, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.483Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 15:06:50.483Z debug(replica): 0N: on_prepare_ok: 2 message(s)
2025-11-24 15:06:50.483Z debug(replica): 0N: on_prepare_ok: quorum received, context=146589306596125555213875987360645679016
2025-11-24 15:06:50.483Z debug(vsr): 0: prepare_timeout stopped
2025-11-24 15:06:50.483Z debug(vsr): 0: primary_abdicate_timeout stopped
2025-11-24 15:06:50.483Z debug(replica): 0N: execute_op: executing view=3 primary=true op=198 checksum=146589306596125555213875987360645679016 (lookup_accounts)
2025-11-24 15:06:50.483Z debug(replica): 0N: execute_op: commit_timestamp=1763996804786721726 prepare.header.timestamp=1763996807707479862
2025-11-24 15:06:50.483Z debug(replica): 0N: execute_op: advancing commit_max=197..198
2025-11-24 15:06:50.483Z debug(replica): 0N: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=196
2025-11-24 15:06:50.483Z debug(replica): 0N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 200729388094235820697392225709929153789, .checksum_padding = 0, .checksum_body = 335666808942148554362807558705772087628, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 24685561199782465795555761651789107145, .request_checksum_padding = 0, .context = 316388078609436534523695121535288287997, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit = 198, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.483Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 200729388094235820697392225709929153789, .checksum_padding = 0, .checksum_body = 335666808942148554362807558705772087628, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 24685561199782465795555761651789107145, .request_checksum_padding = 0, .context = 316388078609436534523695121535288287997, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit = 198, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.483Z debug(forest): entering forest.compact() op=198 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 255374662039738724781684357862070504472: on_reply: slow request, request=196 op=198 size=2320 lookup_accounts time=2778ms
2025-11-24 15:06:50.484Z debug(client_replies): 0: write_reply: wrote (client=255374662039738724781684357862070504472 request=196)
2025-11-24 15:06:50.484Z info(workload): accounts created = 128, transfers = 256726, pending transfers = 0, commands run = 98
2025-11-24 15:06:50.485Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:06:50.485Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:06:50.485Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 15:06:50.485Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 15:06:50.497Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.497Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:50.497Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.497Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.497Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.497Z debug(replica): 0N: primary_pipeline_prepare: request checksum=76534104136831131822182610886228390529 client=255374662039738724781684357862070504472
2025-11-24 15:06:50.500Z debug(replica): 0N: primary_pipeline_prepare: prepare checksum=135380618683088758263918561582128745982 op=199
2025-11-24 15:06:50.500Z debug(vsr): 0: prepare_timeout started
2025-11-24 15:06:50.500Z debug(vsr): 0: primary_abdicate_timeout started
2025-11-24 15:06:50.500Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:06:50.500Z debug(replica): 0N: replicate: replicating op=199 to replica 2
2025-11-24 15:06:50.500Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 135380618683088758263918561582128745982, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.501Z debug(replica): 0N: replicate: replicating op=199 to replica 1
2025-11-24 15:06:50.501Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 135380618683088758263918561582128745982, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.501Z debug(replica): 0N: on_prepare: advancing: op=198..199 checksum=146589306596125555213875987360645679016..135380618683088758263918561582128745982
2025-11-24 15:06:50.501Z debug(journal): 0: set_header_as_dirty: op=199 checksum=135380618683088758263918561582128745982
2025-11-24 15:06:50.501Z debug(replica): 0N: append: appending to journal op=199
2025-11-24 15:06:50.501Z debug(journal): 0: write: view=3 slot=199 op=199 len=832640: 135380618683088758263918561582128745982 starting
2025-11-24 15:06:50.501Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=208666624 len=835584 locked
2025-11-24 15:06:50.502Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 36007449450217024195149819164779099442, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 168590351263038195227348124973002214649, .parent_padding = 0, .prepare_checksum = 146589306596125555213875987360645679016, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit_min = 197, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.502Z debug(replica): 0N: on_prepare_ok: not preparing op=198 checksum=146589306596125555213875987360645679016
2025-11-24 15:06:50.504Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 135380618683088758263918561582128745982, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.504Z debug(replica): 2n: on_prepare: advancing commit_max=197..198
2025-11-24 15:06:50.504Z debug(replica): 2n: on_prepare: caching prepare.op=199 (commit_min=197 op=198 commit_max=198 prepare_max=1007)
2025-11-24 15:06:50.505Z debug(replica): 2n: on_prepare: advancing: op=198..199 checksum=146589306596125555213875987360645679016..135380618683088758263918561582128745982
2025-11-24 15:06:50.505Z debug(journal): 2: set_header_as_dirty: op=199 checksum=135380618683088758263918561582128745982
2025-11-24 15:06:50.505Z debug(replica): 2n: append: appending to journal op=199
2025-11-24 15:06:50.505Z debug(journal): 2: write: view=3 slot=199 op=199 len=832640: 135380618683088758263918561582128745982 starting
2025-11-24 15:06:50.505Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=208666624 len=835584 locked
2025-11-24 15:06:50.505Z debug(replica): 2n: commit_start_journal: cached prepare op=198 checksum=146589306596125555213875987360645679016
2025-11-24 15:06:50.505Z debug(replica): 2n: repair_prepare: op=199 checksum=135380618683088758263918561582128745982 (already writing)
2025-11-24 15:06:50.505Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=197)
2025-11-24 15:06:50.505Z debug(replica): 2n: execute_op: executing view=3 primary=false op=198 checksum=146589306596125555213875987360645679016 (lookup_accounts)
2025-11-24 15:06:50.505Z debug(replica): 2n: execute_op: commit_timestamp=1763996804786721726 prepare.header.timestamp=1763996807707479862
2025-11-24 15:06:50.505Z debug(replica): 2n: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=196
2025-11-24 15:06:50.505Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 200729388094235820697392225709929153789, .checksum_padding = 0, .checksum_body = 335666808942148554362807558705772087628, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 24685561199782465795555761651789107145, .request_checksum_padding = 0, .context = 316388078609436534523695121535288287997, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit = 198, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.505Z debug(replica): 2n: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 200729388094235820697392225709929153789, .checksum_padding = 0, .checksum_body = 335666808942148554362807558705772087628, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 24685561199782465795555761651789107145, .request_checksum_padding = 0, .context = 316388078609436534523695121535288287997, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 198, .commit = 198, .timestamp = 1763996807707479862, .request = 196, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.505Z debug(forest): entering forest.compact() op=198 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 15:06:50.505Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 135380618683088758263918561582128745982, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.505Z debug(replica): 1n: on_prepare: advancing commit_max=197..198
2025-11-24 15:06:50.505Z debug(replica): 1n: on_prepare: caching prepare.op=199 (commit_min=197 op=198 commit_max=198 prepare_max=1007)
2025-11-24 15:06:50.505Z debug(replica): 1n: on_prepare: advancing: op=198..199 checksum=146589306596125555213875987360645679016..135380618683088758263918561582128745982
2025-11-24 15:06:50.505Z debug(journal): 1: set_header_as_dirty: op=199 checksum=135380618683088758263918561582128745982
2025-11-24 15:06:50.505Z debug(replica): 1n: append: appending to journal op=199
2025-11-24 15:06:50.505Z debug(journal): 1: write: view=3 slot=199 op=199 len=832640: 135380618683088758263918561582128745982 starting
2025-11-24 15:06:50.505Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=208666624 len=835584 locked
2025-11-24 15:06:50.505Z debug(replica): 1n: commit_start_journal: cached prepare op=198 checksum=146589306596125555213875987360645679016
2025-11-24 15:06:50.505Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:50.505Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:50.505Z debug(client_replies): 2: write_reply: wrote (client=255374662039738724781684357862070504472 request=196)
2025-11-24 15:06:50.505Z debug(replica): 1n: repair_prepare: op=199 checksum=135380618683088758263918561582128745982 (already writing)
2025-11-24 15:06:50.505Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=197)
2025-11-24 15:06:50.505Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.505Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:50.505Z debug(replica): 1n: execute_op: executing view=3 primary=false op=198 checksum=146589306596125555213875987360645679016 (lookup_accounts)
2025-11-24 15:06:50.505Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:50.505Z debug(replica): 1n: execute_op: commit_timestamp=1763996804786721726 prepare.header.timestamp=1763996807707479862
2025-11-24 15:06:50.505Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=208666624 len=835584 unlocked
2025-11-24 15:06:50.505Z debug(journal): 0: write_header: op=199 sectors[49152..53248]
2025-11-24 15:06:50.505Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:06:50.505Z debug(replica): 1n: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=196
2025-11-24 15:06:50.505Z debug(forest): entering forest.compact() op=198 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 15:06:50.505Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:06:50.505Z debug(journal): 0: write: view=3 slot=199 op=199 len=832640: 135380618683088758263918561582128745982 complete, marking clean
2025-11-24 15:06:50.505Z debug(replica): 0N: send_prepare_ok: op=199 checksum=135380618683088758263918561582128745982
2025-11-24 15:06:50.505Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 121769256372247581696096080029838908452, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .prepare_checksum = 135380618683088758263918561582128745982, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit_min = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.505Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 121769256372247581696096080029838908452, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .prepare_checksum = 135380618683088758263918561582128745982, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit_min = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.505Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=208666624 len=835584 unlocked
2025-11-24 15:06:50.505Z debug(journal): 2: write_header: op=199 sectors[49152..53248]
2025-11-24 15:06:50.505Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 15:06:50.505Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:06:50.505Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 15:06:50.505Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 15:06:50.505Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:06:50.505Z debug(journal): 2: write: view=3 slot=199 op=199 len=832640: 135380618683088758263918561582128745982 complete, marking clean
2025-11-24 15:06:50.505Z debug(replica): 2n: send_prepare_ok: op=199 checksum=135380618683088758263918561582128745982
2025-11-24 15:06:50.505Z debug(replica): 2n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 334864010008117316422982212212973414624, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .prepare_checksum = 135380618683088758263918561582128745982, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit_min = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:50.506Z debug(vsr): 1: ping_timeout fired
2025-11-24 15:06:50.506Z debug(vsr): 1: ping_timeout reset
2025-11-24 15:06:50.506Z debug(replica): 1n: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 221045316635745114418184530514819426968, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36112082300506598, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.525Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:50.525Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:50.525Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:06:50.525Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:06:50.545Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:50.506Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Ping{ .checksum = 221045316635745114418184530514819426968, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36112082300506598, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.506Z debug(replica): 1n: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 221045316635745114418184530514819426968, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36112082300506598, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:50.545Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:53.451Z debug(replica): 0N: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 126154656395144968716872720368296918619, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36112082300506598, .pong_timestamp_wall = 1763996813451219246, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.451Z debug(vsr): 1: start_view_change_message_timeout fired
2025-11-24 15:06:53.451Z debug(vsr): 1: start_view_change_message_timeout reset
2025-11-24 15:06:53.451Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 15:06:53.451Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 15:06:53.451Z debug(vsr): 1: journal_repair_timeout fired
2025-11-24 15:06:53.451Z debug(vsr): 1: journal_repair_timeout reset
2025-11-24 15:06:53.451Z debug(replica): 1n: repair_prepare: op=199 checksum=135380618683088758263918561582128745982 (already writing)
2025-11-24 15:06:53.451Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-11-24 15:06:53.451Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-11-24 15:06:53.451Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 334864010008117316422982212212973414624, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .prepare_checksum = 135380618683088758263918561582128745982, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit_min = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:53.451Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 15:06:53.451Z debug(replica): 0N: on_prepare_ok: 2 message(s)
2025-11-24 15:06:53.451Z debug(replica): 0N: on_prepare_ok: quorum received, context=135380618683088758263918561582128745982
2025-11-24 15:06:53.451Z debug(client_replies): 1: write_reply: wrote (client=255374662039738724781684357862070504472 request=196)
2025-11-24 15:06:53.451Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Ping{ .checksum = 221045316635745114418184530514819426968, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36112082300506598, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.451Z debug(vsr): 0: prepare_timeout stopped
2025-11-24 15:06:53.451Z debug(vsr): 0: primary_abdicate_timeout stopped
2025-11-24 15:06:53.451Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 287201897901535973273819529159984000610, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36112082300506598, .pong_timestamp_wall = 1763996813451507346, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.451Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=208666624 len=835584 unlocked
2025-11-24 15:06:53.451Z debug(journal): 1: write_header: op=199 sectors[49152..53248]
2025-11-24 15:06:53.451Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:06:53.451Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Pong{ .checksum = 126154656395144968716872720368296918619, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36112082300506598, .pong_timestamp_wall = 1763996813451219246, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.451Z debug(clock): 1: learn: replica=0 m0=36112082300506598 t1=1763996813451219246 m2=36112085246003475 t2=1763996813451617696 one_way_delay=1472748438 asymmetric_delay=0 clock_offset=1472349988
2025-11-24 15:06:53.451Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Pong{ .checksum = 287201897901535973273819529159984000610, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36112082300506598, .pong_timestamp_wall = 1763996813451507346, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.451Z debug(clock): 1: learn: replica=2 m0=36112082300506598 t1=1763996813451507346 m2=36112085246245275 t2=1763996813451859496 one_way_delay=1472869338 asymmetric_delay=0 clock_offset=1472517188
2025-11-24 15:06:53.451Z debug(vsr): 1: prepare_timeout rtt=221..294
2025-11-24 15:06:53.451Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:06:53.451Z debug(journal): 1: write: view=3 slot=199 op=199 len=832640: 135380618683088758263918561582128745982 complete, marking clean
2025-11-24 15:06:53.451Z debug(replica): 1n: send_prepare_ok: op=199 checksum=135380618683088758263918561582128745982
2025-11-24 15:06:53.451Z debug(replica): 1n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 39742983484371288588585977437695357859, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .prepare_checksum = 135380618683088758263918561582128745982, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit_min = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:53.454Z debug(replica): 0N: execute_op: executing view=3 primary=true op=199 checksum=135380618683088758263918561582128745982 (create_transfers)
2025-11-24 15:06:53.454Z debug(replica): 0N: execute_op: commit_timestamp=1763996807707479862 prepare.header.timestamp=1763996810497564621
2025-11-24 15:06:53.454Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.454Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:53.454Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.455Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.455Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:53.455Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.458Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.458Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:53.458Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.459Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.459Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:53.459Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.461Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.461Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:53.461Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.463Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.463Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:53.463Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.463Z info(clock): 1: synchronized: accuracy=0ns
2025-11-24 15:06:53.463Z debug(clock): 1: synchronized: truechimers=3/3 clock_offset=0ns..0ns accuracy=0ns
2025-11-24 15:06:53.463Z debug(clock): 1: system time is 10ns behind
2025-11-24 15:06:53.472Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:53.472Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:53.473Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 15:06:53.473Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 15:06:53.477Z debug(replica): 0N: execute_op: advancing commit_max=198..199
2025-11-24 15:06:53.477Z debug(replica): 0N: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=197
2025-11-24 15:06:53.477Z debug(replica): 0N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.477Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.477Z debug(forest): entering forest.compact() op=199 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 255374662039738724781684357862070504472: on_reply: slow request, request=197 op=199 size=832640 create_transfers time=2987ms
2025-11-24 15:06:53.488Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 39742983484371288588585977437695357859, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 146589306596125555213875987360645679016, .parent_padding = 0, .prepare_checksum = 135380618683088758263918561582128745982, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit_min = 198, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:53.489Z debug(replica): 0N: on_prepare_ok: not preparing op=199 checksum=135380618683088758263918561582128745982
2025-11-24 15:06:53.489Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:06:53.489Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:06:53.489Z debug(client_replies): 0: write_reply: wrote (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:53.492Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:53.492Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:53.492Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.492Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.492Z debug(client_replies): 0: read_reply: start (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.493Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 15:06:53.493Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 15:06:53.496Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.496Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.496Z debug(client_replies): 0: read_reply: busy (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.496Z debug(replica): 0N: on_request: ignoring (client_replies busy)
2025-11-24 15:06:53.496Z debug(client_replies): 0: read_reply: done (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.496Z debug(replica): 0N: on_request: repeat reply (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:53.496Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.497Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.497Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:53.497Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.500Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.500Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.500Z debug(client_replies): 0: read_reply: start (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.500Z debug(client_replies): 0: read_reply: done (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.500Z debug(replica): 0N: on_request: repeat reply (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:53.500Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.504Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.504Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.504Z debug(client_replies): 0: read_reply: start (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.508Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.508Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.508Z debug(client_replies): 0: read_reply: busy (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.508Z debug(replica): 0N: on_request: ignoring (client_replies busy)
2025-11-24 15:06:53.508Z debug(client_replies): 0: read_reply: done (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.508Z debug(replica): 0N: on_request: repeat reply (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:53.508Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.511Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.511Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.511Z debug(client_replies): 0: read_reply: start (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.512Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:53.512Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:53.513Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 15:06:53.513Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 15:06:53.515Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.515Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.515Z debug(client_replies): 0: read_reply: busy (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.515Z debug(replica): 0N: on_request: ignoring (client_replies busy)
2025-11-24 15:06:53.515Z debug(client_replies): 0: read_reply: done (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.515Z debug(replica): 0N: on_request: repeat reply (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:53.515Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.519Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.519Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.519Z debug(client_replies): 0: read_reply: start (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.519Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.519Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:53.519Z debug(replica): 0N: primary_pipeline_prepare: request checksum=328867815002123047368592937540206036010 client=255374662039738724781684357862070504472
2025-11-24 15:06:53.519Z debug(replica): 0N: primary_pipeline_prepare: prepare checksum=312925206777049643456755361304928977011 op=200
2025-11-24 15:06:53.519Z debug(vsr): 0: prepare_timeout started
2025-11-24 15:06:53.519Z debug(vsr): 0: primary_abdicate_timeout started
2025-11-24 15:06:53.519Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:06:53.519Z debug(replica): 0N: replicate: replicating op=200 to replica 2
2025-11-24 15:06:53.519Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 312925206777049643456755361304928977011, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .request_checksum = 328867815002123047368592937540206036010, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:53.519Z debug(replica): 0N: replicate: replicating op=200 to replica 1
2025-11-24 15:06:53.519Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 312925206777049643456755361304928977011, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .request_checksum = 328867815002123047368592937540206036010, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:53.519Z debug(replica): 0N: on_prepare: advancing: op=199..200 checksum=135380618683088758263918561582128745982..312925206777049643456755361304928977011
2025-11-24 15:06:53.519Z debug(journal): 0: set_header_as_dirty: op=200 checksum=312925206777049643456755361304928977011
2025-11-24 15:06:53.519Z debug(replica): 0N: append: appending to journal op=200
2025-11-24 15:06:53.519Z debug(journal): 0: write: view=3 slot=200 op=200 len=2320: 312925206777049643456755361304928977011 starting
2025-11-24 15:06:53.519Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=209715200 len=4096 locked
2025-11-24 15:06:53.519Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 312925206777049643456755361304928977011, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .request_checksum = 328867815002123047368592937540206036010, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:53.519Z debug(replica): 2n: on_prepare: advancing commit_max=198..199
2025-11-24 15:06:53.519Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 312925206777049643456755361304928977011, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .request_checksum = 328867815002123047368592937540206036010, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:53.519Z debug(replica): 2n: on_prepare: caching prepare.op=200 (commit_min=198 op=199 commit_max=199 prepare_max=1007)
2025-11-24 15:06:53.519Z debug(replica): 1n: on_prepare: advancing commit_max=198..199
2025-11-24 15:06:53.519Z debug(replica): 1n: on_prepare: caching prepare.op=200 (commit_min=198 op=199 commit_max=199 prepare_max=1007)
2025-11-24 15:06:53.519Z debug(replica): 2n: on_prepare: advancing: op=199..200 checksum=135380618683088758263918561582128745982..312925206777049643456755361304928977011
2025-11-24 15:06:53.519Z debug(replica): 1n: on_prepare: advancing: op=199..200 checksum=135380618683088758263918561582128745982..312925206777049643456755361304928977011
2025-11-24 15:06:53.519Z debug(journal): 2: set_header_as_dirty: op=200 checksum=312925206777049643456755361304928977011
2025-11-24 15:06:53.519Z debug(journal): 1: set_header_as_dirty: op=200 checksum=312925206777049643456755361304928977011
2025-11-24 15:06:53.519Z debug(replica): 2n: append: appending to journal op=200
2025-11-24 15:06:53.519Z debug(replica): 1n: append: appending to journal op=200
2025-11-24 15:06:53.519Z debug(journal): 2: write: view=3 slot=200 op=200 len=2320: 312925206777049643456755361304928977011 starting
2025-11-24 15:06:53.519Z debug(journal): 1: write: view=3 slot=200 op=200 len=2320: 312925206777049643456755361304928977011 starting
2025-11-24 15:06:53.519Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=209715200 len=4096 locked
2025-11-24 15:06:53.519Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=209715200 len=4096 locked
2025-11-24 15:06:53.519Z debug(replica): 2n: commit_start_journal: cached prepare op=199 checksum=135380618683088758263918561582128745982
2025-11-24 15:06:53.519Z debug(replica): 1n: commit_start_journal: cached prepare op=199 checksum=135380618683088758263918561582128745982
2025-11-24 15:06:53.520Z debug(replica): 1n: repair_prepare: op=200 checksum=312925206777049643456755361304928977011 (already writing)
2025-11-24 15:06:53.520Z debug(replica): 2n: repair_prepare: op=200 checksum=312925206777049643456755361304928977011 (already writing)
2025-11-24 15:06:53.520Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=198)
2025-11-24 15:06:53.520Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=198)
2025-11-24 15:06:53.522Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.522Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.522Z debug(client_replies): 0: read_reply: busy (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.522Z debug(replica): 0N: on_request: ignoring (client_replies busy)
2025-11-24 15:06:53.522Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=209715200 len=4096 unlocked
2025-11-24 15:06:53.522Z debug(journal): 0: write_header: op=200 sectors[49152..53248]
2025-11-24 15:06:53.522Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:06:53.522Z debug(client_replies): 0: read_reply: done (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.522Z debug(replica): 0N: on_request: repeat reply (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:53.522Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.522Z debug(replica): 1n: execute_op: executing view=3 primary=false op=199 checksum=135380618683088758263918561582128745982 (create_transfers)
2025-11-24 15:06:53.522Z debug(replica): 1n: execute_op: commit_timestamp=1763996807707479862 prepare.header.timestamp=1763996810497564621
2025-11-24 15:06:53.522Z debug(replica): 2n: execute_op: executing view=3 primary=false op=199 checksum=135380618683088758263918561582128745982 (create_transfers)
2025-11-24 15:06:53.522Z debug(replica): 2n: execute_op: commit_timestamp=1763996807707479862 prepare.header.timestamp=1763996810497564621
2025-11-24 15:06:53.525Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.525Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:53.525Z debug(client_replies): 0: read_reply: start (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:53.525Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:06:53.525Z debug(journal): 0: write: view=3 slot=200 op=200 len=2320: 312925206777049643456755361304928977011 complete, marking clean
2025-11-24 15:06:53.525Z debug(replica): 0N: send_prepare_ok: op=200 checksum=312925206777049643456755361304928977011
2025-11-24 15:06:53.525Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 34619421849973538204888487594215840445, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .prepare_checksum = 312925206777049643456755361304928977011, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit_min = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:53.525Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 34619421849973538204888487594215840445, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .prepare_checksum = 312925206777049643456755361304928977011, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit_min = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:57.067Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 15:06:57.068Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 15:06:57.068Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 15:06:53.542Z debug(replica): 1n: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=197
2025-11-24 15:06:57.068Z debug(forest): entering forest.compact() op=199 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 15:06:57.068Z debug(client_replies): 0: read_reply: done (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:57.068Z debug(replica): 0N: on_request: repeat reply (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:57.068Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:53.545Z debug(replica): 2n: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=197
2025-11-24 15:06:57.068Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.068Z debug(replica): 2n: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.068Z debug(forest): entering forest.compact() op=199 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 15:06:57.072Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.072Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:57.072Z debug(client_replies): 0: read_reply: start (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:57.072Z debug(client_replies): 0: read_reply: done (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:57.072Z debug(replica): 0N: on_request: repeat reply (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:57.072Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.076Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 76534104136831131822182610886228390529, .checksum_padding = 0, .checksum_body = 10905198353313616068311893014653900525, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 832640, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 316388078609436534523695121535288287997, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 197, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2779009267, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:06:57.076Z debug(client_replies): 0: read_reply: start (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:57.076Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.076Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.076Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.076Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.076Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.076Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.077Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.077Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.077Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.077Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.077Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.077Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.077Z debug(client_replies): 0: read_reply: done (client=255374662039738724781684357862070504472 reply=295883502440191293902841125610037159283)
2025-11-24 15:06:57.077Z debug(replica): 0N: on_request: repeat reply (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:57.077Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 295883502440191293902841125610037159283, .checksum_padding = 0, .checksum_body = 120763378505462437788416015557706738912, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 352, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 76534104136831131822182610886228390529, .request_checksum_padding = 0, .context = 71224091819938528778659993364116161830, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 199, .commit = 199, .timestamp = 1763996810497564621, .request = 197, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.080Z warning(replica): 1n: commit_dispatch: slow request, request=197 size=832640 create_transfers time=3561ms
2025-11-24 15:06:57.080Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.080Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:57.080Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.081Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=209715200 len=4096 unlocked
2025-11-24 15:06:57.081Z debug(journal): 1: write_header: op=200 sectors[49152..53248]
2025-11-24 15:06:57.081Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:06:57.081Z debug(client_replies): 1: write_reply: wrote (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:57.081Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.081Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.081Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.081Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.081Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:57.081Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.081Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.081Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:57.081Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.081Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.081Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:57.081Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.081Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:06:57.081Z debug(journal): 1: write: view=3 slot=200 op=200 len=2320: 312925206777049643456755361304928977011 complete, marking clean
2025-11-24 15:06:57.081Z debug(replica): 1n: send_prepare_ok: op=200 checksum=312925206777049643456755361304928977011
2025-11-24 15:06:57.081Z debug(replica): 1n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 155523138008116961984105031525526188809, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .prepare_checksum = 312925206777049643456755361304928977011, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit_min = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:57.082Z warning(replica): 2n: commit_dispatch: slow request, request=197 size=832640 create_transfers time=3563ms
2025-11-24 15:06:57.083Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.083Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:57.083Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.083Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=209715200 len=4096 unlocked
2025-11-24 15:06:57.083Z debug(journal): 2: write_header: op=200 sectors[49152..53248]
2025-11-24 15:06:57.083Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.083Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:06:57.083Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.083Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.083Z debug(client_replies): 2: write_reply: wrote (client=255374662039738724781684357862070504472 request=197)
2025-11-24 15:06:57.083Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.083Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:57.083Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.084Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:06:57.084Z debug(journal): 2: write: view=3 slot=200 op=200 len=2320: 312925206777049643456755361304928977011 complete, marking clean
2025-11-24 15:06:57.084Z debug(replica): 2n: send_prepare_ok: op=200 checksum=312925206777049643456755361304928977011
2025-11-24 15:06:57.084Z debug(replica): 2n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 96825851268998109592218858934318130329, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .prepare_checksum = 312925206777049643456755361304928977011, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit_min = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:57.087Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:06:57.087Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:06:57.091Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 15:06:57.091Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 15:06:57.093Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:57.093Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:57.093Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:06:57.093Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:06:57.107Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:06:57.107Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:06:57.111Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 15:06:57.111Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 15:06:57.111Z debug(vsr): 1: journal_repair_timeout fired
2025-11-24 15:06:57.111Z debug(vsr): 1: journal_repair_timeout reset
2025-11-24 15:06:57.113Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:57.114Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:57.125Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.125Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.125Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.125Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 96825851268998109592218858934318130329, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .prepare_checksum = 312925206777049643456755361304928977011, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit_min = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:57.125Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 15:06:57.125Z debug(replica): 0N: on_prepare_ok: 2 message(s)
2025-11-24 15:06:57.125Z debug(replica): 0N: on_prepare_ok: quorum received, context=312925206777049643456755361304928977011
2025-11-24 15:06:57.125Z debug(vsr): 0: prepare_timeout stopped
2025-11-24 15:06:57.125Z debug(vsr): 0: primary_abdicate_timeout stopped
2025-11-24 15:06:57.126Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.126Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.126Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.126Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.126Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.126Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.126Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 328867815002123047368592937540206036010, .checksum_padding = 0, .checksum_body = 109418571963002055708208744564612312582, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 71224091819938528778659993364116161830, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 198, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2988112627, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.126Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.126Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:06:57.126Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 155523138008116961984105031525526188809, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135380618683088758263918561582128745982, .parent_padding = 0, .prepare_checksum = 312925206777049643456755361304928977011, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit_min = 199, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:57.126Z debug(replica): 0N: on_prepare_ok: 3 message(s)
2025-11-24 15:06:57.126Z debug(replica): 0N: on_prepare_ok: ignoring (quorum received already)
2025-11-24 15:06:57.126Z debug(replica): 0N: execute_op: executing view=3 primary=true op=200 checksum=312925206777049643456755361304928977011 (lookup_accounts)
2025-11-24 15:06:57.126Z debug(replica): 0N: execute_op: commit_timestamp=1763996810497564621 prepare.header.timestamp=1763996813519342335
2025-11-24 15:06:57.126Z debug(replica): 0N: execute_op: advancing commit_max=199..200
2025-11-24 15:06:57.126Z debug(replica): 0N: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=198
2025-11-24 15:06:57.126Z debug(replica): 0N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 218230660324807351539417732250651238053, .checksum_padding = 0, .checksum_body = 44717114344069025134241755976272834355, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 328867815002123047368592937540206036010, .request_checksum_padding = 0, .context = 23787888007671778367813226116975105261, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit = 200, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.126Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 218230660324807351539417732250651238053, .checksum_padding = 0, .checksum_body = 44717114344069025134241755976272834355, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 328867815002123047368592937540206036010, .request_checksum_padding = 0, .context = 23787888007671778367813226116975105261, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 200, .commit = 200, .timestamp = 1763996813519342335, .request = 198, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.126Z debug(forest): entering forest.compact() op=200 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 255374662039738724781684357862070504472: on_reply: slow request, request=198 op=200 size=2320 lookup_accounts time=3629ms
2025-11-24 15:06:57.126Z debug(client_replies): 0: write_reply: wrote (client=255374662039738724781684357862070504472 request=198)
2025-11-24 15:06:57.127Z info(workload): accounts created = 128, transfers = 263228, pending transfers = 0, commands run = 99
2025-11-24 15:06:57.127Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:06:57.127Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:06:57.127Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:06:57.127Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:06:57.131Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 15:06:57.131Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 15:06:57.134Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.134Z debug(replica): 0N: on_request: new request
2025-11-24 15:06:57.134Z debug(replica): 0N: primary_pipeline_prepare: request checksum=180969235032925710693943621489803023469 client=255374662039738724781684357862070504472
2025-11-24 15:06:57.134Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.134Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:06:57.134Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:06:57.135Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:06:57.135Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:06:57.136Z debug(replica): 0N: primary_pipeline_prepare: prepare checksum=324071011354290928302152359153641987094 op=201
2025-11-24 15:06:57.136Z debug(vsr): 0: prepare_timeout started
2025-11-24 15:06:57.136Z debug(vsr): 0: primary_abdicate_timeout started
2025-11-24 15:06:57.136Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:06:57.136Z debug(replica): 0N: replicate: replicating op=201 to replica 2
2025-11-24 15:06:57.136Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:57.136Z debug(replica): 0N: replicate: replicating op=201 to replica 1
2025-11-24 15:06:57.136Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:57.136Z debug(replica): 0N: on_prepare: advancing: op=200..201 checksum=312925206777049643456755361304928977011..324071011354290928302152359153641987094
2025-11-24 15:06:57.136Z debug(journal): 0: set_header_as_dirty: op=201 checksum=324071011354290928302152359153641987094
2025-11-24 15:06:57.136Z debug(replica): 0N: append: appending to journal op=201
2025-11-24 15:06:57.136Z debug(journal): 0: write: view=3 slot=201 op=201 len=335616: 324071011354290928302152359153641987094 starting
2025-11-24 15:06:57.136Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=210763776 len=335872 locked
2025-11-24 15:06:57.137Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:06:57.137Z debug(replica): 2n: on_prepare: advancing commit_max=199..200
2025-11-24 15:06:57.137Z debug(replica): 2n: on_prepare: caching prepare.op=201 (commit_min=199 op=200 commit_max=200 prepare_max=1007)
2025-11-24 15:06:57.137Z debug(replica): 2n: on_prepare: advancing: op=200..201 checksum=312925206777049643456755361304928977011..324071011354290928302152359153641987094
2025-11-24 15:06:57.137Z debug(journal): 2: set_header_as_dirty: op=201 checksum=324071011354290928302152359153641987094
2025-11-24 15:06:57.137Z debug(replica): 2n: append: appending to journal op=201
2025-11-24 15:06:57.137Z debug(journal): 2: write: view=3 slot=201 op=201 len=335616: 324071011354290928302152359153641987094 starting
2025-11-24 15:06:59.434Z info(supervisor): 1: terminating replica
2025-11-24 15:06:57.137Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=210763776 len=335872 locked
2025-11-24 15:06:57.137Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.124Z debug(replica): 2n: commit_start_journal: cached prepare op=200 checksum=312925206777049643456755361304928977011
2025-11-24 15:07:00.124Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:00.124Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:00.124Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=210763776 len=335872 unlocked
2025-11-24 15:07:00.124Z debug(journal): 0: write_header: op=201 sectors[49152..53248]
2025-11-24 15:07:00.124Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:07:00.124Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 15:07:00.125Z debug(replica): 2n: repair_prepare: op=201 checksum=324071011354290928302152359153641987094 (already writing)
2025-11-24 15:06:59.446Z warning(faulty_network): recv error (1,8): error.ConnectionResetByPeer
2025-11-24 15:07:00.125Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=199)
2025-11-24 15:07:00.125Z debug(replica): 2n: execute_op: executing view=3 primary=false op=200 checksum=312925206777049643456755361304928977011 (lookup_accounts)
2025-11-24 15:07:00.125Z debug(replica): 2n: execute_op: commit_timestamp=1763996810497564621 prepare.header.timestamp=1763996813519342335
2025-11-24 15:07:00.125Z debug(replica): 2n: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=198
2025-11-24 15:07:00.125Z debug(forest): entering forest.compact() op=200 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 15:07:00.125Z info(supervisor): injecting network delays: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 481, .jitter_ms = 50 }, .lose = null, .corrupt = null }
2025-11-24 15:07:00.125Z info(message_bus): 2: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 15:07:00.125Z debug(client_replies): 2: write_reply: wrote (client=255374662039738724781684357862070504472 request=198)
2025-11-24 15:07:00.127Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.127Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:00.127Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:00.127Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:07:00.127Z debug(journal): 0: write: view=3 slot=201 op=201 len=335616: 324071011354290928302152359153641987094 complete, marking clean
2025-11-24 15:07:00.127Z debug(replica): 0N: send_prepare_ok: op=201 checksum=324071011354290928302152359153641987094
2025-11-24 15:07:00.127Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 305495129456123645935834738432531535386, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .prepare_checksum = 324071011354290928302152359153641987094, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit_min = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.127Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 305495129456123645935834738432531535386, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .prepare_checksum = 324071011354290928302152359153641987094, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit_min = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.127Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 15:07:00.127Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 15:07:00.127Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 15:07:00.127Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=81ms
2025-11-24 15:07:00.127Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.127Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:07:00.127Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.127Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=210763776 len=335872 unlocked
2025-11-24 15:07:00.127Z debug(journal): 2: write_header: op=201 sectors[49152..53248]
2025-11-24 15:07:00.127Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 locked
2025-11-24 15:07:00.128Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.129Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:00.129Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:00.129Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.129Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:07:00.129Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.129Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=49152 len=4096 unlocked
2025-11-24 15:07:00.129Z debug(journal): 2: write: view=3 slot=201 op=201 len=335616: 324071011354290928302152359153641987094 complete, marking clean
2025-11-24 15:07:00.129Z debug(replica): 2n: send_prepare_ok: op=201 checksum=324071011354290928302152359153641987094
2025-11-24 15:07:00.129Z debug(replica): 2n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 162165951138150192585670029106983328313, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .prepare_checksum = 324071011354290928302152359153641987094, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit_min = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.130Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.130Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:00.130Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:00.131Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.131Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 15:07:00.131Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.132Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.132Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:00.132Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:00.134Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.134Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:00.134Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:00.135Z info(supervisor): sleeping for 1.594s
2025-11-24 15:07:00.135Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.135Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.137Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.137Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.155Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.155Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.157Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.157Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.175Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.175Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.175Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:07:00.175Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:07:00.177Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.177Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.196Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.196Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.196Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:00.197Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.197Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.208Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 15:07:00.208Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 15:07:00.208Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:00.208Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 15:07:00.216Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.216Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.217Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=91ms
2025-11-24 15:07:00.217Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.217Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.217Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:07:00.217Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:07:00.217Z debug(vsr): 0: pulse_timeout fired
2025-11-24 15:07:00.217Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:07:00.236Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.236Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.237Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.237Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.256Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.256Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.257Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.257Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.265Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:00.276Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.276Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.276Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:07:00.276Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:07:00.277Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.277Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.296Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.296Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.298Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.298Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.309Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 15:07:00.309Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 15:07:00.309Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:00.309Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 15:07:00.316Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.316Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.318Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=60ms
2025-11-24 15:07:00.318Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.318Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.318Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:07:00.318Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:07:00.318Z debug(vsr): 0: pulse_timeout fired
2025-11-24 15:07:00.318Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:07:00.331Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:00.336Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.336Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.338Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.338Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.356Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.356Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.358Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.358Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.368Z debug(vsr): 0: prepare_timeout fired
2025-11-24 15:07:00.368Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 15:07:00.368Z debug(vsr): 0: prepare_timeout after=25..3 (rtt=1 min=1 max=1000 attempts=1)
2025-11-24 15:07:00.368Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 15:07:00.368Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 15:07:00.368Z debug(replica): 0N: on_prepare_timeout: replicating to replica 2
2025-11-24 15:07:00.368Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.376Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-24 15:07:00.376Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-24 15:07:00.376Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.376Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.376Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:07:00.376Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:07:00.377Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-24 15:07:00.377Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-24 15:07:00.378Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 15:07:00.378Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 15:07:00.378Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:00.378Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 15:07:00.379Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=88ms
2025-11-24 15:07:00.379Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.379Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.389Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:00.397Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.397Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.399Z debug(vsr): 0: prepare_timeout fired
2025-11-24 15:07:00.399Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 15:07:00.399Z debug(vsr): 0: prepare_timeout after=3..7 (rtt=1 min=1 max=1000 attempts=2)
2025-11-24 15:07:00.399Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 15:07:00.399Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 15:07:00.399Z debug(replica): 0N: on_prepare_timeout: replicating to replica 1
2025-11-24 15:07:00.399Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.399Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.399Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.417Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.418Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.419Z debug(vsr): 0: ping_timeout fired
2025-11-24 15:07:00.419Z debug(vsr): 0: ping_timeout reset
2025-11-24 15:07:00.419Z debug(replica): 0N: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 251814944529202765208088788075380925858, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36112092214204722, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905473, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.419Z debug(replica): 0N: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 251814944529202765208088788075380925858, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36112092214204722, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905473, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.420Z debug(vsr): 0: commit_message_timeout fired
2025-11-24 15:07:00.420Z debug(vsr): 0: commit_message_timeout reset
2025-11-24 15:07:00.420Z debug(replica): 0N: sending commit to replica 1: vsr.message_header.Header.Commit{ .checksum = 107407387196398391061880066771662680578, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 312925206777049643456755361304928977011, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 200, .timestamp_monotonic = 36112092214539612, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.420Z debug(replica): 0N: sending commit to replica 2: vsr.message_header.Header.Commit{ .checksum = 107407387196398391061880066771662680578, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 312925206777049643456755361304928977011, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 200, .timestamp_monotonic = 36112092214539612, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.420Z debug(vsr): 0: start_view_change_message_timeout fired
2025-11-24 15:07:00.420Z debug(vsr): 0: start_view_change_message_timeout reset
2025-11-24 15:07:00.420Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.420Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.420Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:07:00.420Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:07:00.420Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-11-24 15:07:00.420Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-11-24 15:07:00.420Z debug(vsr): 0: upgrade_timeout fired
2025-11-24 15:07:00.420Z debug(vsr): 0: upgrade_timeout reset
2025-11-24 15:07:00.420Z debug(vsr): 0: pulse_timeout fired
2025-11-24 15:07:00.420Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:07:00.438Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.438Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.440Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.441Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.458Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.458Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.461Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.461Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.462Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:00.467Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 15:07:00.467Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 15:07:00.467Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:00.467Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 15:07:00.471Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=78ms
2025-11-24 15:07:00.471Z debug(vsr): 0: prepare_timeout fired
2025-11-24 15:07:00.471Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 15:07:00.471Z debug(vsr): 0: prepare_timeout after=7..5 (rtt=1 min=1 max=1000 attempts=3)
2025-11-24 15:07:00.471Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 15:07:00.471Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 15:07:00.471Z debug(replica): 0N: on_prepare_timeout: replicating to replica 2
2025-11-24 15:07:00.471Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.478Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.478Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.478Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:07:00.478Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:07:00.481Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.481Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.499Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.499Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.501Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.501Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.519Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.519Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.522Z debug(vsr): 0: prepare_timeout fired
2025-11-24 15:07:00.522Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 15:07:00.522Z debug(vsr): 0: prepare_timeout after=5..7 (rtt=1 min=1 max=1000 attempts=4)
2025-11-24 15:07:00.522Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 15:07:00.522Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 15:07:00.522Z debug(replica): 0N: on_prepare_timeout: replicating to replica 1
2025-11-24 15:07:00.522Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.522Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.522Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.522Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:07:00.522Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:07:00.522Z debug(vsr): 0: pulse_timeout fired
2025-11-24 15:07:00.522Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:07:00.539Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.539Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.542Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.542Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.543Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:00.549Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 15:07:00.549Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 15:07:00.549Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:00.549Z warning(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-24 15:07:00.552Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=97ms
2025-11-24 15:07:00.559Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.559Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.562Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.562Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.576Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.576Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:00.577Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:00.579Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.579Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.579Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:07:00.579Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:07:00.582Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.582Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.593Z debug(vsr): 0: prepare_timeout fired
2025-11-24 15:07:00.593Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 15:07:00.593Z debug(vsr): 0: prepare_timeout after=7..28 (rtt=1 min=1 max=1000 attempts=5)
2025-11-24 15:07:00.593Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 15:07:00.593Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 15:07:00.593Z debug(replica): 0N: on_prepare_timeout: replicating to replica 2
2025-11-24 15:07:00.593Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.598Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:00.600Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.600Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.603Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.603Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.620Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.620Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.623Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.623Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.623Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:07:00.623Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:07:00.623Z debug(vsr): 0: pulse_timeout fired
2025-11-24 15:07:00.623Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:07:00.630Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.630Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:00.630Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:00.640Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.640Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.643Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.643Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.649Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 15:07:00.650Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 15:07:00.650Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:00.650Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 15:07:00.653Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=70ms
2025-11-24 15:07:00.660Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.660Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.663Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.663Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.663Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:00.680Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.680Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.680Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:07:00.680Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:07:00.683Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.683Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.700Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.700Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.703Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.703Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.720Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.720Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.723Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 15:07:00.723Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 15:07:00.723Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.723Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.723Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:07:00.723Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:07:00.723Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:00.723Z debug(vsr): 0: pulse_timeout fired
2025-11-24 15:07:00.723Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:07:00.723Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 15:07:00.733Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=82ms
2025-11-24 15:07:00.738Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:00.740Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.740Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.743Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.743Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.760Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.760Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.763Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.763Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.780Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.780Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.780Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:07:00.780Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:07:00.783Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.783Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.801Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.801Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.803Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.803Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.815Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 15:07:00.815Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 15:07:00.816Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:00.816Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 15:07:00.821Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.821Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.824Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=98ms
2025-11-24 15:07:00.824Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.824Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.824Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:07:00.824Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:07:00.824Z debug(vsr): 0: pulse_timeout fired
2025-11-24 15:07:00.824Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:07:00.841Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.841Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.844Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.844Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.845Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:00.861Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.861Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.864Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.864Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.874Z debug(vsr): 0: prepare_timeout fired
2025-11-24 15:07:00.874Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 15:07:00.874Z debug(vsr): 0: prepare_timeout after=28..5 (rtt=1 min=1 max=1000 attempts=6)
2025-11-24 15:07:00.874Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 15:07:00.874Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 15:07:00.874Z debug(replica): 0N: on_prepare_timeout: replicating to replica 1
2025-11-24 15:07:00.874Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.881Z debug(vsr): 2: ping_timeout fired
2025-11-24 15:07:00.881Z debug(vsr): 2: ping_timeout reset
2025-11-24 15:07:00.881Z debug(replica): 2n: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 307927879537255193943383260805229401752, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36112092675903580, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.881Z debug(replica): 2n: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 307927879537255193943383260805229401752, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36112092675903580, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.881Z debug(message_bus): 2: send_message_to_replica: no connection to=1 header=vsr.message_header.Header.Ping{ .checksum = 307927879537255193943383260805229401752, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36112092675903580, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.881Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-24 15:07:00.881Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-24 15:07:00.881Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.881Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.881Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 15:07:00.881Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 15:07:00.881Z debug(vsr): 2: repair_sync_timeout fired
2025-11-24 15:07:00.881Z debug(vsr): 2: repair_sync_timeout reset
2025-11-24 15:07:00.881Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-24 15:07:00.881Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-24 15:07:00.884Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.884Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.901Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.901Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.904Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:00.904Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:00.920Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:00.922Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:00.922Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.922Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 15:07:00.922Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 15:07:00.922Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:00.922Z warning(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-24 15:07:00.924Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=91ms
2025-11-24 15:07:00.924Z debug(vsr): 0: prepare_timeout fired
2025-11-24 15:07:00.924Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 15:07:00.924Z debug(vsr): 0: prepare_timeout after=5..54 (rtt=1 min=1 max=1000 attempts=7)
2025-11-24 15:07:00.924Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 15:07:00.924Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 15:07:00.924Z debug(replica): 0N: on_prepare_timeout: replicating to replica 2
2025-11-24 15:07:00.924Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 324071011354290928302152359153641987094, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:00.924Z debug(vsr): 0: commit_message_timeout fired
2025-11-24 15:07:00.924Z debug(vsr): 0: commit_message_timeout reset
2025-11-24 15:07:00.924Z debug(replica): 0N: sending commit to replica 1: vsr.message_header.Header.Commit{ .checksum = 177815475337988216562245380577866845137, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 312925206777049643456755361304928977011, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 200, .timestamp_monotonic = 36112092719066258, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.924Z debug(replica): 0N: sending commit to replica 2: vsr.message_header.Header.Commit{ .checksum = 177815475337988216562245380577866845137, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 312925206777049643456755361304928977011, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 200, .timestamp_monotonic = 36112092719066258, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:00.924Z debug(vsr): 0: start_view_change_message_timeout fired
2025-11-24 15:07:00.924Z debug(vsr): 0: start_view_change_message_timeout reset
2025-11-24 15:07:00.924Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:02.968Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:02.968Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 15:07:02.968Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 15:07:02.968Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-11-24 15:07:02.968Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-11-24 15:07:02.969Z debug(vsr): 0: pulse_timeout fired
2025-11-24 15:07:02.969Z debug(vsr): 0: pulse_timeout reset
2025-11-24 15:07:00.942Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 15:07:02.969Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 15:07:00.994Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-24 15:07:02.970Z info(supervisor): 2: terminating replica
2025-11-24 15:07:02.970Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:02.970Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:02.970Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:02.972Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:02.972Z debug(replica): 0N: on_request: new request
2025-11-24 15:07:02.972Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 15:07:02.972Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 162165951138150192585670029106983328313, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312925206777049643456755361304928977011, .parent_padding = 0, .prepare_checksum = 324071011354290928302152359153641987094, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit_min = 200, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 15:07:02.972Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 15:07:02.972Z debug(replica): 0N: on_prepare_ok: 2 message(s)
2025-11-24 15:07:02.972Z debug(replica): 0N: on_prepare_ok: quorum received, context=324071011354290928302152359153641987094
2025-11-24 15:07:02.972Z debug(vsr): 0: prepare_timeout stopped
2025-11-24 15:07:02.972Z debug(vsr): 0: primary_abdicate_timeout stopped
2025-11-24 15:07:02.974Z debug(replica): 0N: execute_op: executing view=3 primary=true op=201 checksum=324071011354290928302152359153641987094 (create_transfers)
2025-11-24 15:07:02.974Z debug(replica): 0N: execute_op: commit_timestamp=1763996813519342335 prepare.header.timestamp=1763996817134622428
2025-11-24 15:07:02.983Z warning(faulty_network): recv error (2,6): error.ConnectionResetByPeer
2025-11-24 15:07:02.984Z debug(replica): 0N: execute_op: advancing commit_max=200..201
2025-11-24 15:07:02.984Z debug(replica): 0N: client_table_entry_update: client=255374662039738724781684357862070504472 session=2 request=199
2025-11-24 15:07:02.984Z debug(replica): 0N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 116126704651522001605273429085529721793, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .context = 209558556484901195552116997746044673281, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 201, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:02.984Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 116126704651522001605273429085529721793, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .context = 209558556484901195552116997746044673281, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 201, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:02.984Z debug(forest): entering forest.compact() op=201 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 15:07:02.989Z debug(client_replies): 0: write_reply: wrote (client=255374662039738724781684357862070504472 request=199)
2025-11-24 15:07:02.991Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 180969235032925710693943621489803023469, .checksum_padding = 0, .checksum_body = 85865090390091033397650344763507473192, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 335616, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 23787888007671778367813226116975105261, .parent_padding = 0, .client = 255374662039738724781684357862070504472, .session = 2, .timestamp = 0, .request = 199, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3629326203, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:02.991Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 15:07:02.991Z debug(client_replies): 0: read_reply: start (client=255374662039738724781684357862070504472 reply=116126704651522001605273429085529721793)
2025-11-24 15:07:02.991Z warning(clock): 0: synchronization failed, partitioned (sources=1 samples=1)
2025-11-24 15:07:02.991Z debug(client_replies): 0: read_reply: done (client=255374662039738724781684357862070504472 reply=116126704651522001605273429085529721793)
2025-11-24 15:07:02.991Z debug(replica): 0N: on_request: repeat reply (client=255374662039738724781684357862070504472 request=199)
2025-11-24 15:07:02.991Z debug(replica): 0N: sending reply to client 255374662039738724781684357862070504472: vsr.message_header.Header.Reply{ .checksum = 116126704651522001605273429085529721793, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 180969235032925710693943621489803023469, .request_checksum_padding = 0, .context = 209558556484901195552116997746044673281, .context_padding = 0, .client = 255374662039738724781684357862070504472, .op = 201, .commit = 201, .timestamp = 1763996817134622428, .request = 199, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 15:07:02.993Z info(supervisor): injecting network loss: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 481, .jitter_ms = 50 }, .lose = 2/100, .corrupt = null }
2025-11-24 15:07:03.001Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 15:07:03.001Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 15:07:03.003Z info(supervisor): 0: pausing replica
2025-11-24 15:07:03.013Z info(supervisor): sleeping for 8.094s
2025-11-24 15:07:03.040Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:03.046Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:03.110Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:03.142Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:03.188Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:03.208Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:03.276Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:03.301Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:03.373Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:03.384Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
warning(client): 255374662039738724781684357862070504472: on_reply: slow request, request=199 op=201 size=335616 create_transfers time=6293ms
2025-11-24 15:07:03.441Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:03.443Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-24 15:07:03.468Z warning(faulty_network): send error (2,2): error.ConnectionResetByPeer
2025-11-24 15:07:03.544Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:03.544Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-24 15:07:03.613Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:03.644Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:03.717Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:03.725Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:03.816Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:03.817Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:03.873Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:03.907Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:03.963Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:03.973Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:04.032Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-24 15:07:04.052Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:04.130Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:04.135Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:04.230Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:04.232Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:04.298Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-11-24 15:07:04.326Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:04.388Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:04.404Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-24 15:07:04.457Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-24 15:07:04.509Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:04.511Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:04.592Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:04.614Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:04.662Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:04.678Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:04.720Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:04.761Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:04.818Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-11-24 15:07:04.841Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:04.910Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:04.932Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:04.988Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:05.009Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:05.052Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:05.063Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:05.121Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-11-24 15:07:05.127Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:05.203Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:05.224Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-24 15:07:05.273Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-24 15:07:05.288Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:05.343Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:05.376Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:05.431Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:05.440Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:05.489Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:05.497Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:05.542Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:05.570Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:05.608Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:05.656Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-11-24 15:07:05.710Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:05.751Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:05.787Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:05.842Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:05.878Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:05.939Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-11-24 15:07:05.962Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:06.030Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-24 15:07:06.058Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:06.095Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:06.156Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:06.156Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:06.240Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-24 15:07:06.260Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:06.315Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:06.346Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:06.415Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:06.431Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:06.499Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:06.530Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:06.586Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:06.599Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:06.676Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:06.680Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:06.769Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-11-24 15:07:06.774Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:06.840Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:06.873Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-24 15:07:06.910Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:06.957Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:06.987Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:07.036Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:07.085Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:07.136Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:07.157Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:07.226Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:07.235Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:07.311Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:07.312Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:07.369Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:07.400Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:07.440Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:07.502Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:07.529Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:07.606Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-11-24 15:07:07.630Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:07.678Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-11-24 15:07:07.698Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:07.752Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-24 15:07:07.778Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:07.853Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:07.872Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:07.934Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:07.962Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:08.027Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:08.054Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:08.115Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:08.154Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:08.210Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:08.246Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:08.272Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:08.300Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:08.342Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-11-24 15:07:08.390Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:08.410Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:08.456Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:08.490Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-11-24 15:07:08.540Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-24 15:07:08.599Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
warning(message_bus): 255374662039738724781684357862070504472: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-11-24 15:07:08.641Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:08.674Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:08.729Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:08.732Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:08.820Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:08.823Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:08.887Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:08.889Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:08.953Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:08.997Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:09.010Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:09.075Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:09.111Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:09.172Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:09.173Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:09.243Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-11-24 15:07:09.261Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:09.329Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-24 15:07:09.342Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:09.389Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:09.419Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:09.477Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:09.487Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:09.554Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:09.556Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:09.614Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:09.647Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:09.687Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:09.714Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:09.757Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:09.814Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:09.840Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:09.889Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:09.921Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:09.975Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:10.018Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-11-24 15:07:10.072Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:10.095Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-24 15:07:10.175Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:10.190Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:10.282Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:10.287Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:10.362Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:10.375Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:10.445Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:10.446Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:10.511Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:10.529Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:10.568Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:10.630Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:10.653Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:10.687Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:10.759Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:10.786Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:10.852Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:10.881Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-11-24 15:07:10.944Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2025-11-24 15:07:10.960Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-24 15:07:11.027Z warning(faulty_network): connect failed (1,0): error.ConnectionRefused
2025-11-24 15:07:11.046Z warning(faulty_network): connect failed (2,4): error.ConnectionRefused
2025-11-24 15:07:11.111Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2025-11-24 15:07:11.114Z info(supervisor): healing network
2025-11-24 15:07:11.124Z info(supervisor): sleeping for 4.885s
2025-11-24 15:07:11.141Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-11-24 15:07:11.170Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 15:07:11.234Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-11-24 15:07:11.249Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 15:07:11.317Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-24 15:07:11.338Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 15:07:11.409Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-24 15:07:11.412Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 15:07:11.478Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-24 15:07:11.484Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 15:07:11.555Z warning(faulty_network): connect failed (2,1): error.ConnectionRefused
2025-11-24 15:07:11.565Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 15:07:11.613Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-24 15:07:11.635Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 15:07:11.679Z warning(faulty_network): connect failed (2,2): error.ConnectionRefused
2025-11-24 15:07:11.710Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
