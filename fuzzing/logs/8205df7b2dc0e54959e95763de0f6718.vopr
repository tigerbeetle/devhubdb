7011151950770478268303940535 session=2 request=6
2025-11-17 17:52:52.742Z debug(replica): 1N: on_request: replying to duplicate request
2025-11-17 17:52:52.778Z debug(replica): 2n: execute_op: executing view=1 primary=false op=8 checksum=20732215057688413493214901479122403171 (lookup_accounts)
2025-11-17 17:52:52.778Z debug(replica): 2n: execute_op: commit_timestamp=1763401971449650316 prepare.header.timestamp=1763401971498017481
2025-11-17 17:52:52.778Z debug(client_replies): 1: read_reply: start (client=307563676187011151950770478268303940535 reply=250995263067311418456194341728165108623)
2025-11-17 17:52:52.778Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 250995263067311418456194341728165108623, .checksum_padding = 0, .checksum_body = 163142189146558185265411974777684875497, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 276740660588594861691018980159638949443, .request_checksum_padding = 0, .context = 212858284328073263759856072444790407261, .context_padding = 0, .client = 307563676187011151950770478268303940535, .op = 8, .commit = 8, .timestamp = 1763401971498017481, .request = 6, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:52:52.778Z debug(replica): 2n: client_table_entry_update: client=307563676187011151950770478268303940535 session=2 request=6
2025-11-17 17:52:52.778Z debug(replica): 0n: sending reply to client 307563676187011151950770478268303940535: vsr.message_header.Header.Reply{ .checksum = 250995263067311418456194341728165108623, .checksum_padding = 0, .checksum_body = 163142189146558185265411974777684875497, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 276740660588594861691018980159638949443, .request_checksum_padding = 0, .context = 212858284328073263759856072444790407261, .context_padding = 0, .client = 307563676187011151950770478268303940535, .op = 8, .commit = 8, .timestamp = 1763401971498017481, .request = 6, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:52:52.778Z debug(forest): entering forest.compact() op=8 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-17 17:52:52.778Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276740660588594861691018980159638949443, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 320831262443475319012519573695964522338, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 6, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 55936991, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:52:52.778Z debug(replica): 1N: on_request: replying to duplicate request
2025-11-17 17:52:52.778Z debug(forest): entering forest.compact() op=8 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-17 17:52:52.778Z debug(client_replies): 1: read_reply: busy (client=307563676187011151950770478268303940535 reply=250995263067311418456194341728165108623)
2025-11-17 17:52:52.778Z debug(replica): 1N: on_request: ignoring (client_replies busy)
2025-11-17 17:52:52.778Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276740660588594861691018980159638949443, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 320831262443475319012519573695964522338, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 6, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 55936991, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:52:52.778Z debug(replica): 1N: on_request: replying to duplicate request
2025-11-17 17:52:52.778Z debug(client_replies): 1: read_reply: busy (client=307563676187011151950770478268303940535 reply=250995263067311418456194341728165108623)
2025-11-17 17:52:52.778Z debug(replica): 1N: on_request: ignoring (client_replies busy)
2025-11-17 17:52:52.778Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276740660588594861691018980159638949443, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 320831262443475319012519573695964522338, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 6, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 55936991, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:52:52.778Z debug(replica): 1N: on_request: replying to duplicate request
2025-11-17 17:52:52.778Z debug(client_replies): 1: read_reply: busy (client=307563676187011151950770478268303940535 reply=250995263067311418456194341728165108623)
2025-11-17 17:52:52.778Z debug(replica): 1N: on_request: ignoring (client_replies busy)
2025-11-17 17:52:52.778Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 276740660588594861691018980159638949443, .checksum_padding = 0, .checksum_body = 89618358641903479169915062276417095592, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 320831262443475319012519573695964522338, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 6, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 55936991, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:52:52.778Z debug(replica): 1N: on_request: replying to duplicate request
2025-11-17 17:52:52.778Z debug(client_replies): 1: read_reply: busy (client=307563676187011151950770478268303940535 reply=250995263067311418456194341728165108623)
2025-11-17 17:52:52.778Z debug(replica): 1N: on_request: ignoring (client_replies busy)
2025-11-17 17:52:52.778Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 69138345007743210460571671699233710429, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 133645958909191294593214934557715756573, .parent_padding = 0, .prepare_checksum = 20732215057688413493214901479122403171, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 307563676187011151950770478268303940535, .op = 8, .commit_min = 7, .timestamp = 1763401971498017481, .request = 6, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-17 17:53:05.460Z debug(replica): 1N: on_prepare_ok: not preparing op=8 checksum=20732215057688413493214901479122403171
2025-11-17 17:53:05.460Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.460Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.460Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.460Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:05.460Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:05.461Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=9437184 len=4096 unlocked
2025-11-17 17:53:05.461Z debug(journal): 1: write_header: op=9 sectors[0..4096]
2025-11-17 17:53:05.461Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-17 17:53:05.461Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.461Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.461Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:52:52.780Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=9437184 len=4096 unlocked
2025-11-17 17:53:05.464Z debug(client_replies): 1: read_reply: done (client=307563676187011151950770478268303940535 reply=250995263067311418456194341728165108623)
2025-11-17 17:53:05.465Z debug(replica): 1N: on_request: repeat reply (client=307563676187011151950770478268303940535 request=6)
2025-11-17 17:53:05.301Z warning(faulty_network): recv error (2,6): error.ConnectionResetByPeer
2025-11-17 17:53:05.465Z debug(journal): 0: write_header: op=9 sectors[0..4096]
2025-11-17 17:53:05.465Z debug(replica): 1N: sending reply to client 307563676187011151950770478268303940535: vsr.message_header.Header.Reply{ .checksum = 250995263067311418456194341728165108623, .checksum_padding = 0, .checksum_body = 163142189146558185265411974777684875497, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 276740660588594861691018980159638949443, .request_checksum_padding = 0, .context = 212858284328073263759856072444790407261, .context_padding = 0, .client = 307563676187011151950770478268303940535, .op = 8, .commit = 8, .timestamp = 1763401971498017481, .request = 6, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.465Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 locked
2025-11-17 17:53:05.465Z debug(client_replies): 0: write_reply: wrote (client=307563676187011151950770478268303940535 request=6)
2025-11-17 17:53:05.466Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.466Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.466Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.466Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.466Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.466Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.466Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.466Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.466Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.466Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.471Z debug(replica): 1N: on_request: new request
2025-11-17 17:53:05.471Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 17:53:05.471Z warning(replica): 1N: on_messages: message count=9 suspended=0
2025-11-17 17:53:05.471Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-17 17:53:05.471Z debug(journal): 1: write: view=1 slot=9 op=9 len=2048: 144691170804996908792801217802906376571 complete, marking clean
2025-11-17 17:53:05.471Z debug(replica): 1N: send_prepare_ok: op=9 checksum=144691170804996908792801217802906376571
2025-11-17 17:53:05.471Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 95407386907045248935637935122517062337, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 20732215057688413493214901479122403171, .parent_padding = 0, .prepare_checksum = 144691170804996908792801217802906376571, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 307563676187011151950770478268303940535, .op = 9, .commit_min = 8, .timestamp = 1763401972729181500, .request = 7, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-17 17:53:05.471Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 95407386907045248935637935122517062337, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 20732215057688413493214901479122403171, .parent_padding = 0, .prepare_checksum = 144691170804996908792801217802906376571, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 307563676187011151950770478268303940535, .op = 9, .commit_min = 8, .timestamp = 1763401972729181500, .request = 7, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-17 17:53:05.471Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-17 17:53:05.471Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-17 17:53:05.471Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-17 17:53:05.471Z info(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-11-17 17:53:05.472Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=88ms
2025-11-17 17:53:05.474Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-11-17 17:53:05.474Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-11-17 17:53:05.474Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=0 len=4096 unlocked
2025-11-17 17:53:05.474Z debug(journal): 0: write: view=1 slot=9 op=9 len=2048: 144691170804996908792801217802906376571 complete, marking clean
2025-11-17 17:53:05.474Z debug(replica): 0n: send_prepare_ok: op=9 checksum=144691170804996908792801217802906376571
2025-11-17 17:53:05.474Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 68658370849732235524993078657050355946, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 20732215057688413493214901479122403171, .parent_padding = 0, .prepare_checksum = 144691170804996908792801217802906376571, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 307563676187011151950770478268303940535, .op = 9, .commit_min = 8, .timestamp = 1763401972729181500, .request = 7, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:05.474Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:05.474Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-11-17 17:53:05.474Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-11-17 17:53:05.474Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-11-17 17:53:05.474Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-11-17 17:53:05.474Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.474Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-11-17 17:53:05.474Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.475Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.475Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-11-17 17:53:05.475Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.475Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.929Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-11-17 17:53:05.929Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 291651862331754387120322604790120708581, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 212858284328073263759856072444790407261, .parent_padding = 0, .client = 307563676187011151950770478268303940535, .session = 2, .timestamp = 0, .request = 7, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1220197889, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:05.482Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:05.929Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:05.929Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-11-17 17:53:05.930Z info(message_bus): 1: on_connect: connected to=2
2025-11-17 17:53:05.930Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=68ms
2025-11-17 17:53:05.940Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:05.940Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:05.949Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:05.949Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:05.960Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:05.960Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:05.960Z debug(vsr): 0: journal_repair_timeout fired
2025-11-17 17:53:05.960Z debug(vsr): 0: journal_repair_timeout reset
2025-11-17 17:53:05.970Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:05.970Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:05.980Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:05.980Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:05.990Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:05.990Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:05.990Z debug(vsr): 1: journal_repair_timeout fired
2025-11-17 17:53:05.990Z debug(vsr): 1: journal_repair_timeout reset
2025-11-17 17:53:05.998Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-11-17 17:53:05.998Z info(message_bus): 0: on_connect: connected to=2
2025-11-17 17:53:06.000Z debug(vsr): 1: pulse_timeout fired
2025-11-17 17:53:06.000Z debug(vsr): 1: pulse_timeout reset
2025-11-17 17:53:06.000Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:06.000Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:06.010Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:06.010Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:06.020Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:06.020Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:06.030Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:06.030Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:06.040Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:06.040Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:06.050Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:06.050Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:06.060Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:06.060Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:06.060Z debug(vsr): 0: journal_repair_timeout fired
2025-11-17 17:53:06.060Z debug(vsr): 0: journal_repair_timeout reset
2025-11-17 17:53:06.070Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:06.070Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:06.081Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:06.081Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:06.090Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:06.090Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:06.090Z debug(vsr): 1: journal_repair_timeout fired
2025-11-17 17:53:06.090Z debug(vsr): 1: journal_repair_timeout reset
2025-11-17 17:53:06.100Z debug(vsr): 1: pulse_timeout fired
2025-11-17 17:53:06.100Z debug(vsr): 1: pulse_timeout reset
2025-11-17 17:53:06.101Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:06.101Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:06.110Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:06.110Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:06.121Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:06.121Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:06.131Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:06.131Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:06.141Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:06.141Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:06.151Z debug(vsr): 1: prepare_timeout fired
2025-11-17 17:53:06.151Z debug(vsr): 1: prepare_timeout backing off
2025-11-17 17:53:06.151Z debug(vsr): 1: prepare_timeout after=25..3 (rtt=1 min=1 max=1000 attempts=1)
2025-11-17 17:53:06.151Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-11-17 17:53:06.151Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-11-17 17:53:06.151Z debug(replica): 1N: on_prepare_timeout: replicating to replica 0
2025-11-17 17:53:06.151Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 144691170804996908792801217802906376571, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 20732215057688413493214901479122403171, .parent_padding = 0, .request_checksum = 291651862331754387120322604790120708581, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 307563676187011151950770478268303940535, .op = 9, .commit = 8, .timestamp = 1763401972729181500, .request = 7, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-17 17:53:06.151Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:06.151Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:06.161Z debug(vsr): 0: ping_timeout fired
2025-11-17 17:53:06.161Z debug(vsr): 0: ping_timeout reset
2025-11-17 17:53:06.172Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:07.566Z debug(replica): 0n: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 206163637937119265985290300179784648610, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 14885849465625888, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:07.567Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:07.567Z debug(replica): 0n: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 206163637937119265985290300179784648610, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 14885849465625888, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:07.567Z debug(vsr): 0: start_view_change_message_timeout fired
2025-11-17 17:53:07.567Z debug(vsr): 0: start_view_change_message_timeout reset
2025-11-17 17:53:07.567Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:07.567Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:07.567Z debug(vsr): 0: journal_repair_timeout fired
2025-11-17 17:53:07.567Z debug(vsr): 0: journal_repair_timeout reset
2025-11-17 17:53:07.567Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-11-17 17:53:07.567Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-11-17 17:53:07.577Z debug(vsr): 1: prepare_timeout fired
2025-11-17 17:53:07.577Z debug(vsr): 1: prepare_timeout backing off
2025-11-17 17:53:07.577Z debug(vsr): 1: prepare_timeout after=3..6 (rtt=1 min=1 max=1000 attempts=2)
2025-11-17 17:53:07.577Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-11-17 17:53:07.577Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-11-17 17:53:07.577Z debug(replica): 1N: on_prepare_timeout: replicating to replica 2
2025-11-17 17:53:07.577Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 144691170804996908792801217802906376571, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 20732215057688413493214901479122403171, .parent_padding = 0, .request_checksum = 291651862331754387120322604790120708581, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 307563676187011151950770478268303940535, .op = 9, .commit = 8, .timestamp = 1763401972729181500, .request = 7, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-17 17:53:07.587Z debug(vsr): 1: ping_timeout fired
2025-11-17 17:53:07.587Z debug(vsr): 1: ping_timeout reset
2025-11-17 17:53:07.587Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:08.039Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:08.039Z debug(replica): 1N: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 227290473839780744695982197992585263725, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 14885849938475904, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:08.039Z debug(replica): 1N: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 227290473839780744695982197992585263725, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 14885849938475904, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:08.039Z debug(vsr): 1: commit_message_timeout fired
2025-11-17 17:53:08.039Z debug(vsr): 1: commit_message_timeout reset
2025-11-17 17:53:08.039Z debug(replica): 1N: sending commit to replica 0: vsr.message_header.Header.Commit{ .checksum = 261605265301470015772249225358124534845, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 20732215057688413493214901479122403171, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 8, .timestamp_monotonic = 14885849938681865, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:08.039Z debug(replica): 1N: sending commit to replica 2: vsr.message_header.Header.Commit{ .checksum = 261605265301470015772249225358124534845, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 20732215057688413493214901479122403171, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 8, .timestamp_monotonic = 14885849938681865, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 17:53:08.040Z debug(vsr): 1: start_view_change_message_timeout fired
2025-11-17 17:53:08.040Z debug(vsr): 1: start_view_change_message_timeout reset
2025-11-17 17:53:08.040Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.040Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.040Z debug(vsr): 1: journal_repair_timeout fired
2025-11-17 17:53:08.040Z debug(vsr): 1: journal_repair_timeout reset
2025-11-17 17:53:08.040Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-11-17 17:53:08.040Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-11-17 17:53:08.050Z debug(vsr): 1: pulse_timeout fired
2025-11-17 17:53:08.050Z debug(vsr): 1: pulse_timeout reset
2025-11-17 17:53:05.466Z error(supervisor): 2: replica terminated unexpectedly with process.Child.Term{ .Signal = 9 }
2025-11-17 17:53:08.060Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-17 17:53:08.060Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-17 17:53:08.060Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.060Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.060Z info(supervisor): 0: terminating replica
2025-11-17 17:53:08.080Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.080Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.090Z debug(vsr): 1: prepare_timeout fired
2025-11-17 17:53:08.090Z debug(vsr): 1: prepare_timeout backing off
2025-11-17 17:53:08.090Z debug(vsr): 1: prepare_timeout after=6..4 (rtt=1 min=1 max=1000 attempts=3)
2025-11-17 17:53:08.090Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-11-17 17:53:08.090Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-11-17 17:53:08.090Z debug(replica): 1N: on_prepare_timeout: replicating to replica 0
2025-11-17 17:53:08.090Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 144691170804996908792801217802906376571, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 20732215057688413493214901479122403171, .parent_padding = 0, .request_checksum = 291651862331754387120322604790120708581, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 307563676187011151950770478268303940535, .op = 9, .commit = 8, .timestamp = 1763401972729181500, .request = 7, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-17 17:53:08.101Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.101Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.121Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.121Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.131Z debug(vsr): 1: prepare_timeout fired
2025-11-17 17:53:08.131Z debug(vsr): 1: prepare_timeout backing off
2025-11-17 17:53:08.131Z debug(vsr): 1: prepare_timeout after=4..8 (rtt=1 min=1 max=1000 attempts=4)
2025-11-17 17:53:08.131Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-11-17 17:53:08.131Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-11-17 17:53:08.131Z debug(replica): 1N: on_prepare_timeout: replicating to replica 2
2025-11-17 17:53:08.131Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 144691170804996908792801217802906376571, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 20732215057688413493214901479122403171, .parent_padding = 0, .request_checksum = 291651862331754387120322604790120708581, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 307563676187011151950770478268303940535, .op = 9, .commit = 8, .timestamp = 1763401972729181500, .request = 7, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-17 17:53:08.142Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.142Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.142Z debug(vsr): 1: journal_repair_timeout fired
2025-11-17 17:53:08.142Z debug(vsr): 1: journal_repair_timeout reset
2025-11-17 17:53:08.152Z debug(vsr): 1: pulse_timeout fired
2025-11-17 17:53:08.152Z debug(vsr): 1: pulse_timeout reset
2025-11-17 17:53:08.162Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.163Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.183Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.183Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.203Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.203Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.213Z debug(vsr): 1: prepare_timeout fired
2025-11-17 17:53:08.213Z debug(vsr): 1: prepare_timeout backing off
2025-11-17 17:53:08.213Z debug(vsr): 1: prepare_timeout after=8..5 (rtt=1 min=1 max=1000 attempts=5)
2025-11-17 17:53:08.213Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-11-17 17:53:08.213Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-11-17 17:53:08.213Z debug(replica): 1N: on_prepare_timeout: replicating to replica 0
2025-11-17 17:53:08.213Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 144691170804996908792801217802906376571, .checksum_padding = 0, .checksum_body = 71337854473999541367526457703787389717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2048, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 20732215057688413493214901479122403171, .parent_padding = 0, .request_checksum = 291651862331754387120322604790120708581, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 307563676187011151950770478268303940535, .op = 9, .commit = 8, .timestamp = 1763401972729181500, .request = 7, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-11-17 17:53:08.223Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.223Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.243Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 17:53:08.243Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 17:53:08.243Z debug(vsr): 1: journal_repair_timeout fired
2025-11-17 17:53:08.243Z debug(vsr): 1: journal_repair_timeout reset
2025-11-17 17:53:08.253Z debug(vsr): 1: pulse_timeout fired
2025-11-17 17:53:08.253Z debug(vsr): 1: pulse_timeout reset
2025-11-17 17:53:08.264Z debug(vsr): 1: prepare_timeout fired
2025-11-17 17:53:08.264Z debug(vsr): 1: prepare_timeout backing off
2025-11-17 17:53:08.264Z debug(vsr): 1: prepare_timeout after=5..26 (rtt=1 min=1 max=1000 attempts=6)
2025-11-17 17:53:08.264Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-11-17 17:53:08.264Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-11-17 17:53:08.472Z info(supervisor): 1: terminating replica
2025-11-17 17:53:08.993Z error: TestFailed
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:425:25: 0x1313dab in run (vortex)
                        return error.TestFailed;
                        ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:207:5: 0x1317017 in main (vortex)
    try supervisor.run();
    ^
/root/tigerbeetle/working/main/src/vortex.zig:61:42: 0x1328893 in main (vortex)
        .supervisor => |supervisor_args| try Supervisor.main(allocator, supervisor_args),
                                         ^
