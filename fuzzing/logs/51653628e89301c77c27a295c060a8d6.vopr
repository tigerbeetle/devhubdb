pair_budget_timeout reset
2025-11-21 14:14:44.902Z debug(client_replies): 0: write_reply: wrote (client=243817616567816617951908231182086174208 request=122)
2025-11-21 14:14:44.902Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=131072000 len=868352 unlocked
2025-11-21 14:14:44.902Z debug(journal): 1: write_header: op=125 sectors[28672..32768]
2025-11-21 14:14:44.902Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 locked
2025-11-21 14:14:44.902Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 unlocked
2025-11-21 14:14:44.902Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=131072000 len=868352 unlocked
2025-11-21 14:14:44.921Z debug(vsr): 2: commit_message_timeout fired
2025-11-21 14:14:44.902Z debug(journal): 1: write: view=2 slot=125 op=125 len=867840: 330002756715031635083948435367268611252 complete, marking clean
2025-11-21 14:14:47.220Z debug(journal): 0: write_header: op=125 sectors[28672..32768]
2025-11-21 14:14:47.220Z debug(vsr): 2: commit_message_timeout reset
2025-11-21 14:14:47.220Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 locked
2025-11-21 14:14:47.220Z debug(replica): 1n: send_prepare_ok: op=125 checksum=330002756715031635083948435367268611252
2025-11-21 14:14:47.220Z debug(replica): 2N: sending commit to replica 0: vsr.message_header.Header.Commit{ .checksum = 92537678017640338605317605908220955171, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 104758529840829115134059307449034571791, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 124, .timestamp_monotonic = 35849467512385920, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.220Z debug(replica): 1n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 166026603518646993008702245130714663632, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 104758529840829115134059307449034571791, .parent_padding = 0, .prepare_checksum = 330002756715031635083948435367268611252, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 125, .commit_min = 124, .timestamp = 1763734484893024080, .request = 123, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.220Z debug(replica): 2N: sending commit to replica 1: vsr.message_header.Header.Commit{ .checksum = 92537678017640338605317605908220955171, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 104758529840829115134059307449034571791, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 124, .timestamp_monotonic = 35849467512385920, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.220Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 unlocked
2025-11-21 14:14:47.220Z debug(journal): 0: write: view=2 slot=125 op=125 len=867840: 330002756715031635083948435367268611252 complete, marking clean
2025-11-21 14:14:47.220Z debug(replica): 0n: send_prepare_ok: op=125 checksum=330002756715031635083948435367268611252
2025-11-21 14:14:47.220Z debug(replica): 0n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 115360486154974312306207157526153051920, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 104758529840829115134059307449034571791, .parent_padding = 0, .prepare_checksum = 330002756715031635083948435367268611252, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 125, .commit_min = 124, .timestamp = 1763734484893024080, .request = 123, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.220Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-21 14:14:47.220Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-21 14:14:47.220Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:47.220Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:47.220Z debug(vsr): 2: journal_repair_timeout fired
2025-11-21 14:14:47.220Z debug(vsr): 2: journal_repair_timeout reset
2025-11-21 14:14:47.220Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Commit{ .checksum = 92537678017640338605317605908220955171, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 104758529840829115134059307449034571791, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 124, .timestamp_monotonic = 35849467512385920, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.220Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Commit{ .checksum = 92537678017640338605317605908220955171, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 104758529840829115134059307449034571791, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 124, .timestamp_monotonic = 35849467512385920, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.221Z debug(vsr): 0: normal_heartbeat_timeout reset
2025-11-21 14:14:47.221Z debug(vsr): 1: normal_heartbeat_timeout reset
2025-11-21 14:14:47.221Z debug(replica): 0n: on_commit: checksum verified
2025-11-21 14:14:47.221Z debug(replica): 1n: on_commit: checksum verified
2025-11-21 14:14:47.221Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-21 14:14:47.221Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-21 14:14:47.221Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 166026603518646993008702245130714663632, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 104758529840829115134059307449034571791, .parent_padding = 0, .prepare_checksum = 330002756715031635083948435367268611252, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 125, .commit_min = 124, .timestamp = 1763734484893024080, .request = 123, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.221Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-21 14:14:47.221Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2025-11-21 14:14:47.221Z debug(replica): 2N: on_prepare_ok: quorum received, context=330002756715031635083948435367268611252
2025-11-21 14:14:47.221Z debug(vsr): 2: prepare_timeout stopped
2025-11-21 14:14:47.221Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-11-21 14:14:47.222Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 115360486154974312306207157526153051920, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 104758529840829115134059307449034571791, .parent_padding = 0, .prepare_checksum = 330002756715031635083948435367268611252, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 125, .commit_min = 124, .timestamp = 1763734484893024080, .request = 123, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.222Z debug(replica): 2N: on_prepare_ok: 3 message(s)
2025-11-21 14:14:47.222Z debug(replica): 2N: on_prepare_ok: ignoring (quorum received already)
2025-11-21 14:14:47.224Z debug(replica): 2N: execute_op: executing view=2 primary=true op=125 checksum=330002756715031635083948435367268611252 (create_transfers)
2025-11-21 14:14:47.224Z debug(replica): 2N: execute_op: commit_timestamp=1763734484835540898 prepare.header.timestamp=1763734484893024080
2025-11-21 14:14:47.224Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 113978213020109400429195985864521269425, .checksum_padding = 0, .checksum_body = 244094086048493021474636042024767745549, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 867840, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 208315658551819947669219287083016706590, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 123, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43880321, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.224Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:14:47.224Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 113978213020109400429195985864521269425, .checksum_padding = 0, .checksum_body = 244094086048493021474636042024767745549, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 867840, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 208315658551819947669219287083016706590, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 123, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43880321, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.231Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:47.231Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:47.231Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:47.231Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:47.249Z debug(replica): 2N: execute_op: advancing commit_max=124..125
2025-11-21 14:14:47.249Z debug(replica): 2N: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=123
2025-11-21 14:14:47.249Z debug(replica): 2N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 227174293761102900971787540486870324156, .checksum_padding = 0, .checksum_body = 126028430488729450552476907461189298401, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 113978213020109400429195985864521269425, .request_checksum_padding = 0, .context = 248679553676749655464759667536693079194, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 125, .commit = 125, .timestamp = 1763734484893024080, .request = 123, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.249Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 227174293761102900971787540486870324156, .checksum_padding = 0, .checksum_body = 126028430488729450552476907461189298401, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 113978213020109400429195985864521269425, .request_checksum_padding = 0, .context = 248679553676749655464759667536693079194, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 125, .commit = 125, .timestamp = 1763734484893024080, .request = 123, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.249Z debug(forest): entering forest.compact() op=125 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 243817616567816617951908231182086174208: on_reply: slow request, request=123 op=125 size=867840 create_transfers time=2365ms
2025-11-21 14:14:47.251Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:47.251Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:47.251Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:47.251Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:47.262Z debug(client_replies): 2: write_reply: wrote (client=243817616567816617951908231182086174208 request=123)
2025-11-21 14:14:47.266Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 113978213020109400429195985864521269425, .checksum_padding = 0, .checksum_body = 244094086048493021474636042024767745549, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 867840, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 208315658551819947669219287083016706590, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 123, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43880321, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.266Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:14:47.266Z debug(client_replies): 2: read_reply: start (client=243817616567816617951908231182086174208 reply=227174293761102900971787540486870324156)
2025-11-21 14:14:47.270Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 113978213020109400429195985864521269425, .checksum_padding = 0, .checksum_body = 244094086048493021474636042024767745549, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 867840, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 208315658551819947669219287083016706590, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 123, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43880321, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.270Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:14:47.270Z debug(client_replies): 2: read_reply: busy (client=243817616567816617951908231182086174208 reply=227174293761102900971787540486870324156)
2025-11-21 14:14:47.270Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2025-11-21 14:14:47.270Z debug(client_replies): 2: read_reply: done (client=243817616567816617951908231182086174208 reply=227174293761102900971787540486870324156)
2025-11-21 14:14:47.270Z debug(replica): 2N: on_request: repeat reply (client=243817616567816617951908231182086174208 request=123)
2025-11-21 14:14:47.270Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 227174293761102900971787540486870324156, .checksum_padding = 0, .checksum_body = 126028430488729450552476907461189298401, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 113978213020109400429195985864521269425, .request_checksum_padding = 0, .context = 248679553676749655464759667536693079194, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 125, .commit = 125, .timestamp = 1763734484893024080, .request = 123, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.271Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:47.271Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:47.271Z debug(vsr): 0: journal_repair_timeout fired
2025-11-21 14:14:47.271Z debug(vsr): 0: journal_repair_timeout reset
2025-11-21 14:14:47.271Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:47.271Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:47.272Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:47.272Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:47.273Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183793737098881381658201186652049961026, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 248679553676749655464759667536693079194, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 124, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2365360898, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:47.273Z debug(replica): 2N: primary_pipeline_prepare: request checksum=183793737098881381658201186652049961026 client=243817616567816617951908231182086174208
2025-11-21 14:14:47.273Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183793737098881381658201186652049961026, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 248679553676749655464759667536693079194, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 124, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2365360898, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=9630121651004175730687618018424484769 op=126
2025-11-21 14:14:47.273Z debug(vsr): 2: prepare_timeout started
2025-11-21 14:14:47.273Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:14:47.273Z debug(vsr): 2: primary_abdicate_timeout started
2025-11-21 14:14:47.273Z debug(vsr): 2: pulse_timeout reset
2025-11-21 14:14:47.273Z debug(replica): 2N: replicate: replicating op=126 to replica 0
2025-11-21 14:14:47.273Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 183793737098881381658201186652049961026, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 248679553676749655464759667536693079194, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 124, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2365360898, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(replica): 2N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 9630121651004175730687618018424484769, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .request_checksum = 183793737098881381658201186652049961026, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(replica): 2N: replicate: replicating op=126 to replica 1
2025-11-21 14:14:47.273Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 9630121651004175730687618018424484769, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .request_checksum = 183793737098881381658201186652049961026, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(replica): 2N: on_prepare: advancing: op=125..126 checksum=330002756715031635083948435367268611252..9630121651004175730687618018424484769
2025-11-21 14:14:47.273Z debug(journal): 2: set_header_as_dirty: op=126 checksum=9630121651004175730687618018424484769
2025-11-21 14:14:47.273Z debug(replica): 2N: append: appending to journal op=126
2025-11-21 14:14:47.273Z debug(journal): 2: write: view=2 slot=126 op=126 len=2320: 9630121651004175730687618018424484769 starting
2025-11-21 14:14:47.273Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 9630121651004175730687618018424484769, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .request_checksum = 183793737098881381658201186652049961026, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=132120576 len=4096 locked
2025-11-21 14:14:47.273Z debug(replica): 0n: on_prepare: advancing commit_max=124..125
2025-11-21 14:14:47.273Z debug(replica): 0n: on_prepare: caching prepare.op=126 (commit_min=124 op=125 commit_max=125 prepare_max=1007)
2025-11-21 14:14:47.273Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 9630121651004175730687618018424484769, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .request_checksum = 183793737098881381658201186652049961026, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(replica): 1n: on_prepare: advancing commit_max=124..125
2025-11-21 14:14:47.273Z debug(replica): 1n: on_prepare: caching prepare.op=126 (commit_min=124 op=125 commit_max=125 prepare_max=1007)
2025-11-21 14:14:47.273Z debug(replica): 0n: on_prepare: advancing: op=125..126 checksum=330002756715031635083948435367268611252..9630121651004175730687618018424484769
2025-11-21 14:14:47.273Z debug(journal): 0: set_header_as_dirty: op=126 checksum=9630121651004175730687618018424484769
2025-11-21 14:14:47.273Z debug(replica): 0n: append: appending to journal op=126
2025-11-21 14:14:47.273Z debug(journal): 0: write: view=2 slot=126 op=126 len=2320: 9630121651004175730687618018424484769 starting
2025-11-21 14:14:47.273Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=132120576 len=4096 locked
2025-11-21 14:14:47.273Z debug(replica): 1n: on_prepare: advancing: op=125..126 checksum=330002756715031635083948435367268611252..9630121651004175730687618018424484769
2025-11-21 14:14:47.273Z debug(journal): 1: set_header_as_dirty: op=126 checksum=9630121651004175730687618018424484769
2025-11-21 14:14:47.273Z debug(replica): 1n: append: appending to journal op=126
2025-11-21 14:14:47.273Z debug(replica): 0n: commit_start_journal: cached prepare op=125 checksum=330002756715031635083948435367268611252
2025-11-21 14:14:47.273Z debug(journal): 1: write: view=2 slot=126 op=126 len=2320: 9630121651004175730687618018424484769 starting
2025-11-21 14:14:47.273Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183793737098881381658201186652049961026, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 248679553676749655464759667536693079194, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 124, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2365360898, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=132120576 len=4096 locked
2025-11-21 14:14:47.273Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:47.273Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:14:47.273Z debug(replica): 1n: commit_start_journal: cached prepare op=125 checksum=330002756715031635083948435367268611252
2025-11-21 14:14:47.273Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=132120576 len=4096 unlocked
2025-11-21 14:14:47.273Z debug(journal): 2: write_header: op=126 sectors[28672..32768]
2025-11-21 14:14:47.273Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 locked
2025-11-21 14:14:47.273Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 unlocked
2025-11-21 14:14:47.273Z debug(journal): 2: write: view=2 slot=126 op=126 len=2320: 9630121651004175730687618018424484769 complete, marking clean
2025-11-21 14:14:47.273Z debug(replica): 2N: send_prepare_ok: op=126 checksum=9630121651004175730687618018424484769
2025-11-21 14:14:47.273Z debug(replica): 2N: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 323706282567891636422085304391426917688, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .prepare_checksum = 9630121651004175730687618018424484769, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit_min = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 323706282567891636422085304391426917688, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .prepare_checksum = 9630121651004175730687618018424484769, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit_min = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.273Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-21 14:14:47.273Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2025-11-21 14:14:47.273Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2025-11-21 14:14:47.275Z debug(replica): 0n: repair_prepare: op=126 checksum=9630121651004175730687618018424484769 (already writing)
2025-11-21 14:14:47.275Z debug(replica): 1n: repair_prepare: op=126 checksum=9630121651004175730687618018424484769 (already writing)
2025-11-21 14:14:47.275Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=124)
2025-11-21 14:14:47.275Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=124)
2025-11-21 14:14:47.277Z debug(replica): 0n: execute_op: executing view=2 primary=false op=125 checksum=330002756715031635083948435367268611252 (create_transfers)
2025-11-21 14:14:47.277Z debug(replica): 0n: execute_op: commit_timestamp=1763734484835540898 prepare.header.timestamp=1763734484893024080
2025-11-21 14:14:47.277Z debug(replica): 1n: execute_op: executing view=2 primary=false op=125 checksum=330002756715031635083948435367268611252 (create_transfers)
2025-11-21 14:14:47.277Z debug(replica): 1n: execute_op: commit_timestamp=1763734484835540898 prepare.header.timestamp=1763734484893024080
2025-11-21 14:14:47.292Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:47.292Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:47.302Z debug(replica): 0n: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=123
2025-11-21 14:14:47.302Z debug(replica): 1n: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=123
2025-11-21 14:14:47.302Z debug(forest): entering forest.compact() op=125 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-21 14:14:47.302Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 227174293761102900971787540486870324156, .checksum_padding = 0, .checksum_body = 126028430488729450552476907461189298401, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 113978213020109400429195985864521269425, .request_checksum_padding = 0, .context = 248679553676749655464759667536693079194, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 125, .commit = 125, .timestamp = 1763734484893024080, .request = 123, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.302Z debug(replica): 0n: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 227174293761102900971787540486870324156, .checksum_padding = 0, .checksum_body = 126028430488729450552476907461189298401, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 384, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 113978213020109400429195985864521269425, .request_checksum_padding = 0, .context = 248679553676749655464759667536693079194, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 125, .commit = 125, .timestamp = 1763734484893024080, .request = 123, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.302Z debug(forest): entering forest.compact() op=125 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-21 14:14:47.312Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:47.312Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:47.315Z debug(vsr): 1: journal_repair_timeout fired
2025-11-21 14:14:47.315Z debug(vsr): 1: journal_repair_timeout reset
2025-11-21 14:14:47.315Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=132120576 len=4096 unlocked
2025-11-21 14:14:47.315Z debug(journal): 0: write_header: op=126 sectors[28672..32768]
2025-11-21 14:14:47.315Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 locked
2025-11-21 14:14:47.315Z debug(client_replies): 0: write_reply: wrote (client=243817616567816617951908231182086174208 request=123)
2025-11-21 14:14:47.315Z debug(replica): 1n: repair_prepare: op=126 checksum=9630121651004175730687618018424484769 (already writing)
2025-11-21 14:14:47.315Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=132120576 len=4096 unlocked
2025-11-21 14:14:47.315Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 unlocked
2025-11-21 14:14:47.315Z debug(journal): 1: write_header: op=126 sectors[28672..32768]
2025-11-21 14:14:47.315Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 locked
2025-11-21 14:14:47.315Z debug(journal): 0: write: view=2 slot=126 op=126 len=2320: 9630121651004175730687618018424484769 complete, marking clean
2025-11-21 14:14:47.315Z debug(client_replies): 1: write_reply: wrote (client=243817616567816617951908231182086174208 request=123)
2025-11-21 14:14:47.315Z debug(replica): 0n: send_prepare_ok: op=126 checksum=9630121651004175730687618018424484769
2025-11-21 14:14:47.315Z debug(replica): 0n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 133717915114598798653725536430582027182, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .prepare_checksum = 9630121651004175730687618018424484769, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit_min = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.315Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 unlocked
2025-11-21 14:14:47.315Z debug(journal): 1: write: view=2 slot=126 op=126 len=2320: 9630121651004175730687618018424484769 complete, marking clean
2025-11-21 14:14:47.315Z debug(replica): 1n: send_prepare_ok: op=126 checksum=9630121651004175730687618018424484769
2025-11-21 14:14:47.315Z debug(replica): 1n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 92053696448519842742635100533593072155, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .prepare_checksum = 9630121651004175730687618018424484769, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit_min = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.316Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 133717915114598798653725536430582027182, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .prepare_checksum = 9630121651004175730687618018424484769, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit_min = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.316Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-21 14:14:47.316Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2025-11-21 14:14:47.316Z debug(replica): 2N: on_prepare_ok: quorum received, context=9630121651004175730687618018424484769
2025-11-21 14:14:47.316Z debug(vsr): 2: prepare_timeout stopped
2025-11-21 14:14:47.316Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-11-21 14:14:47.316Z debug(replica): 2N: execute_op: executing view=2 primary=true op=126 checksum=9630121651004175730687618018424484769 (lookup_accounts)
2025-11-21 14:14:47.316Z debug(replica): 2N: execute_op: commit_timestamp=1763734484893024080 prepare.header.timestamp=1763734487273379530
2025-11-21 14:14:47.316Z debug(replica): 2N: execute_op: advancing commit_max=125..126
2025-11-21 14:14:47.316Z debug(replica): 2N: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=124
2025-11-21 14:14:47.316Z debug(replica): 2N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 77365550943869214885912658566618021606, .checksum_padding = 0, .checksum_body = 160054490748456122850509773005473820872, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183793737098881381658201186652049961026, .request_checksum_padding = 0, .context = 138472267244259895241156921744282186236, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit = 126, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.316Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 77365550943869214885912658566618021606, .checksum_padding = 0, .checksum_body = 160054490748456122850509773005473820872, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183793737098881381658201186652049961026, .request_checksum_padding = 0, .context = 138472267244259895241156921744282186236, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit = 126, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.316Z debug(forest): entering forest.compact() op=126 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-21 14:14:47.316Z info(workload): accounts created = 128, transfers = 147300, pending transfers = 0, commands run = 62
2025-11-21 14:14:47.316Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 92053696448519842742635100533593072155, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 330002756715031635083948435367268611252, .parent_padding = 0, .prepare_checksum = 9630121651004175730687618018424484769, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit_min = 125, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.316Z debug(replica): 2N: on_prepare_ok: not preparing op=126 checksum=9630121651004175730687618018424484769
2025-11-21 14:14:47.316Z debug(client_replies): 2: write_reply: wrote (client=243817616567816617951908231182086174208 request=124)
2025-11-21 14:14:47.325Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:47.325Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:47.325Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:47.325Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:47.331Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 10270657548574614927858685453288906375, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138472267244259895241156921744282186236, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 125, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43531402, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.331Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:47.331Z debug(replica): 2N: primary_pipeline_prepare: request checksum=10270657548574614927858685453288906375 client=243817616567816617951908231182086174208
2025-11-21 14:14:47.331Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 10270657548574614927858685453288906375, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138472267244259895241156921744282186236, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 125, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43531402, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.331Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:14:47.331Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 10270657548574614927858685453288906375, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138472267244259895241156921744282186236, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 125, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43531402, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.336Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=152741066998109468735118760318535158690 op=127
2025-11-21 14:14:47.336Z debug(vsr): 2: prepare_timeout started
2025-11-21 14:14:47.336Z debug(vsr): 2: primary_abdicate_timeout started
2025-11-21 14:14:47.336Z debug(vsr): 2: pulse_timeout reset
2025-11-21 14:14:47.336Z debug(replica): 2N: replicate: replicating op=127 to replica 0
2025-11-21 14:14:47.336Z debug(replica): 2N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 152741066998109468735118760318535158690, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .request_checksum = 10270657548574614927858685453288906375, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.336Z debug(replica): 2N: replicate: replicating op=127 to replica 1
2025-11-21 14:14:47.336Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 152741066998109468735118760318535158690, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .request_checksum = 10270657548574614927858685453288906375, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.336Z debug(replica): 2N: on_prepare: advancing: op=126..127 checksum=9630121651004175730687618018424484769..152741066998109468735118760318535158690
2025-11-21 14:14:47.336Z debug(journal): 2: set_header_as_dirty: op=127 checksum=152741066998109468735118760318535158690
2025-11-21 14:14:47.336Z debug(replica): 2N: append: appending to journal op=127
2025-11-21 14:14:47.336Z debug(journal): 2: write: view=2 slot=127 op=127 len=950784: 152741066998109468735118760318535158690 starting
2025-11-21 14:14:47.336Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=133169152 len=954368 locked
2025-11-21 14:14:47.341Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 152741066998109468735118760318535158690, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .request_checksum = 10270657548574614927858685453288906375, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.341Z debug(replica): 0n: on_prepare: advancing commit_max=125..126
2025-11-21 14:14:47.341Z debug(replica): 0n: on_prepare: caching prepare.op=127 (commit_min=125 op=126 commit_max=126 prepare_max=1007)
2025-11-21 14:14:47.341Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 10270657548574614927858685453288906375, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138472267244259895241156921744282186236, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 125, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43531402, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:47.341Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:47.341Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:14:47.341Z debug(replica): 0n: on_prepare: advancing: op=126..127 checksum=9630121651004175730687618018424484769..152741066998109468735118760318535158690
2025-11-21 14:14:47.341Z debug(journal): 0: set_header_as_dirty: op=127 checksum=152741066998109468735118760318535158690
2025-11-21 14:14:47.341Z debug(replica): 0n: append: appending to journal op=127
2025-11-21 14:14:47.341Z debug(journal): 0: write: view=2 slot=127 op=127 len=950784: 152741066998109468735118760318535158690 starting
2025-11-21 14:14:47.341Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=133169152 len=954368 unlocked
2025-11-21 14:14:47.341Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=133169152 len=954368 locked
2025-11-21 14:14:47.341Z debug(journal): 2: write_header: op=127 sectors[28672..32768]
2025-11-21 14:14:47.341Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 locked
2025-11-21 14:14:47.341Z debug(replica): 0n: commit_start_journal: cached prepare op=126 checksum=9630121651004175730687618018424484769
2025-11-21 14:14:47.342Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:47.342Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:47.342Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 unlocked
2025-11-21 14:14:47.342Z debug(journal): 2: write: view=2 slot=127 op=127 len=950784: 152741066998109468735118760318535158690 complete, marking clean
2025-11-21 14:14:47.342Z debug(replica): 2N: send_prepare_ok: op=127 checksum=152741066998109468735118760318535158690
2025-11-21 14:14:47.342Z debug(replica): 0n: repair_prepare: op=127 checksum=152741066998109468735118760318535158690 (already writing)
2025-11-21 14:14:47.342Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 152741066998109468735118760318535158690, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .request_checksum = 10270657548574614927858685453288906375, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.342Z debug(replica): 1n: on_prepare: advancing commit_max=125..126
2025-11-21 14:14:47.342Z debug(replica): 1n: on_prepare: caching prepare.op=127 (commit_min=125 op=126 commit_max=126 prepare_max=1007)
2025-11-21 14:14:47.342Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=125)
2025-11-21 14:14:47.342Z debug(replica): 1n: on_prepare: advancing: op=126..127 checksum=9630121651004175730687618018424484769..152741066998109468735118760318535158690
2025-11-21 14:14:47.342Z debug(replica): 2N: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 41092284738517503058440585683347620646, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .prepare_checksum = 152741066998109468735118760318535158690, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit_min = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:47.342Z debug(replica): 0n: execute_op: executing view=2 primary=false op=126 checksum=9630121651004175730687618018424484769 (lookup_accounts)
2025-11-21 14:14:49.976Z debug(journal): 1: set_header_as_dirty: op=127 checksum=152741066998109468735118760318535158690
2025-11-21 14:14:49.976Z debug(replica): 1n: append: appending to journal op=127
2025-11-21 14:14:49.976Z debug(replica): 0n: execute_op: commit_timestamp=1763734484893024080 prepare.header.timestamp=1763734487273379530
2025-11-21 14:14:49.976Z debug(journal): 1: write: view=2 slot=127 op=127 len=950784: 152741066998109468735118760318535158690 starting
2025-11-21 14:14:49.976Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 41092284738517503058440585683347620646, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .prepare_checksum = 152741066998109468735118760318535158690, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit_min = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:49.976Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=133169152 len=954368 locked
2025-11-21 14:14:49.976Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-21 14:14:49.976Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2025-11-21 14:14:49.976Z debug(replica): 1n: commit_start_journal: cached prepare op=126 checksum=9630121651004175730687618018424484769
2025-11-21 14:14:49.976Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2025-11-21 14:14:49.977Z warning(clock): 2: synchronization failed, partitioned (sources=1 samples=1)
2025-11-21 14:14:49.977Z debug(replica): 0n: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=124
2025-11-21 14:14:49.977Z debug(forest): entering forest.compact() op=126 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-21 14:14:49.977Z debug(replica): 1n: repair_prepare: op=127 checksum=152741066998109468735118760318535158690 (already writing)
2025-11-21 14:14:49.977Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=125)
2025-11-21 14:14:49.977Z debug(replica): 1n: execute_op: executing view=2 primary=false op=126 checksum=9630121651004175730687618018424484769 (lookup_accounts)
2025-11-21 14:14:49.977Z debug(replica): 1n: execute_op: commit_timestamp=1763734484893024080 prepare.header.timestamp=1763734487273379530
2025-11-21 14:14:49.977Z debug(replica): 1n: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=124
2025-11-21 14:14:49.977Z debug(replica): 1n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 77365550943869214885912658566618021606, .checksum_padding = 0, .checksum_body = 160054490748456122850509773005473820872, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183793737098881381658201186652049961026, .request_checksum_padding = 0, .context = 138472267244259895241156921744282186236, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit = 126, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:49.977Z warning(replica): 0n: commit_dispatch: slow request, request=124 size=2320 lookup_accounts time=2635ms
2025-11-21 14:14:49.977Z debug(replica): 1n: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 77365550943869214885912658566618021606, .checksum_padding = 0, .checksum_body = 160054490748456122850509773005473820872, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183793737098881381658201186652049961026, .request_checksum_padding = 0, .context = 138472267244259895241156921744282186236, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 126, .commit = 126, .timestamp = 1763734487273379530, .request = 124, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:49.977Z debug(forest): entering forest.compact() op=126 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-21 14:14:49.977Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:49.977Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:49.977Z debug(client_replies): 0: write_reply: wrote (client=243817616567816617951908231182086174208 request=124)
2025-11-21 14:14:49.977Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:49.977Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:49.977Z debug(client_replies): 1: write_reply: wrote (client=243817616567816617951908231182086174208 request=124)
2025-11-21 14:14:49.978Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=133169152 len=954368 unlocked
2025-11-21 14:14:49.978Z debug(journal): 1: write_header: op=127 sectors[28672..32768]
2025-11-21 14:14:49.978Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 locked
2025-11-21 14:14:49.978Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 unlocked
2025-11-21 14:14:49.978Z debug(journal): 1: write: view=2 slot=127 op=127 len=950784: 152741066998109468735118760318535158690 complete, marking clean
2025-11-21 14:14:49.978Z debug(replica): 1n: send_prepare_ok: op=127 checksum=152741066998109468735118760318535158690
2025-11-21 14:14:49.978Z debug(replica): 1n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 240103301539685884470977456031711724985, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .prepare_checksum = 152741066998109468735118760318535158690, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit_min = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:49.981Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 10270657548574614927858685453288906375, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138472267244259895241156921744282186236, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 125, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43531402, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:49.981Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:49.981Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:14:49.981Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 240103301539685884470977456031711724985, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .prepare_checksum = 152741066998109468735118760318535158690, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit_min = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:49.981Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-21 14:14:49.981Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2025-11-21 14:14:49.981Z debug(replica): 2N: on_prepare_ok: quorum received, context=152741066998109468735118760318535158690
2025-11-21 14:14:49.981Z debug(vsr): 2: prepare_timeout stopped
2025-11-21 14:14:49.981Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-11-21 14:14:49.981Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 10270657548574614927858685453288906375, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138472267244259895241156921744282186236, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 125, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43531402, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:49.981Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:14:49.981Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 10270657548574614927858685453288906375, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138472267244259895241156921744282186236, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 125, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43531402, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:49.982Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=133169152 len=954368 unlocked
2025-11-21 14:14:49.982Z debug(journal): 0: write_header: op=127 sectors[28672..32768]
2025-11-21 14:14:49.982Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 locked
2025-11-21 14:14:49.982Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=28672 len=4096 unlocked
2025-11-21 14:14:49.982Z debug(journal): 0: write: view=2 slot=127 op=127 len=950784: 152741066998109468735118760318535158690 complete, marking clean
2025-11-21 14:14:49.982Z debug(replica): 0n: send_prepare_ok: op=127 checksum=152741066998109468735118760318535158690
2025-11-21 14:14:49.982Z debug(replica): 0n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 216243625584350387492863827481537409921, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .prepare_checksum = 152741066998109468735118760318535158690, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit_min = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:49.984Z debug(replica): 2N: execute_op: executing view=2 primary=true op=127 checksum=152741066998109468735118760318535158690 (create_transfers)
2025-11-21 14:14:49.984Z debug(replica): 2N: execute_op: commit_timestamp=1763734487273379530 prepare.header.timestamp=1763734487331721087
2025-11-21 14:14:49.997Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:49.997Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:49.997Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:49.997Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.012Z debug(replica): 2N: execute_op: advancing commit_max=126..127
2025-11-21 14:14:50.012Z debug(replica): 2N: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=125
2025-11-21 14:14:50.012Z debug(replica): 2N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 316310883546601932513397946834569295766, .checksum_padding = 0, .checksum_body = 246772979246947312926126037105083667280, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 280, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 10270657548574614927858685453288906375, .request_checksum_padding = 0, .context = 51320505015944716928197161233515872830, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit = 127, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.012Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 316310883546601932513397946834569295766, .checksum_padding = 0, .checksum_body = 246772979246947312926126037105083667280, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 280, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 10270657548574614927858685453288906375, .request_checksum_padding = 0, .context = 51320505015944716928197161233515872830, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit = 127, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.012Z debug(forest): entering forest.compact() op=127 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=true
2025-11-21 14:14:50.012Z debug(manifest_log): 2: flush: writing 0 block(s)
warning(client): 243817616567816617951908231182086174208: on_reply: slow request, request=125 op=127 size=950784 create_transfers time=2690ms
2025-11-21 14:14:50.017Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.017Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.017Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.017Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.028Z debug(forest): swap_mutable_and_immutable(Account.id)
2025-11-21 14:14:50.028Z debug(forest): swap_mutable_and_immutable(Account.user_data_128)
2025-11-21 14:14:50.028Z debug(forest): swap_mutable_and_immutable(Account.user_data_64)
2025-11-21 14:14:50.028Z debug(forest): swap_mutable_and_immutable(Account.user_data_32)
2025-11-21 14:14:50.028Z debug(forest): swap_mutable_and_immutable(Account.ledger)
2025-11-21 14:14:50.028Z debug(forest): swap_mutable_and_immutable(Account.code)
2025-11-21 14:14:50.028Z debug(forest): swap_mutable_and_immutable(Account)
2025-11-21 14:14:50.028Z debug(forest): swap_mutable_and_immutable(Transfer.id)
2025-11-21 14:14:50.037Z debug(vsr): 0: start_view_change_message_timeout fired
2025-11-21 14:14:50.037Z debug(vsr): 0: start_view_change_message_timeout reset
2025-11-21 14:14:50.037Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.037Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.037Z debug(vsr): 0: journal_repair_timeout fired
2025-11-21 14:14:50.037Z debug(vsr): 0: journal_repair_timeout reset
2025-11-21 14:14:50.037Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-11-21 14:14:50.037Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-11-21 14:14:50.037Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.037Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.039Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 162031021886803528938247614646857030259, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 51320505015944716928197161233515872830, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 126, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2690925502, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.039Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:14:50.039Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 162031021886803528938247614646857030259, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 51320505015944716928197161233515872830, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 126, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2690925502, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.047Z debug(vsr): 1: journal_repair_timeout fired
2025-11-21 14:14:50.047Z debug(vsr): 1: journal_repair_timeout reset
2025-11-21 14:14:50.050Z debug(forest): swap_mutable_and_immutable(Transfer.debit_account_id)
2025-11-21 14:14:50.057Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.057Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.058Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.058Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.077Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.078Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.078Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.078Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.088Z debug(forest): swap_mutable_and_immutable(Transfer.credit_account_id)
2025-11-21 14:14:50.098Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.098Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.098Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.098Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.118Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.118Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.118Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.118Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.126Z debug(forest): swap_mutable_and_immutable(Transfer.amount)
2025-11-21 14:14:50.138Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.138Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.138Z debug(vsr): 0: journal_repair_timeout fired
2025-11-21 14:14:50.138Z debug(vsr): 0: journal_repair_timeout reset
2025-11-21 14:14:50.138Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.138Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.148Z debug(vsr): 1: journal_repair_timeout fired
2025-11-21 14:14:50.148Z debug(vsr): 1: journal_repair_timeout reset
2025-11-21 14:14:50.158Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.158Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.158Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.158Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.165Z debug(forest): swap_mutable_and_immutable(Transfer.pending_id)
2025-11-21 14:14:50.165Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_128)
2025-11-21 14:14:50.165Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_64)
2025-11-21 14:14:50.165Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_32)
2025-11-21 14:14:50.165Z debug(forest): swap_mutable_and_immutable(Transfer.ledger)
2025-11-21 14:14:50.178Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.178Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.178Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.178Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.184Z debug(forest): swap_mutable_and_immutable(Transfer.code)
2025-11-21 14:14:50.198Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.198Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.198Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.198Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.214Z debug(forest): swap_mutable_and_immutable(Transfer)
2025-11-21 14:14:50.218Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.218Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.218Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.218Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.238Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.238Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.238Z debug(vsr): 0: journal_repair_timeout fired
2025-11-21 14:14:50.238Z debug(vsr): 0: journal_repair_timeout reset
2025-11-21 14:14:50.238Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.238Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.242Z debug(forest): swap_mutable_and_immutable(Transfer.expires_at)
2025-11-21 14:14:50.242Z debug(forest): swap_mutable_and_immutable(TransferPending)
2025-11-21 14:14:50.242Z debug(forest): swap_mutable_and_immutable(TransferPending.status)
2025-11-21 14:14:50.242Z debug(forest): swap_mutable_and_immutable(AccountEvent)
2025-11-21 14:14:50.248Z debug(vsr): 1: start_view_change_message_timeout fired
2025-11-21 14:14:50.248Z debug(vsr): 1: start_view_change_message_timeout reset
2025-11-21 14:14:50.248Z debug(vsr): 1: journal_repair_timeout fired
2025-11-21 14:14:50.248Z debug(vsr): 1: journal_repair_timeout reset
2025-11-21 14:14:50.248Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-11-21 14:14:50.248Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-11-21 14:14:50.258Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.258Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.258Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.258Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.274Z debug(forest): swap_mutable_and_immutable(Account.imported)
2025-11-21 14:14:50.274Z debug(forest): swap_mutable_and_immutable(Transfer.imported)
2025-11-21 14:14:50.274Z debug(forest): swap_mutable_and_immutable(Account.closed)
2025-11-21 14:14:50.274Z debug(forest): swap_mutable_and_immutable(Transfer.closing)
2025-11-21 14:14:50.274Z debug(forest): swap_mutable_and_immutable(AccountEvent.account_timestamp)
2025-11-21 14:14:50.278Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.278Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.278Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.278Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.284Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_status)
2025-11-21 14:14:50.298Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:50.298Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:50.299Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:14:50.299Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:14:50.303Z debug(forest): swap_mutable_and_immutable(AccountEvent.dr_account_id_expired)
2025-11-21 14:14:50.303Z debug(forest): swap_mutable_and_immutable(AccountEvent.cr_account_id_expired)
2025-11-21 14:14:50.303Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_id_expired)
2025-11-21 14:14:50.303Z debug(forest): swap_mutable_and_immutable(AccountEvent.ledger_expired)
2025-11-21 14:14:50.303Z debug(forest): swap_mutable_and_immutable(AccountEvent.prunable)
2025-11-21 14:14:50.315Z warning(replica): 2N: commit_dispatch: slow request, request=125 size=950784 create_transfers time=334ms
2025-11-21 14:14:50.315Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 162031021886803528938247614646857030259, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 51320505015944716928197161233515872830, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 126, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2690925502, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.315Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:50.315Z debug(replica): 2N: primary_pipeline_prepare: request checksum=162031021886803528938247614646857030259 client=243817616567816617951908231182086174208
2025-11-21 14:14:50.315Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=101446382550583357760015182284154118271 op=128
2025-11-21 14:14:50.315Z debug(vsr): 2: prepare_timeout started
2025-11-21 14:14:50.315Z debug(vsr): 2: primary_abdicate_timeout started
2025-11-21 14:14:50.315Z debug(vsr): 2: pulse_timeout reset
2025-11-21 14:14:50.315Z debug(replica): 2N: replicate: replicating op=128 to replica 0
2025-11-21 14:14:50.315Z debug(replica): 2N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 101446382550583357760015182284154118271, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:50.315Z debug(replica): 2N: replicate: replicating op=128 to replica 1
2025-11-21 14:14:50.315Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 101446382550583357760015182284154118271, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:50.315Z debug(replica): 2N: on_prepare: advancing: op=127..128 checksum=152741066998109468735118760318535158690..101446382550583357760015182284154118271
2025-11-21 14:14:50.315Z debug(journal): 2: set_header_as_dirty: op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:50.315Z debug(replica): 2N: append: appending to journal op=128
2025-11-21 14:14:50.315Z debug(journal): 2: write: view=2 slot=128 op=128 len=2320: 101446382550583357760015182284154118271 starting
2025-11-21 14:14:50.315Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=134217728 len=4096 locked
2025-11-21 14:14:50.315Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 101446382550583357760015182284154118271, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:50.315Z debug(replica): 0n: on_prepare: advancing commit_max=126..127
2025-11-21 14:14:50.315Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 162031021886803528938247614646857030259, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 51320505015944716928197161233515872830, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 126, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2690925502, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.315Z debug(replica): 0n: on_prepare: caching prepare.op=128 (commit_min=126 op=127 commit_max=127 prepare_max=1007)
2025-11-21 14:14:50.315Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:50.315Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:14:50.315Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 101446382550583357760015182284154118271, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:50.316Z debug(replica): 1n: on_prepare: advancing commit_max=126..127
2025-11-21 14:14:50.316Z debug(replica): 1n: on_prepare: caching prepare.op=128 (commit_min=126 op=127 commit_max=127 prepare_max=1007)
2025-11-21 14:14:50.316Z debug(replica): 0n: on_prepare: advancing: op=127..128 checksum=152741066998109468735118760318535158690..101446382550583357760015182284154118271
2025-11-21 14:14:50.316Z debug(journal): 0: set_header_as_dirty: op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:50.316Z debug(replica): 0n: append: appending to journal op=128
2025-11-21 14:14:50.316Z debug(journal): 0: write: view=2 slot=128 op=128 len=2320: 101446382550583357760015182284154118271 starting
2025-11-21 14:14:50.316Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=134217728 len=4096 locked
2025-11-21 14:14:50.316Z debug(replica): 0n: commit_start_journal: cached prepare op=127 checksum=152741066998109468735118760318535158690
2025-11-21 14:14:50.316Z debug(replica): 1n: on_prepare: advancing: op=127..128 checksum=152741066998109468735118760318535158690..101446382550583357760015182284154118271
2025-11-21 14:14:50.316Z debug(journal): 1: set_header_as_dirty: op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:50.316Z debug(replica): 1n: append: appending to journal op=128
2025-11-21 14:14:50.316Z debug(journal): 1: write: view=2 slot=128 op=128 len=2320: 101446382550583357760015182284154118271 starting
2025-11-21 14:14:50.316Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=134217728 len=4096 locked
2025-11-21 14:14:50.316Z debug(replica): 1n: commit_start_journal: cached prepare op=127 checksum=152741066998109468735118760318535158690
2025-11-21 14:14:50.316Z debug(client_replies): 2: write_reply: wrote (client=243817616567816617951908231182086174208 request=125)
2025-11-21 14:14:50.316Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=134217728 len=4096 unlocked
2025-11-21 14:14:50.316Z debug(journal): 2: write_header: op=128 sectors[32768..36864]
2025-11-21 14:14:50.316Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2025-11-21 14:14:50.316Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2025-11-21 14:14:50.316Z debug(journal): 2: write: view=2 slot=128 op=128 len=2320: 101446382550583357760015182284154118271 complete, marking clean
2025-11-21 14:14:50.316Z debug(replica): 2N: send_prepare_ok: op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:50.316Z debug(replica): 2N: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 83093630970765182268045027153344056193, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .prepare_checksum = 101446382550583357760015182284154118271, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit_min = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:50.316Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 83093630970765182268045027153344056193, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .prepare_checksum = 101446382550583357760015182284154118271, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit_min = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:50.316Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-21 14:14:50.316Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2025-11-21 14:14:50.316Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2025-11-21 14:14:50.317Z debug(replica): 0n: repair_prepare: op=128 checksum=101446382550583357760015182284154118271 (already writing)
2025-11-21 14:14:50.317Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=126)
2025-11-21 14:14:50.317Z debug(replica): 1n: repair_prepare: op=128 checksum=101446382550583357760015182284154118271 (already writing)
2025-11-21 14:14:50.317Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=126)
2025-11-21 14:14:50.319Z debug(replica): 0n: execute_op: executing view=2 primary=false op=127 checksum=152741066998109468735118760318535158690 (create_transfers)
2025-11-21 14:14:50.319Z debug(replica): 0n: execute_op: commit_timestamp=1763734487273379530 prepare.header.timestamp=1763734487331721087
2025-11-21 14:14:50.319Z debug(replica): 1n: execute_op: executing view=2 primary=false op=127 checksum=152741066998109468735118760318535158690 (create_transfers)
2025-11-21 14:14:50.319Z debug(replica): 1n: execute_op: commit_timestamp=1763734487273379530 prepare.header.timestamp=1763734487331721087
2025-11-21 14:14:50.320Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 10270657548574614927858685453288906375, .checksum_padding = 0, .checksum_body = 230988528584550911914125112885285748484, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 950784, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138472267244259895241156921744282186236, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 125, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 43531402, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.320Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:14:50.320Z debug(client_replies): 2: read_reply: start (client=243817616567816617951908231182086174208 reply=316310883546601932513397946834569295766)
2025-11-21 14:14:50.320Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 216243625584350387492863827481537409921, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 9630121651004175730687618018424484769, .parent_padding = 0, .prepare_checksum = 152741066998109468735118760318535158690, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit_min = 126, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:50.320Z debug(replica): 2N: on_prepare_ok: not preparing op=127 checksum=152741066998109468735118760318535158690
2025-11-21 14:14:50.320Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.320Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.320Z debug(vsr): 2: journal_repair_timeout fired
2025-11-21 14:14:50.320Z debug(vsr): 2: journal_repair_timeout reset
2025-11-21 14:14:50.320Z debug(client_replies): 2: read_reply: done (client=243817616567816617951908231182086174208 reply=316310883546601932513397946834569295766)
2025-11-21 14:14:50.320Z debug(replica): 2N: on_request: repeat reply (client=243817616567816617951908231182086174208 request=125)
2025-11-21 14:14:50.320Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 316310883546601932513397946834569295766, .checksum_padding = 0, .checksum_body = 246772979246947312926126037105083667280, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 280, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 10270657548574614927858685453288906375, .request_checksum_padding = 0, .context = 51320505015944716928197161233515872830, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit = 127, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.340Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.340Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.346Z debug(replica): 0n: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=125
2025-11-21 14:14:50.346Z debug(forest): entering forest.compact() op=127 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=true
2025-11-21 14:14:50.346Z debug(manifest_log): 0: flush: writing 0 block(s)
2025-11-21 14:14:50.347Z debug(replica): 1n: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=125
2025-11-21 14:14:50.347Z debug(replica): 1n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 316310883546601932513397946834569295766, .checksum_padding = 0, .checksum_body = 246772979246947312926126037105083667280, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 280, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 10270657548574614927858685453288906375, .request_checksum_padding = 0, .context = 51320505015944716928197161233515872830, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit = 127, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.347Z debug(replica): 1n: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 316310883546601932513397946834569295766, .checksum_padding = 0, .checksum_body = 246772979246947312926126037105083667280, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 280, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 10270657548574614927858685453288906375, .request_checksum_padding = 0, .context = 51320505015944716928197161233515872830, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 127, .commit = 127, .timestamp = 1763734487331721087, .request = 125, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:50.347Z debug(forest): entering forest.compact() op=127 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=true
2025-11-21 14:14:50.347Z debug(manifest_log): 1: flush: writing 0 block(s)
2025-11-21 14:14:50.360Z debug(forest): swap_mutable_and_immutable(Account.id)
2025-11-21 14:14:50.360Z debug(forest): swap_mutable_and_immutable(Account.user_data_128)
2025-11-21 14:14:50.360Z debug(forest): swap_mutable_and_immutable(Account.user_data_64)
2025-11-21 14:14:50.360Z debug(forest): swap_mutable_and_immutable(Account.user_data_32)
2025-11-21 14:14:50.360Z debug(forest): swap_mutable_and_immutable(Account.ledger)
2025-11-21 14:14:50.360Z debug(forest): swap_mutable_and_immutable(Account.code)
2025-11-21 14:14:50.360Z debug(forest): swap_mutable_and_immutable(Account)
2025-11-21 14:14:50.360Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.360Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.361Z debug(forest): swap_mutable_and_immutable(Transfer.id)
2025-11-21 14:14:50.361Z debug(forest): swap_mutable_and_immutable(Account.id)
2025-11-21 14:14:50.361Z debug(forest): swap_mutable_and_immutable(Account.user_data_128)
2025-11-21 14:14:50.361Z debug(forest): swap_mutable_and_immutable(Account.user_data_64)
2025-11-21 14:14:50.361Z debug(forest): swap_mutable_and_immutable(Account.user_data_32)
2025-11-21 14:14:50.361Z debug(forest): swap_mutable_and_immutable(Account.ledger)
2025-11-21 14:14:50.361Z debug(forest): swap_mutable_and_immutable(Account.code)
2025-11-21 14:14:50.361Z debug(forest): swap_mutable_and_immutable(Account)
2025-11-21 14:14:50.362Z debug(forest): swap_mutable_and_immutable(Transfer.id)
2025-11-21 14:14:50.380Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.380Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.382Z debug(forest): swap_mutable_and_immutable(Transfer.debit_account_id)
2025-11-21 14:14:50.384Z debug(forest): swap_mutable_and_immutable(Transfer.debit_account_id)
2025-11-21 14:14:50.401Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.401Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.411Z debug(vsr): 2: pulse_timeout fired
2025-11-21 14:14:50.411Z debug(vsr): 2: pulse_timeout reset
2025-11-21 14:14:50.421Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.421Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.421Z debug(vsr): 2: journal_repair_timeout fired
2025-11-21 14:14:50.421Z debug(vsr): 2: journal_repair_timeout reset
2025-11-21 14:14:50.421Z debug(forest): swap_mutable_and_immutable(Transfer.credit_account_id)
2025-11-21 14:14:50.424Z debug(forest): swap_mutable_and_immutable(Transfer.credit_account_id)
2025-11-21 14:14:50.441Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.441Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.459Z debug(forest): swap_mutable_and_immutable(Transfer.amount)
2025-11-21 14:14:50.461Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.461Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.463Z debug(forest): swap_mutable_and_immutable(Transfer.amount)
2025-11-21 14:14:50.481Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.481Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.499Z debug(forest): swap_mutable_and_immutable(Transfer.pending_id)
2025-11-21 14:14:50.499Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_128)
2025-11-21 14:14:50.499Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_64)
2025-11-21 14:14:50.499Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_32)
2025-11-21 14:14:50.499Z debug(forest): swap_mutable_and_immutable(Transfer.ledger)
2025-11-21 14:14:50.501Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.501Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.503Z debug(forest): swap_mutable_and_immutable(Transfer.pending_id)
2025-11-21 14:14:50.503Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_128)
2025-11-21 14:14:50.503Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_64)
2025-11-21 14:14:50.503Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_32)
2025-11-21 14:14:50.503Z debug(forest): swap_mutable_and_immutable(Transfer.ledger)
2025-11-21 14:14:50.511Z debug(vsr): 2: pulse_timeout fired
2025-11-21 14:14:50.511Z debug(vsr): 2: pulse_timeout reset
2025-11-21 14:14:50.518Z debug(forest): swap_mutable_and_immutable(Transfer.code)
2025-11-21 14:14:50.521Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.521Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.521Z debug(vsr): 2: journal_repair_timeout fired
2025-11-21 14:14:50.521Z debug(vsr): 2: journal_repair_timeout reset
2025-11-21 14:14:50.523Z debug(forest): swap_mutable_and_immutable(Transfer.code)
2025-11-21 14:14:50.541Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.541Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.549Z debug(forest): swap_mutable_and_immutable(Transfer)
2025-11-21 14:14:50.554Z debug(forest): swap_mutable_and_immutable(Transfer)
2025-11-21 14:14:50.561Z debug(vsr): 2: prepare_timeout fired
2025-11-21 14:14:50.561Z debug(vsr): 2: prepare_timeout backing off
2025-11-21 14:14:50.561Z debug(vsr): 2: prepare_timeout after=25..3 (rtt=1 min=1 max=1000 attempts=1)
2025-11-21 14:14:50.561Z debug(replica): 2N: on_prepare_timeout: waiting for replica 0
2025-11-21 14:14:50.561Z debug(replica): 2N: on_prepare_timeout: waiting for replica 1
2025-11-21 14:14:50.561Z debug(replica): 2N: on_prepare_timeout: replicating to replica 1
2025-11-21 14:14:50.561Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 101446382550583357760015182284154118271, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:50.561Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.561Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.578Z debug(forest): swap_mutable_and_immutable(Transfer.expires_at)
2025-11-21 14:14:50.578Z debug(forest): swap_mutable_and_immutable(TransferPending)
2025-11-21 14:14:50.578Z debug(forest): swap_mutable_and_immutable(TransferPending.status)
2025-11-21 14:14:50.578Z debug(forest): swap_mutable_and_immutable(AccountEvent)
2025-11-21 14:14:50.581Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.581Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.583Z debug(forest): swap_mutable_and_immutable(Transfer.expires_at)
2025-11-21 14:14:50.583Z debug(forest): swap_mutable_and_immutable(TransferPending)
2025-11-21 14:14:50.583Z debug(forest): swap_mutable_and_immutable(TransferPending.status)
2025-11-21 14:14:50.583Z debug(forest): swap_mutable_and_immutable(AccountEvent)
2025-11-21 14:14:50.591Z debug(vsr): 2: prepare_timeout fired
2025-11-21 14:14:50.591Z debug(vsr): 2: prepare_timeout backing off
2025-11-21 14:14:50.591Z debug(vsr): 2: prepare_timeout after=3..7 (rtt=1 min=1 max=1000 attempts=2)
2025-11-21 14:14:50.591Z debug(replica): 2N: on_prepare_timeout: waiting for replica 0
2025-11-21 14:14:50.591Z debug(replica): 2N: on_prepare_timeout: waiting for replica 1
2025-11-21 14:14:50.591Z debug(replica): 2N: on_prepare_timeout: replicating to replica 0
2025-11-21 14:14:50.591Z debug(replica): 2N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 101446382550583357760015182284154118271, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:50.602Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.602Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:50.610Z debug(forest): swap_mutable_and_immutable(Account.imported)
2025-11-21 14:14:50.610Z debug(forest): swap_mutable_and_immutable(Transfer.imported)
2025-11-21 14:14:50.610Z debug(forest): swap_mutable_and_immutable(Account.closed)
2025-11-21 14:14:50.610Z debug(forest): swap_mutable_and_immutable(Transfer.closing)
2025-11-21 14:14:50.610Z debug(forest): swap_mutable_and_immutable(AccountEvent.account_timestamp)
2025-11-21 14:14:50.612Z debug(vsr): 2: pulse_timeout fired
2025-11-21 14:14:50.612Z debug(vsr): 2: pulse_timeout reset
2025-11-21 14:14:50.618Z debug(forest): swap_mutable_and_immutable(Account.imported)
2025-11-21 14:14:50.618Z debug(forest): swap_mutable_and_immutable(Transfer.imported)
2025-11-21 14:14:50.620Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_status)
2025-11-21 14:14:50.622Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:50.618Z debug(forest): swap_mutable_and_immutable(Account.closed)
2025-11-21 14:14:52.555Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:52.555Z debug(vsr): 2: journal_repair_timeout fired
2025-11-21 14:14:52.555Z debug(vsr): 2: journal_repair_timeout reset
2025-11-21 14:14:52.555Z debug(forest): swap_mutable_and_immutable(Transfer.closing)
2025-11-21 14:14:52.555Z debug(forest): swap_mutable_and_immutable(AccountEvent.account_timestamp)
2025-11-21 14:14:52.555Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 162031021886803528938247614646857030259, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 51320505015944716928197161233515872830, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 126, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2690925502, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.555Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:52.555Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:14:52.566Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_status)
2025-11-21 14:14:52.575Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:52.575Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:52.575Z debug(forest): swap_mutable_and_immutable(AccountEvent.dr_account_id_expired)
2025-11-21 14:14:52.575Z debug(forest): swap_mutable_and_immutable(AccountEvent.cr_account_id_expired)
2025-11-21 14:14:52.575Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_id_expired)
2025-11-21 14:14:52.575Z debug(forest): swap_mutable_and_immutable(AccountEvent.ledger_expired)
2025-11-21 14:14:52.575Z debug(forest): swap_mutable_and_immutable(AccountEvent.prunable)
2025-11-21 14:14:52.586Z debug(forest): swap_mutable_and_immutable(AccountEvent.dr_account_id_expired)
2025-11-21 14:14:52.586Z debug(forest): swap_mutable_and_immutable(AccountEvent.cr_account_id_expired)
2025-11-21 14:14:52.586Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_id_expired)
2025-11-21 14:14:52.586Z debug(forest): swap_mutable_and_immutable(AccountEvent.ledger_expired)
2025-11-21 14:14:52.586Z debug(forest): swap_mutable_and_immutable(AccountEvent.prunable)
2025-11-21 14:14:52.588Z warning(replica): 0n: commit_dispatch: slow request, request=125 size=950784 create_transfers time=2272ms
2025-11-21 14:14:52.588Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 101446382550583357760015182284154118271, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:52.588Z debug(replica): 0n: on_prepare: ignoring (repair)
2025-11-21 14:14:52.588Z debug(replica): 0n: repair_header: op=128 checksum=101446382550583357760015182284154118271 (checksum dirty)
2025-11-21 14:14:52.588Z debug(journal): 0: set_header_as_dirty: op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:52.588Z debug(replica): 0n: write_prepare: ignoring op=128 checksum=101446382550583357760015182284154118271 (already writing exact)
2025-11-21 14:14:52.588Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=134217728 len=4096 unlocked
2025-11-21 14:14:52.588Z debug(journal): 0: write_header: op=128 sectors[32768..36864]
2025-11-21 14:14:52.588Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2025-11-21 14:14:52.588Z debug(client_replies): 0: write_reply: wrote (client=243817616567816617951908231182086174208 request=125)
2025-11-21 14:14:52.588Z warning(clock): 0: synchronization failed, partitioned (sources=1 samples=1)
2025-11-21 14:14:52.588Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:52.588Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:14:52.588Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2025-11-21 14:14:52.588Z debug(journal): 0: write: view=2 slot=128 op=128 len=2320: 101446382550583357760015182284154118271 complete, marking clean
2025-11-21 14:14:52.588Z debug(replica): 0n: send_prepare_ok: op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:52.588Z debug(replica): 0n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 338514293372520857727144588515437116178, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .prepare_checksum = 101446382550583357760015182284154118271, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit_min = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:52.589Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 338514293372520857727144588515437116178, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .prepare_checksum = 101446382550583357760015182284154118271, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit_min = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:52.589Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-21 14:14:52.589Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2025-11-21 14:14:52.589Z debug(replica): 2N: on_prepare_ok: quorum received, context=101446382550583357760015182284154118271
2025-11-21 14:14:52.589Z debug(vsr): 2: prepare_timeout stopped
2025-11-21 14:14:52.589Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-11-21 14:14:52.589Z debug(replica): 2N: execute_op: executing view=2 primary=true op=128 checksum=101446382550583357760015182284154118271 (lookup_accounts)
2025-11-21 14:14:52.589Z debug(replica): 2N: execute_op: commit_timestamp=1763734487331721087 prepare.header.timestamp=1763734490315722907
2025-11-21 14:14:52.589Z debug(replica): 2N: execute_op: advancing commit_max=127..128
2025-11-21 14:14:52.589Z debug(replica): 2N: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=126
2025-11-21 14:14:52.589Z debug(replica): 2N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 11752891098444440516716446257971096730, .checksum_padding = 0, .checksum_body = 39397370366339973182548243742255506941, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .context = 137474194209229661362275614424877054564, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 128, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.589Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 11752891098444440516716446257971096730, .checksum_padding = 0, .checksum_body = 39397370366339973182548243742255506941, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .context = 137474194209229661362275614424877054564, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 128, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.589Z debug(forest): entering forest.compact() op=128 constants.lsm_compaction_ops=32 first_beat=true last_half_beat=false half_beat=false last_beat=false
2025-11-21 14:14:52.589Z debug(compaction): Account.id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.user_data_128:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.user_data_64:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.user_data_32:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.ledger:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.code:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.debit_account_id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.credit_account_id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.amount:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.pending_id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.user_data_128:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.user_data_64:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.user_data_32:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.ledger:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.code:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.expires_at:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): TransferPending:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): TransferPending.status:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.imported:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.imported:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.closed:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.closing:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.account_timestamp:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.transfer_pending_status:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.dr_account_id_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.cr_account_id_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.transfer_pending_id_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.ledger_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.prunable:1: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.user_data_128:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.user_data_64:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.user_data_32:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.ledger:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.code:3: bar_commence: nothing to compact
warning(client): 243817616567816617951908231182086174208: on_reply: slow request, request=126 op=128 size=2320 lookup_accounts time=2550ms
2025-11-21 14:14:52.589Z debug(compaction): Account:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.debit_account_id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.credit_account_id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.amount:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.pending_id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.user_data_128:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.user_data_64:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.user_data_32:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.ledger:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.code:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.expires_at:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): TransferPending:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): TransferPending.status:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.imported:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.imported:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.closed:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.closing:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.account_timestamp:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.transfer_pending_status:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.dr_account_id_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.cr_account_id_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.transfer_pending_id_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.ledger_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.prunable:3: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.user_data_128:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.user_data_64:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.user_data_32:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.ledger:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.code:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.debit_account_id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.credit_account_id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.amount:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.pending_id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.user_data_128:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z info(workload): accounts created = 128, transfers = 154725, pending transfers = 0, commands run = 63
2025-11-21 14:14:52.589Z debug(compaction): Transfer.user_data_64:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.user_data_32:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.ledger:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.code:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.expires_at:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): TransferPending:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): TransferPending.status:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.imported:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.imported:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Account.closed:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): Transfer.closing:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.account_timestamp:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.transfer_pending_status:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.dr_account_id_expired:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.cr_account_id_expired:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.transfer_pending_id_expired:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.ledger_expired:5: bar_commence: nothing to compact
2025-11-21 14:14:52.589Z debug(compaction): AccountEvent.prunable:5: bar_commence: nothing to compact
2025-11-21 14:14:52.590Z debug(client_replies): 2: write_reply: wrote (client=243817616567816617951908231182086174208 request=126)
2025-11-21 14:14:52.595Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:52.595Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:14:52.598Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.598Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:52.598Z debug(replica): 2N: primary_pipeline_prepare: request checksum=301936341350523303595950859833152205546 client=243817616567816617951908231182086174208
2025-11-21 14:14:52.599Z warning(replica): 1n: commit_dispatch: slow request, request=125 size=950784 create_transfers time=2283ms
2025-11-21 14:14:52.599Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 162031021886803528938247614646857030259, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 51320505015944716928197161233515872830, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 126, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2690925502, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.599Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:14:52.599Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 162031021886803528938247614646857030259, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 51320505015944716928197161233515872830, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 126, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2690925502, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.600Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 101446382550583357760015182284154118271, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:52.600Z debug(replica): 1n: on_prepare: ignoring (repair)
2025-11-21 14:14:52.600Z debug(replica): 1n: repair_header: op=128 checksum=101446382550583357760015182284154118271 (checksum dirty)
2025-11-21 14:14:52.600Z debug(journal): 1: set_header_as_dirty: op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:52.600Z debug(replica): 1n: write_prepare: ignoring op=128 checksum=101446382550583357760015182284154118271 (already writing exact)
2025-11-21 14:14:52.600Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=134217728 len=4096 unlocked
2025-11-21 14:14:52.600Z debug(journal): 1: write_header: op=128 sectors[32768..36864]
2025-11-21 14:14:52.600Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2025-11-21 14:14:52.600Z debug(client_replies): 1: write_reply: wrote (client=243817616567816617951908231182086174208 request=125)
2025-11-21 14:14:52.600Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=124893907086854881142772784205180813707 op=129
2025-11-21 14:14:52.600Z debug(vsr): 2: prepare_timeout started
2025-11-21 14:14:52.600Z debug(vsr): 2: primary_abdicate_timeout started
2025-11-21 14:14:52.600Z debug(vsr): 2: pulse_timeout reset
2025-11-21 14:14:52.600Z debug(replica): 2N: replicate: replicating op=129 to replica 0
2025-11-21 14:14:52.600Z debug(replica): 2N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 124893907086854881142772784205180813707, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:52.600Z debug(replica): 2N: replicate: replicating op=129 to replica 1
2025-11-21 14:14:52.600Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 124893907086854881142772784205180813707, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:52.601Z debug(replica): 2N: on_prepare: advancing: op=128..129 checksum=101446382550583357760015182284154118271..124893907086854881142772784205180813707
2025-11-21 14:14:52.601Z debug(journal): 2: set_header_as_dirty: op=129 checksum=124893907086854881142772784205180813707
2025-11-21 14:14:52.601Z debug(replica): 2N: append: appending to journal op=129
2025-11-21 14:14:52.601Z debug(journal): 2: write: view=2 slot=129 op=129 len=543488: 124893907086854881142772784205180813707 starting
2025-11-21 14:14:52.601Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=135266304 len=544768 locked
2025-11-21 14:14:52.601Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 162031021886803528938247614646857030259, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 51320505015944716928197161233515872830, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 126, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2690925502, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.601Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:14:52.601Z debug(client_replies): 2: read_reply: start (client=243817616567816617951908231182086174208 reply=11752891098444440516716446257971096730)
2025-11-21 14:14:52.601Z debug(client_replies): 2: read_reply: done (client=243817616567816617951908231182086174208 reply=11752891098444440516716446257971096730)
2025-11-21 14:14:52.601Z debug(replica): 2N: on_request: repeat reply (client=243817616567816617951908231182086174208 request=126)
2025-11-21 14:14:52.601Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 11752891098444440516716446257971096730, .checksum_padding = 0, .checksum_body = 39397370366339973182548243742255506941, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .context = 137474194209229661362275614424877054564, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 128, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.602Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.602Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:14:52.602Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.602Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2025-11-21 14:14:52.603Z debug(journal): 1: write: view=2 slot=128 op=128 len=2320: 101446382550583357760015182284154118271 complete, marking clean
2025-11-21 14:14:52.603Z debug(replica): 1n: send_prepare_ok: op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:52.603Z debug(replica): 1n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 164425059802933392491091929089906552533, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .prepare_checksum = 101446382550583357760015182284154118271, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit_min = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:52.603Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 124893907086854881142772784205180813707, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:52.604Z debug(replica): 0n: on_prepare: advancing commit_max=127..128
2025-11-21 14:14:52.604Z debug(replica): 0n: on_prepare: caching prepare.op=129 (commit_min=127 op=128 commit_max=128 prepare_max=1007)
2025-11-21 14:14:52.604Z debug(replica): 0n: on_prepare: advancing: op=128..129 checksum=101446382550583357760015182284154118271..124893907086854881142772784205180813707
2025-11-21 14:14:52.604Z debug(journal): 0: set_header_as_dirty: op=129 checksum=124893907086854881142772784205180813707
2025-11-21 14:14:52.604Z debug(replica): 0n: append: appending to journal op=129
2025-11-21 14:14:52.604Z debug(journal): 0: write: view=2 slot=129 op=129 len=543488: 124893907086854881142772784205180813707 starting
2025-11-21 14:14:52.604Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=135266304 len=544768 locked
2025-11-21 14:14:52.604Z debug(replica): 0n: commit_start_journal: cached prepare op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:52.604Z debug(replica): 0n: repair_prepare: op=129 checksum=124893907086854881142772784205180813707 (already writing)
2025-11-21 14:14:52.604Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=127)
2025-11-21 14:14:52.604Z debug(replica): 0n: execute_op: executing view=2 primary=false op=128 checksum=101446382550583357760015182284154118271 (lookup_accounts)
2025-11-21 14:14:52.604Z debug(replica): 0n: execute_op: commit_timestamp=1763734487331721087 prepare.header.timestamp=1763734490315722907
2025-11-21 14:14:52.604Z debug(replica): 0n: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=126
2025-11-21 14:14:52.604Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 11752891098444440516716446257971096730, .checksum_padding = 0, .checksum_body = 39397370366339973182548243742255506941, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .context = 137474194209229661362275614424877054564, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 128, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.604Z debug(replica): 0n: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 11752891098444440516716446257971096730, .checksum_padding = 0, .checksum_body = 39397370366339973182548243742255506941, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 162031021886803528938247614646857030259, .request_checksum_padding = 0, .context = 137474194209229661362275614424877054564, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit = 128, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.604Z debug(forest): entering forest.compact() op=128 constants.lsm_compaction_ops=32 first_beat=true last_half_beat=false half_beat=false last_beat=false
2025-11-21 14:14:52.604Z debug(compaction): Account.id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.user_data_128:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.user_data_64:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.user_data_32:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.ledger:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.code:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.debit_account_id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.credit_account_id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.amount:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.pending_id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.user_data_128:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.user_data_64:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.user_data_32:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.ledger:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.code:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.expires_at:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): TransferPending:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): TransferPending.status:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.imported:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.imported:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.closed:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.closing:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.account_timestamp:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.transfer_pending_status:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.dr_account_id_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.cr_account_id_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.transfer_pending_id_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.ledger_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.prunable:1: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.user_data_128:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.user_data_64:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.user_data_32:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.ledger:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.code:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.debit_account_id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.credit_account_id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.amount:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.pending_id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.user_data_128:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.user_data_64:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.user_data_32:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.ledger:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.code:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.expires_at:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): TransferPending:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): TransferPending.status:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.imported:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.imported:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.closed:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.closing:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.account_timestamp:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.transfer_pending_status:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.dr_account_id_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.cr_account_id_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.transfer_pending_id_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.ledger_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent.prunable:3: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.user_data_128:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.user_data_64:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.user_data_32:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.ledger:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.code:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.debit_account_id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.credit_account_id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.amount:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.pending_id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.user_data_128:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.user_data_64:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.user_data_32:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.ledger:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.code:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.expires_at:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): TransferPending:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): TransferPending.status:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): AccountEvent:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Account.imported:5: bar_commence: nothing to compact
2025-11-21 14:14:52.604Z debug(compaction): Transfer.imported:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(compaction): Account.closed:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(compaction): Transfer.closing:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(compaction): AccountEvent.account_timestamp:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(compaction): AccountEvent.transfer_pending_status:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(compaction): AccountEvent.dr_account_id_expired:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(compaction): AccountEvent.cr_account_id_expired:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(compaction): AccountEvent.transfer_pending_id_expired:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(compaction): AccountEvent.ledger_expired:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(compaction): AccountEvent.prunable:5: bar_commence: nothing to compact
2025-11-21 14:14:52.605Z debug(client_replies): 0: write_reply: wrote (client=243817616567816617951908231182086174208 request=126)
2025-11-21 14:14:52.605Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:14:52.605Z debug(replica): 2N: on_request: new request
2025-11-21 14:14:52.605Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:14:52.605Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 124893907086854881142772784205180813707, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:14:52.605Z debug(replica): 1n: on_prepare: advancing commit_max=127..128
2025-11-21 14:14:52.605Z debug(replica): 1n: on_prepare: caching prepare.op=129 (commit_min=127 op=128 commit_max=128 prepare_max=1007)
2025-11-21 14:14:52.605Z debug(replica): 1n: on_prepare: advancing: op=128..129 checksum=101446382550583357760015182284154118271..124893907086854881142772784205180813707
2025-11-21 14:14:52.605Z debug(journal): 1: set_header_as_dirty: op=129 checksum=124893907086854881142772784205180813707
2025-11-21 14:14:52.605Z debug(replica): 1n: append: appending to journal op=129
2025-11-21 14:14:52.605Z debug(journal): 1: write: view=2 slot=129 op=129 len=543488: 124893907086854881142772784205180813707 starting
2025-11-21 14:14:52.605Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=135266304 len=544768 locked
2025-11-21 14:14:52.605Z debug(replica): 1n: commit_start_journal: cached prepare op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:14:52.606Z debug(replica): 1n: repair_prepare: op=129 checksum=124893907086854881142772784205180813707 (already writing)
2025-11-21 14:14:52.606Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=127)
2025-11-21 14:14:52.606Z debug(replica): 1n: execute_op: executing view=2 primary=false op=128 checksum=101446382550583357760015182284154118271 (lookup_accounts)
2025-11-21 14:14:52.606Z debug(replica): 1n: execute_op: commit_timestamp=1763734487331721087 prepare.header.timestamp=1763734490315722907
2025-11-21 14:14:52.606Z debug(replica): 1n: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=126
2025-11-21 14:14:52.606Z debug(forest): entering forest.compact() op=128 constants.lsm_compaction_ops=32 first_beat=true last_half_beat=false half_beat=false last_beat=false
2025-11-21 14:14:52.606Z debug(compaction): Account.id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.user_data_128:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.user_data_64:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.user_data_32:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.ledger:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.code:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.debit_account_id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.credit_account_id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.amount:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.pending_id:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.user_data_128:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.user_data_64:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.user_data_32:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.ledger:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.code:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.expires_at:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): TransferPending:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): TransferPending.status:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.imported:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.imported:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.closed:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.closing:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.account_timestamp:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.transfer_pending_status:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.dr_account_id_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.cr_account_id_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.transfer_pending_id_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.ledger_expired:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.prunable:1: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.user_data_128:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.user_data_64:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.user_data_32:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.ledger:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.code:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.debit_account_id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.credit_account_id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.amount:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.pending_id:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.user_data_128:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.user_data_64:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.user_data_32:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.ledger:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.code:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.expires_at:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): TransferPending:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): TransferPending.status:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.imported:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.imported:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.closed:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.closing:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.account_timestamp:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.transfer_pending_status:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.dr_account_id_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.cr_account_id_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.transfer_pending_id_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.ledger_expired:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): AccountEvent.prunable:3: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.user_data_128:5: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.user_data_64:5: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.user_data_32:5: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.ledger:5: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account.code:5: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Account:5: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.debit_account_id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.606Z debug(compaction): Transfer.credit_account_id:5: bar_commence: nothing to compact
2025-11-21 14:14:52.608Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-21 14:14:52.615Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:14:52.608Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-21 14:18:46.163Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:18:46.163Z debug(vsr): 0: journal_repair_timeout fired
2025-11-21 14:14:52.606Z debug(compaction): Transfer.amount:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(vsr): 0: journal_repair_timeout reset
2025-11-21 14:18:46.163Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=135266304 len=544768 unlocked
2025-11-21 14:18:46.163Z debug(journal): 2: write_header: op=129 sectors[32768..36864]
2025-11-21 14:18:46.163Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2025-11-21 14:18:46.163Z debug(compaction): Transfer.pending_id:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): Transfer.user_data_128:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): Transfer.user_data_64:5: bar_commence: nothing to compact
2025-11-21 14:15:11.085Z info(supervisor): sleeping for 1.913s
2025-11-21 14:18:46.163Z debug(compaction): Transfer.user_data_32:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): Transfer.ledger:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): Transfer.code:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 164425059802933392491091929089906552533, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 152741066998109468735118760318535158690, .parent_padding = 0, .prepare_checksum = 101446382550583357760015182284154118271, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 128, .commit_min = 127, .timestamp = 1763734490315722907, .request = 126, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-21 14:18:46.163Z debug(compaction): Transfer:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(replica): 0n: repair_prepare: op=129 checksum=124893907086854881142772784205180813707 (already writing)
warning(message_bus): 243817616567816617951908231182086174208: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionTimedOut
2025-11-21 14:18:46.163Z debug(compaction): Transfer.expires_at:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(replica): 2N: on_prepare_ok: not preparing op=128 checksum=101446382550583357760015182284154118271
2025-11-21 14:18:46.163Z debug(compaction): TransferPending:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): TransferPending.status:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): AccountEvent:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): Account.imported:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): Transfer.imported:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=135266304 len=544768 unlocked
2025-11-21 14:18:46.163Z debug(compaction): Account.closed:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(journal): 0: write_header: op=129 sectors[32768..36864]
2025-11-21 14:18:46.163Z debug(compaction): Transfer.closing:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2025-11-21 14:18:46.163Z debug(compaction): AccountEvent.account_timestamp:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): AccountEvent.transfer_pending_status:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): AccountEvent.dr_account_id_expired:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): AccountEvent.cr_account_id_expired:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): AccountEvent.transfer_pending_id_expired:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): AccountEvent.ledger_expired:5: bar_commence: nothing to compact
2025-11-21 14:18:46.163Z debug(compaction): AccountEvent.prunable:5: bar_commence: nothing to compact
2025-11-21 14:18:46.164Z warning(replica): 1n: commit_dispatch: slow request, request=126 size=2320 lookup_accounts time=233558ms
2025-11-21 14:18:46.164Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2025-11-21 14:18:46.164Z warning(clock): 1: synchronization failed, partitioned (sources=1 samples=1)
2025-11-21 14:18:46.164Z error(clock): 1: no agreement on cluster time (partitioned or too many clock faults)
2025-11-21 14:18:46.164Z debug(journal): 2: write: view=2 slot=129 op=129 len=543488: 124893907086854881142772784205180813707 complete, marking clean
2025-11-21 14:18:46.164Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:18:46.164Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:18:46.164Z debug(replica): 2N: send_prepare_ok: op=129 checksum=124893907086854881142772784205180813707
2025-11-21 14:18:46.164Z debug(replica): 2N: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 297642728746136917182523862835633746854, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .prepare_checksum = 124893907086854881142772784205180813707, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit_min = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:18:46.164Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 297642728746136917182523862835633746854, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .prepare_checksum = 124893907086854881142772784205180813707, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit_min = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:18:46.164Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-21 14:18:46.164Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2025-11-21 14:18:46.164Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2025-11-21 14:18:46.164Z debug(client_replies): 1: write_reply: wrote (client=243817616567816617951908231182086174208 request=126)
2025-11-21 14:18:46.166Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.166Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.166Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.167Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.167Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.167Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.167Z debug(replica): 2N: on_request: new request
2025-11-21 14:18:46.167Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:18:46.167Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.167Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=135266304 len=544768 unlocked
2025-11-21 14:18:46.167Z debug(journal): 1: write_header: op=129 sectors[32768..36864]
2025-11-21 14:18:46.167Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2025-11-21 14:18:46.168Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2025-11-21 14:18:46.168Z debug(journal): 1: write: view=2 slot=129 op=129 len=543488: 124893907086854881142772784205180813707 complete, marking clean
2025-11-21 14:18:46.168Z debug(replica): 1n: send_prepare_ok: op=129 checksum=124893907086854881142772784205180813707
2025-11-21 14:18:46.168Z debug(replica): 1n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 113230906461435453720212474716993797507, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .prepare_checksum = 124893907086854881142772784205180813707, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit_min = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:18:46.169Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.169Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.169Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.169Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2025-11-21 14:18:46.169Z debug(journal): 0: write: view=2 slot=129 op=129 len=543488: 124893907086854881142772784205180813707 complete, marking clean
2025-11-21 14:18:46.169Z debug(replica): 0n: send_prepare_ok: op=129 checksum=124893907086854881142772784205180813707
2025-11-21 14:18:46.169Z debug(replica): 0n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 299395025425334892804292433923811691136, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .prepare_checksum = 124893907086854881142772784205180813707, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit_min = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:18:46.170Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.170Z debug(replica): 2N: on_request: new request
2025-11-21 14:18:46.170Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:18:46.170Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.170Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.170Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.170Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.PingClient{ .checksum = 73461412729141687529489708949848402088, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 243817616567816617951908231182086174208, .ping_timestamp_monotonic = 35849482974970523, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.170Z debug(replica): 1n: sending pong_client to client 243817616567816617951908231182086174208: vsr.message_header.Header.PongClient{ .checksum = 135113357936052644366470349106404789915, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 35849482974970523, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.171Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.171Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.171Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.171Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.PingClient{ .checksum = 73461412729141687529489708949848402088, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 243817616567816617951908231182086174208, .ping_timestamp_monotonic = 35849482974970523, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.171Z debug(replica): 0n: sending pong_client to client 243817616567816617951908231182086174208: vsr.message_header.Header.PongClient{ .checksum = 48562447639805424596892448978365017769, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 35849482974970523, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.172Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.172Z debug(replica): 2N: on_request: new request
2025-11-21 14:18:46.172Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:18:46.173Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.173Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.173Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.173Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.173Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.173Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.173Z info(supervisor): 0: terminating replica
2025-11-21 14:18:46.175Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.175Z debug(replica): 2N: on_request: new request
2025-11-21 14:18:46.175Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-21 14:18:46.175Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 113230906461435453720212474716993797507, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .prepare_checksum = 124893907086854881142772784205180813707, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit_min = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:18:46.175Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-21 14:18:46.175Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2025-11-21 14:18:46.175Z debug(replica): 2N: on_prepare_ok: quorum received, context=124893907086854881142772784205180813707
2025-11-21 14:18:46.175Z debug(vsr): 2: prepare_timeout stopped
2025-11-21 14:18:46.175Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-11-21 14:18:46.176Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.176Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.176Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.176Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.PingClient{ .checksum = 320790255943680792317573969613715250224, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 243817616567816617951908231182086174208, .ping_timestamp_monotonic = 35849513095553663, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.176Z debug(replica): 1n: sending pong_client to client 243817616567816617951908231182086174208: vsr.message_header.Header.PongClient{ .checksum = 238924358881134284094020202204547138123, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 35849513095553663, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.177Z debug(replica): 2N: execute_op: executing view=2 primary=true op=129 checksum=124893907086854881142772784205180813707 (create_transfers)
2025-11-21 14:18:46.177Z debug(replica): 2N: execute_op: commit_timestamp=1763734490315722907 prepare.header.timestamp=1763734492598262470
2025-11-21 14:18:46.178Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.178Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.178Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.180Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.180Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.180Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.181Z warning(faulty_network): send error (0,9): error.ConnectionResetByPeer
2025-11-21 14:18:46.182Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.182Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.182Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.182Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.PingClient{ .checksum = 186693295404799925508723664671109707755, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 243817616567816617951908231182086174208, .ping_timestamp_monotonic = 35849543184028654, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.183Z debug(replica): 1n: sending pong_client to client 243817616567816617951908231182086174208: vsr.message_header.Header.PongClient{ .checksum = 189647788170268970972241020013171546732, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 35849543184028654, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.183Z info(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 0 } orderly shutdown
2025-11-21 14:18:46.185Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.185Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-21 14:18:46.185Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.186Z debug(replica): 2N: execute_op: advancing commit_max=128..129
2025-11-21 14:18:46.186Z debug(replica): 2N: client_table_entry_update: client=243817616567816617951908231182086174208 session=2 request=127
2025-11-21 14:18:46.186Z debug(replica): 2N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 82216191341518774535572639787288652946, .checksum_padding = 0, .checksum_body = 197111286194544526219153631000130935659, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 312, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .context = 324831183829313342830928092791806788348, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 129, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.186Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 82216191341518774535572639787288652946, .checksum_padding = 0, .checksum_body = 197111286194544526219153631000130935659, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 312, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .context = 324831183829313342830928092791806788348, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 129, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.186Z debug(forest): entering forest.compact() op=129 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-21 14:18:46.186Z warning(faulty_network): send error (2,2): error.ConnectionResetByPeer
2025-11-21 14:18:46.191Z info(supervisor): sleeping for 9.717s
2025-11-21 14:18:46.193Z debug(client_replies): 2: write_reply: wrote (client=243817616567816617951908231182086174208 request=127)
2025-11-21 14:18:46.195Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:18:46.195Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:18:46.205Z debug(vsr): 1: journal_repair_timeout fired
2025-11-21 14:18:46.205Z debug(vsr): 1: journal_repair_timeout reset
2025-11-21 14:18:46.215Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:18:46.215Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:18:46.235Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:18:46.235Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:18:46.255Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:18:46.195Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:46.255Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:18:46.264Z warning(faulty_network): connect failed (0,0): error.ConnectionRefused
2025-11-21 14:18:52.792Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.792Z debug(client_replies): 2: read_reply: start (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.792Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 299395025425334892804292433923811691136, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 101446382550583357760015182284154118271, .parent_padding = 0, .prepare_checksum = 124893907086854881142772784205180813707, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit_min = 128, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-21 14:18:52.792Z debug(replica): 2N: on_prepare_ok: not preparing op=129 checksum=124893907086854881142772784205180813707
warning(message_bus): 243817616567816617951908231182086174208: on_recv: from=vsr.Peer{ .replica = 0 } error.ConnectionResetByPeer
2025-11-21 14:18:52.792Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.PingClient{ .checksum = 248511931853153281550500797003552447060, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 243817616567816617951908231182086174208, .ping_timestamp_monotonic = 35849710424016237, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.792Z debug(replica): 1n: sending pong_client to client 243817616567816617951908231182086174208: vsr.message_header.Header.PongClient{ .checksum = 177436849366658427819211639209590065479, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 35849710424016237, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.794Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.794Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.794Z debug(client_replies): 2: read_reply: busy (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.794Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2025-11-21 14:18:52.796Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.796Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.796Z debug(client_replies): 2: read_reply: busy (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.796Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2025-11-21 14:18:52.797Z debug(client_replies): 2: read_reply: done (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.797Z debug(replica): 2N: on_request: repeat reply (client=243817616567816617951908231182086174208 request=127)
2025-11-21 14:18:52.797Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 82216191341518774535572639787288652946, .checksum_padding = 0, .checksum_body = 197111286194544526219153631000130935659, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 312, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .context = 324831183829313342830928092791806788348, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 129, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.799Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.799Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.799Z debug(client_replies): 2: read_reply: start (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.801Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.801Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.801Z debug(client_replies): 2: read_reply: busy (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.801Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2025-11-21 14:18:52.803Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.803Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.803Z debug(client_replies): 2: read_reply: busy (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.803Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2025-11-21 14:18:52.803Z warning(message_bus): 2: on_recv: from=vsr.Peer{ .client = 243817616567816617951908231182086174208 } error.ConnectionResetByPeer
2025-11-21 14:18:52.804Z debug(client_replies): 2: read_reply: done (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.804Z debug(replica): 2N: on_request: repeat reply (client=243817616567816617951908231182086174208 request=127)
2025-11-21 14:18:52.804Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 82216191341518774535572639787288652946, .checksum_padding = 0, .checksum_body = 197111286194544526219153631000130935659, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 312, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .context = 324831183829313342830928092791806788348, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 129, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.804Z debug(message_bus): 2: send_message_to_client: no connection to=243817616567816617951908231182086174208
2025-11-21 14:18:52.806Z info(message_bus): 2: set_and_verify_peer connection from client_likely=243817616567816617951908231182086174208
2025-11-21 14:18:52.806Z info(message_bus): 2: set_and_verify_peer connection from client=243817616567816617951908231182086174208
2025-11-21 14:18:52.806Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.806Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.806Z debug(client_replies): 2: read_reply: start (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.806Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PingClient{ .checksum = 248511931853153281550500797003552447060, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping_client, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .client = 243817616567816617951908231182086174208, .ping_timestamp_monotonic = 35849710424016237, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.806Z debug(replica): 2N: sending pong_client to client 243817616567816617951908231182086174208: vsr.message_header.Header.PongClient{ .checksum = 294086643655667733765514844110485721697, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong_client, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 35849710424016237, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.806Z info(message_bus): 2: on_recv: from=vsr.Peer{ .replica = 0 } orderly shutdown
2025-11-21 14:18:52.808Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.808Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.808Z debug(client_replies): 2: read_reply: busy (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.808Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2025-11-21 14:18:52.808Z debug(client_replies): 2: read_reply: done (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.808Z debug(replica): 2N: on_request: repeat reply (client=243817616567816617951908231182086174208 request=127)
2025-11-21 14:18:52.808Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 82216191341518774535572639787288652946, .checksum_padding = 0, .checksum_body = 197111286194544526219153631000130935659, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 312, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .context = 324831183829313342830928092791806788348, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 129, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
warning(client): 243817616567816617951908231182086174208: on_reply: slow request, request=127 op=129 size=543488 create_transfers time=240215ms
2025-11-21 14:18:52.810Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.810Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.810Z debug(client_replies): 2: read_reply: start (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.812Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:18:52.812Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:18:52.813Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.813Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.813Z debug(client_replies): 2: read_reply: busy (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.813Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2025-11-21 14:18:52.813Z debug(client_replies): 2: read_reply: done (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.813Z debug(replica): 2N: on_request: repeat reply (client=243817616567816617951908231182086174208 request=127)
2025-11-21 14:18:52.813Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 82216191341518774535572639787288652946, .checksum_padding = 0, .checksum_body = 197111286194544526219153631000130935659, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 312, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .context = 324831183829313342830928092791806788348, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 129, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.815Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.815Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.815Z debug(client_replies): 2: read_reply: start (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.817Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 301936341350523303595950859833152205546, .checksum_padding = 0, .checksum_body = 39441550135915134458773539673578893606, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 543488, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 137474194209229661362275614424877054564, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 127, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2550515059, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.817Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-21 14:18:52.817Z debug(client_replies): 2: read_reply: busy (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.817Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2025-11-21 14:18:52.817Z debug(client_replies): 2: read_reply: done (client=243817616567816617951908231182086174208 reply=82216191341518774535572639787288652946)
2025-11-21 14:18:52.817Z debug(replica): 2N: on_request: repeat reply (client=243817616567816617951908231182086174208 request=127)
2025-11-21 14:18:52.817Z debug(replica): 2N: sending reply to client 243817616567816617951908231182086174208: vsr.message_header.Header.Reply{ .checksum = 82216191341518774535572639787288652946, .checksum_padding = 0, .checksum_body = 197111286194544526219153631000130935659, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 312, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 301936341350523303595950859833152205546, .request_checksum_padding = 0, .context = 324831183829313342830928092791806788348, .context_padding = 0, .client = 243817616567816617951908231182086174208, .op = 129, .commit = 129, .timestamp = 1763734492598262470, .request = 127, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.817Z warning(clock): 2: synchronization failed, partitioned (sources=1 samples=1)
2025-11-21 14:18:52.817Z error(clock): 2: no agreement on cluster time (partitioned or too many clock faults)
2025-11-21 14:18:52.827Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:18:52.827Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:18:52.832Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-21 14:18:52.832Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-21 14:18:52.839Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 191928955381515032805957132723078127863, .checksum_padding = 0, .checksum_body = 47671698171823809637714282734873753688, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 324831183829313342830928092791806788348, .parent_padding = 0, .client = 243817616567816617951908231182086174208, .session = 2, .timestamp = 0, .request = 128, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 4294967295, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.840Z debug(replica): 2N: on_request: new request
2025-11-21 14:18:52.840Z debug(vsr): 2: primary_abdicate_timeout started
2025-11-21 14:18:52.840Z warning(replica): 2N: on_request: dropping (clock not synchronized)
2025-11-21 14:18:52.842Z debug(vsr): 1: journal_repair_timeout fired
2025-11-21 14:18:52.842Z debug(vsr): 1: journal_repair_timeout reset
2025-11-21 14:18:52.843Z error(supervisor): liveness check: too slow request
2025-11-21 14:18:52.844Z info(supervisor): 1: terminating replica
2025-11-21 14:18:52.847Z debug(vsr): 2: ping_timeout fired
2025-11-21 14:18:52.847Z debug(vsr): 2: ping_timeout reset
2025-11-21 14:18:52.847Z debug(replica): 2N: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 150000894554064447508924157546654828302, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 35849713139211858, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692840448, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.847Z debug(message_bus): 2: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Ping{ .checksum = 150000894554064447508924157546654828302, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 35849713139211858, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692840448, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.847Z debug(replica): 2N: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 150000894554064447508924157546654828302, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 35849713139211858, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692840448, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.847Z debug(vsr): 2: commit_message_timeout fired
2025-11-21 14:18:52.847Z debug(vsr): 2: commit_message_timeout reset
2025-11-21 14:18:52.847Z debug(replica): 2N: sending commit to replica 0: vsr.message_header.Header.Commit{ .checksum = 95827520960235074435848543336254118974, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 124893907086854881142772784205180813707, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 129, .timestamp_monotonic = 35849713139338887, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.847Z debug(message_bus): 2: send_message_to_replica: no connection to=0 header=vsr.message_header.Header.Commit{ .checksum = 95827520960235074435848543336254118974, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 124893907086854881142772784205180813707, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 129, .timestamp_monotonic = 35849713139338887, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.847Z debug(replica): 2N: sending commit to replica 1: vsr.message_header.Header.Commit{ .checksum = 95827520960235074435848543336254118974, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 124893907086854881142772784205180813707, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 129, .timestamp_monotonic = 35849713139338887, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-21 14:18:52.847Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-21 14:18:52.847Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-21 14:18:52.847Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-21 14:18:52.847Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-21 14:18:52.847Z debug(vsr): 2: journal_repair_timeout fired
2025-11-21 14:18:52.847Z debug(vsr): 2: journal_repair_timeout reset
2025-11-21 14:18:52.847Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-21 14:18:52.847Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-21 14:18:52.853Z info(supervisor): 2: terminating replica
2025-11-21 14:18:52.883Z error: TestFailed
warning(message_bus): 243817616567816617951908231182086174208: on_recv: from=vsr.Peer{ .replica = 0 } error.ConnectionResetByPeer
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:295:21: 0x1307e40 in run (vortex)
                    return error.TestFailed;
                    ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:207:5: 0x130ca01 in main (vortex)
    try supervisor.run();
    ^
/root/tigerbeetle/working/main/src/vortex.zig:61:42: 0x1321664 in main (vortex)
        .supervisor => |supervisor_args| try Supervisor.main(allocator, supervisor_args),
                                         ^
