rved = { 0, 0, 0 } }
2025-12-07 12:44:42.413Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:42.413Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:42.423Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:42.423Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:42.423Z debug(vsr): 0: journal_repair_timeout fired
2025-12-07 12:44:42.423Z debug(vsr): 0: journal_repair_timeout reset
2025-12-07 12:44:42.431Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-07 12:44:42.431Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-07 12:44:42.433Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:42.433Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:42.441Z debug(vsr): 2: journal_repair_timeout fired
2025-12-07 12:44:42.441Z debug(vsr): 2: journal_repair_timeout reset
2025-12-07 12:44:42.443Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:42.443Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:42.447Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 55289713311495882860033561441360490038, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 261783137709561629537946279077579058775, .parent_padding = 0, .prepare_checksum = 336266405203611309553246573784598909073, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 33, .commit_min = 34, .timestamp = 1765111480611324587, .request = 31, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.447Z debug(replica): 1N: on_prepare_ok: not preparing op=33 checksum=336266405203611309553246573784598909073
2025-12-07 12:44:42.447Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 305568265243436658138545919813361226046, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 336266405203611309553246573784598909073, .parent_padding = 0, .prepare_checksum = 63240329442105506921015589920831535087, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 34, .commit_min = 34, .timestamp = 1765111480614675202, .request = 32, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.447Z debug(replica): 1N: on_prepare_ok: not preparing op=34 checksum=63240329442105506921015589920831535087
2025-12-07 12:44:42.447Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 247860371964847383942032221959795256207, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 63240329442105506921015589920831535087, .parent_padding = 0, .prepare_checksum = 85970882213097295027892943847923688077, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 35, .commit_min = 34, .timestamp = 1765111481505553348, .request = 33, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.447Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:42.447Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-12-07 12:44:42.447Z debug(replica): 1N: on_prepare_ok: quorum received, context=85970882213097295027892943847923688077
2025-12-07 12:44:42.447Z debug(vsr): 1: prepare_timeout stopped
2025-12-07 12:44:42.447Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-12-07 12:44:42.448Z debug(replica): 1N: execute_op: executing view=1 primary=true op=35 checksum=85970882213097295027892943847923688077 (create_transfers)
2025-12-07 12:44:42.448Z debug(replica): 1N: execute_op: commit_timestamp=1765111480614675202 prepare.header.timestamp=1765111481505553348
2025-12-07 12:44:42.451Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-07 12:44:42.451Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-07 12:44:42.460Z debug(replica): 1N: execute_op: advancing commit_max=34..35
2025-12-07 12:44:42.460Z debug(replica): 1N: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=33
2025-12-07 12:44:42.460Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 180819639911901493609280856564294139522, .checksum_padding = 0, .checksum_body = 129401523131532684110404109993544250211, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 432, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 86141672477479994539735429556997328706, .request_checksum_padding = 0, .context = 222893685366458787395379171808406800102, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 35, .commit = 35, .timestamp = 1765111481505553348, .request = 33, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.460Z debug(replica): 1N: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 180819639911901493609280856564294139522, .checksum_padding = 0, .checksum_body = 129401523131532684110404109993544250211, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 432, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 86141672477479994539735429556997328706, .request_checksum_padding = 0, .context = 222893685366458787395379171808406800102, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 35, .commit = 35, .timestamp = 1765111481505553348, .request = 33, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.460Z debug(forest): entering forest.compact() op=35 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 308518150047980233983200754653755704418: on_reply: slow request, request=33 op=35 size=331136 create_transfers time=957ms
2025-12-07 12:44:42.463Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:42.463Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:42.469Z debug(client_replies): 1: write_reply: wrote (client=308518150047980233983200754653755704418 request=33)
2025-12-07 12:44:42.469Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:42.469Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:42.469Z debug(vsr): 1: journal_repair_timeout fired
2025-12-07 12:44:42.469Z debug(vsr): 1: journal_repair_timeout reset
2025-12-07 12:44:42.471Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-07 12:44:42.471Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-07 12:44:42.471Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 195733741345923175235940755205463443404, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 222893685366458787395379171808406800102, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 34, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 957649553, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.471Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 195733741345923175235940755205463443404, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 222893685366458787395379171808406800102, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 34, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 957649553, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.471Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-07 12:44:42.471Z debug(replica): 1N: on_request: new request
2025-12-07 12:44:42.471Z debug(replica): 1N: primary_pipeline_prepare: request checksum=195733741345923175235940755205463443404 client=308518150047980233983200754653755704418
2025-12-07 12:44:42.471Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 195733741345923175235940755205463443404, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 222893685366458787395379171808406800102, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 34, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 957649553, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.471Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=238875211410305915178031641343879996928 op=36
2025-12-07 12:44:42.471Z debug(vsr): 1: prepare_timeout started
2025-12-07 12:44:42.471Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-07 12:44:42.471Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:42.471Z debug(replica): 1N: replicate: replicating op=36 to replica 0
2025-12-07 12:44:42.471Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 238875211410305915178031641343879996928, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 85970882213097295027892943847923688077, .parent_padding = 0, .request_checksum = 195733741345923175235940755205463443404, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit = 35, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.471Z debug(replica): 1N: replicate: replicating op=36 to replica 2
2025-12-07 12:44:42.471Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 238875211410305915178031641343879996928, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 85970882213097295027892943847923688077, .parent_padding = 0, .request_checksum = 195733741345923175235940755205463443404, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit = 35, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.472Z debug(replica): 1N: on_prepare: advancing: op=35..36 checksum=85970882213097295027892943847923688077..238875211410305915178031641343879996928
2025-12-07 12:44:42.472Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 238875211410305915178031641343879996928, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 85970882213097295027892943847923688077, .parent_padding = 0, .request_checksum = 195733741345923175235940755205463443404, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit = 35, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.472Z debug(journal): 1: set_header_as_dirty: op=36 checksum=238875211410305915178031641343879996928
2025-12-07 12:44:42.472Z debug(replica): 1N: append: appending to journal op=36
2025-12-07 12:44:42.472Z debug(replica): 0n: on_prepare: advancing commit_max=34..35
2025-12-07 12:44:42.472Z debug(replica): 0n: on_prepare: caching prepare.op=36 (commit_min=34 op=35 commit_max=35 prepare_max=1007)
2025-12-07 12:44:42.472Z debug(journal): 1: write: view=1 slot=36 op=36 len=1216: 238875211410305915178031641343879996928 starting
2025-12-07 12:44:42.472Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=37748736 len=4096 locked
2025-12-07 12:44:42.472Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 238875211410305915178031641343879996928, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 85970882213097295027892943847923688077, .parent_padding = 0, .request_checksum = 195733741345923175235940755205463443404, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit = 35, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.472Z debug(replica): 0n: on_prepare: advancing: op=35..36 checksum=85970882213097295027892943847923688077..238875211410305915178031641343879996928
2025-12-07 12:44:42.472Z debug(journal): 0: set_header_as_dirty: op=36 checksum=238875211410305915178031641343879996928
2025-12-07 12:44:42.472Z debug(replica): 2n: on_prepare: advancing commit_max=34..35
2025-12-07 12:44:42.472Z debug(replica): 0n: append: appending to journal op=36
2025-12-07 12:44:42.472Z debug(replica): 2n: on_prepare: caching prepare.op=36 (commit_min=34 op=35 commit_max=35 prepare_max=1007)
2025-12-07 12:44:42.472Z debug(journal): 0: write: view=1 slot=36 op=36 len=1216: 238875211410305915178031641343879996928 starting
2025-12-07 12:44:42.472Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=37748736 len=4096 locked
2025-12-07 12:44:42.472Z debug(replica): 0n: commit_start_journal: cached prepare op=35 checksum=85970882213097295027892943847923688077
2025-12-07 12:44:42.472Z debug(replica): 2n: on_prepare: advancing: op=35..36 checksum=85970882213097295027892943847923688077..238875211410305915178031641343879996928
2025-12-07 12:44:42.472Z debug(journal): 2: set_header_as_dirty: op=36 checksum=238875211410305915178031641343879996928
2025-12-07 12:44:42.472Z debug(replica): 2n: append: appending to journal op=36
2025-12-07 12:44:42.472Z debug(journal): 2: write: view=1 slot=36 op=36 len=1216: 238875211410305915178031641343879996928 starting
2025-12-07 12:44:42.472Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=37748736 len=4096 unlocked
2025-12-07 12:44:42.472Z debug(journal): 1: write_header: op=36 sectors[8192..12288]
2025-12-07 12:44:42.472Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=37748736 len=4096 locked
2025-12-07 12:44:42.472Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:42.472Z debug(replica): 2n: commit_start_journal: cached prepare op=35 checksum=85970882213097295027892943847923688077
2025-12-07 12:44:42.472Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:42.472Z debug(journal): 1: write: view=1 slot=36 op=36 len=1216: 238875211410305915178031641343879996928 complete, marking clean
2025-12-07 12:44:42.472Z debug(replica): 1N: send_prepare_ok: op=36 checksum=238875211410305915178031641343879996928
2025-12-07 12:44:42.472Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 129518621081655754632215993237747821443, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 85970882213097295027892943847923688077, .parent_padding = 0, .prepare_checksum = 238875211410305915178031641343879996928, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit_min = 35, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.472Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 129518621081655754632215993237747821443, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 85970882213097295027892943847923688077, .parent_padding = 0, .prepare_checksum = 238875211410305915178031641343879996928, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit_min = 35, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.472Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:42.472Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-07 12:44:42.472Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-07 12:44:42.472Z debug(replica): 0n: repair_prepare: op=36 checksum=238875211410305915178031641343879996928 (already writing)
2025-12-07 12:44:42.472Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=34)
2025-12-07 12:44:42.472Z debug(replica): 2n: repair_prepare: op=36 checksum=238875211410305915178031641343879996928 (already writing)
2025-12-07 12:44:42.472Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=34)
2025-12-07 12:44:42.473Z debug(replica): 0n: execute_op: executing view=1 primary=false op=35 checksum=85970882213097295027892943847923688077 (create_transfers)
2025-12-07 12:44:42.473Z debug(replica): 0n: execute_op: commit_timestamp=1765111480614675202 prepare.header.timestamp=1765111481505553348
2025-12-07 12:44:42.473Z debug(replica): 2n: execute_op: executing view=1 primary=false op=35 checksum=85970882213097295027892943847923688077 (create_transfers)
2025-12-07 12:44:42.473Z debug(replica): 2n: execute_op: commit_timestamp=1765111480614675202 prepare.header.timestamp=1765111481505553348
2025-12-07 12:44:42.485Z debug(replica): 0n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=33
2025-12-07 12:44:42.485Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 180819639911901493609280856564294139522, .checksum_padding = 0, .checksum_body = 129401523131532684110404109993544250211, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 432, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 86141672477479994539735429556997328706, .request_checksum_padding = 0, .context = 222893685366458787395379171808406800102, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 35, .commit = 35, .timestamp = 1765111481505553348, .request = 33, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.485Z debug(replica): 0n: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 180819639911901493609280856564294139522, .checksum_padding = 0, .checksum_body = 129401523131532684110404109993544250211, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 432, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 86141672477479994539735429556997328706, .request_checksum_padding = 0, .context = 222893685366458787395379171808406800102, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 35, .commit = 35, .timestamp = 1765111481505553348, .request = 33, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.485Z debug(replica): 2n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=33
2025-12-07 12:44:42.485Z debug(forest): entering forest.compact() op=35 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:42.485Z debug(forest): entering forest.compact() op=35 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:42.489Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:42.489Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:42.494Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=37748736 len=4096 unlocked
2025-12-07 12:44:42.494Z debug(journal): 0: write_header: op=36 sectors[8192..12288]
2025-12-07 12:44:42.494Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:42.494Z debug(client_replies): 0: write_reply: wrote (client=308518150047980233983200754653755704418 request=33)
2025-12-07 12:44:42.494Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:42.494Z debug(journal): 0: write: view=1 slot=36 op=36 len=1216: 238875211410305915178031641343879996928 complete, marking clean
2025-12-07 12:44:42.494Z debug(replica): 0n: send_prepare_ok: op=36 checksum=238875211410305915178031641343879996928
2025-12-07 12:44:42.494Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 11074122810676198359008959813738693932, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 85970882213097295027892943847923688077, .parent_padding = 0, .prepare_checksum = 238875211410305915178031641343879996928, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit_min = 35, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.497Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=37748736 len=4096 unlocked
2025-12-07 12:44:42.497Z debug(journal): 2: write_header: op=36 sectors[8192..12288]
2025-12-07 12:44:42.497Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:42.497Z debug(client_replies): 2: write_reply: wrote (client=308518150047980233983200754653755704418 request=33)
2025-12-07 12:44:42.497Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:42.497Z debug(journal): 2: write: view=1 slot=36 op=36 len=1216: 238875211410305915178031641343879996928 complete, marking clean
2025-12-07 12:44:42.497Z debug(replica): 2n: send_prepare_ok: op=36 checksum=238875211410305915178031641343879996928
2025-12-07 12:44:42.497Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 228596006507457076194847753451179022501, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 85970882213097295027892943847923688077, .parent_padding = 0, .prepare_checksum = 238875211410305915178031641343879996928, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit_min = 35, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.497Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 228596006507457076194847753451179022501, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 85970882213097295027892943847923688077, .parent_padding = 0, .prepare_checksum = 238875211410305915178031641343879996928, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit_min = 35, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:42.497Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:42.497Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-12-07 12:44:42.497Z debug(replica): 1N: on_prepare_ok: quorum received, context=238875211410305915178031641343879996928
2025-12-07 12:44:42.497Z debug(vsr): 1: prepare_timeout stopped
2025-12-07 12:44:42.497Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-12-07 12:44:42.497Z debug(replica): 1N: execute_op: executing view=1 primary=true op=36 checksum=238875211410305915178031641343879996928 (lookup_accounts)
2025-12-07 12:44:42.497Z debug(replica): 1N: execute_op: commit_timestamp=1765111481505553348 prepare.header.timestamp=1765111482471866398
2025-12-07 12:44:42.497Z debug(replica): 1N: execute_op: advancing commit_max=35..36
2025-12-07 12:44:42.497Z debug(replica): 1N: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=34
2025-12-07 12:44:42.497Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 101242688849568941386140975785512591926, .checksum_padding = 0, .checksum_body = 135507521567594955949836790787384546610, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 195733741345923175235940755205463443404, .request_checksum_padding = 0, .context = 18124691114262502011199125918491031931, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit = 36, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.497Z debug(replica): 1N: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 101242688849568941386140975785512591926, .checksum_padding = 0, .checksum_body = 135507521567594955949836790787384546610, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 195733741345923175235940755205463443404, .request_checksum_padding = 0, .context = 18124691114262502011199125918491031931, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit = 36, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.497Z debug(forest): entering forest.compact() op=36 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:42.497Z info(workload): accounts created = 59, transfers = 2584, pending transfers = 0, commands run = 17
2025-12-07 12:44:42.498Z debug(client_replies): 1: write_reply: wrote (client=308518150047980233983200754653755704418 request=34)
2025-12-07 12:44:42.498Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 122829339514885198021462345860929232069, .checksum_padding = 0, .checksum_body = 15138541315695898263274915221378946327, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 41280, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 18124691114262502011199125918491031931, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 35, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 26190602, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:42.498Z debug(replica): 1N: on_request: new request
2025-12-07 12:44:42.498Z debug(replica): 1N: primary_pipeline_prepare: request checksum=122829339514885198021462345860929232069 client=308518150047980233983200754653755704418
2025-12-07 12:44:42.498Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=256040641953740902299633398237491859755 op=37
2025-12-07 12:44:42.498Z debug(vsr): 1: prepare_timeout started
2025-12-07 12:44:42.498Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 122829339514885198021462345860929232069, .checksum_padding = 0, .checksum_body = 15138541315695898263274915221378946327, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 41280, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 18124691114262502011199125918491031931, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 35, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 26190602, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.070Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-07 12:44:43.070Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:43.070Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-07 12:44:43.070Z debug(replica): 1N: replicate: replicating op=37 to replica 0
2025-12-07 12:44:43.070Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 256040641953740902299633398237491859755, .checksum_padding = 0, .checksum_body = 15138541315695898263274915221378946327, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 41280, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 238875211410305915178031641343879996928, .parent_padding = 0, .request_checksum = 122829339514885198021462345860929232069, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit = 36, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.070Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 122829339514885198021462345860929232069, .checksum_padding = 0, .checksum_body = 15138541315695898263274915221378946327, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 41280, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 18124691114262502011199125918491031931, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 35, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 26190602, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.070Z debug(replica): 1N: replicate: replicating op=37 to replica 2
2025-12-07 12:44:43.070Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 256040641953740902299633398237491859755, .checksum_padding = 0, .checksum_body = 15138541315695898263274915221378946327, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 41280, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 238875211410305915178031641343879996928, .parent_padding = 0, .request_checksum = 122829339514885198021462345860929232069, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit = 36, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.070Z debug(replica): 1N: on_prepare: advancing: op=36..37 checksum=238875211410305915178031641343879996928..256040641953740902299633398237491859755
2025-12-07 12:44:43.070Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:43.070Z debug(journal): 1: set_header_as_dirty: op=37 checksum=256040641953740902299633398237491859755
2025-12-07 12:44:43.070Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:43.070Z debug(replica): 1N: append: appending to journal op=37
2025-12-07 12:44:43.070Z debug(journal): 1: write: view=1 slot=37 op=37 len=41280: 256040641953740902299633398237491859755 starting
2025-12-07 12:44:43.070Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=38797312 len=45056 locked
2025-12-07 12:44:43.070Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 256040641953740902299633398237491859755, .checksum_padding = 0, .checksum_body = 15138541315695898263274915221378946327, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 41280, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 238875211410305915178031641343879996928, .parent_padding = 0, .request_checksum = 122829339514885198021462345860929232069, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit = 36, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.070Z debug(replica): 0n: on_prepare: advancing commit_max=35..36
2025-12-07 12:44:43.070Z debug(replica): 0n: on_prepare: caching prepare.op=37 (commit_min=35 op=36 commit_max=36 prepare_max=1007)
2025-12-07 12:44:43.070Z debug(replica): 0n: on_prepare: advancing: op=36..37 checksum=238875211410305915178031641343879996928..256040641953740902299633398237491859755
2025-12-07 12:44:43.070Z debug(journal): 0: set_header_as_dirty: op=37 checksum=256040641953740902299633398237491859755
2025-12-07 12:44:43.070Z debug(replica): 0n: append: appending to journal op=37
2025-12-07 12:44:43.070Z debug(journal): 0: write: view=1 slot=37 op=37 len=41280: 256040641953740902299633398237491859755 starting
2025-12-07 12:44:43.070Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=38797312 len=45056 locked
2025-12-07 12:44:43.070Z debug(replica): 0n: commit_start_journal: cached prepare op=36 checksum=238875211410305915178031641343879996928
2025-12-07 12:44:42.507Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-07 12:44:43.070Z debug(replica): 0n: repair_prepare: op=37 checksum=256040641953740902299633398237491859755 (already writing)
2025-12-07 12:44:43.070Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=35)
2025-12-07 12:44:43.070Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-07 12:44:43.070Z debug(replica): 0n: execute_op: executing view=1 primary=false op=36 checksum=238875211410305915178031641343879996928 (lookup_accounts)
2025-12-07 12:44:43.071Z debug(replica): 0n: execute_op: commit_timestamp=1765111481505553348 prepare.header.timestamp=1765111482471866398
2025-12-07 12:44:43.071Z debug(replica): 0n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=34
2025-12-07 12:44:43.071Z debug(forest): entering forest.compact() op=36 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.071Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=38797312 len=45056 unlocked
2025-12-07 12:44:43.071Z debug(journal): 1: write_header: op=37 sectors[8192..12288]
2025-12-07 12:44:43.071Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.071Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.071Z debug(journal): 1: write: view=1 slot=37 op=37 len=41280: 256040641953740902299633398237491859755 complete, marking clean
2025-12-07 12:44:43.071Z debug(replica): 1N: send_prepare_ok: op=37 checksum=256040641953740902299633398237491859755
2025-12-07 12:44:43.071Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 1541617409047202866438697040463599900, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 238875211410305915178031641343879996928, .parent_padding = 0, .prepare_checksum = 256040641953740902299633398237491859755, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit_min = 36, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.071Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 1541617409047202866438697040463599900, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 238875211410305915178031641343879996928, .parent_padding = 0, .prepare_checksum = 256040641953740902299633398237491859755, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit_min = 36, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.071Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:43.071Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-07 12:44:43.071Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-07 12:44:43.071Z debug(client_replies): 0: write_reply: wrote (client=308518150047980233983200754653755704418 request=34)
2025-12-07 12:44:43.079Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 256040641953740902299633398237491859755, .checksum_padding = 0, .checksum_body = 15138541315695898263274915221378946327, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 41280, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 238875211410305915178031641343879996928, .parent_padding = 0, .request_checksum = 122829339514885198021462345860929232069, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit = 36, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.079Z debug(replica): 2n: on_prepare: advancing commit_max=35..36
2025-12-07 12:44:43.079Z debug(replica): 2n: on_prepare: caching prepare.op=37 (commit_min=35 op=36 commit_max=36 prepare_max=1007)
2025-12-07 12:44:43.079Z debug(replica): 2n: on_prepare: advancing: op=36..37 checksum=238875211410305915178031641343879996928..256040641953740902299633398237491859755
2025-12-07 12:44:43.079Z debug(journal): 2: set_header_as_dirty: op=37 checksum=256040641953740902299633398237491859755
2025-12-07 12:44:43.079Z debug(replica): 2n: append: appending to journal op=37
2025-12-07 12:44:43.079Z debug(journal): 2: write: view=1 slot=37 op=37 len=41280: 256040641953740902299633398237491859755 starting
2025-12-07 12:44:43.079Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=38797312 len=45056 locked
2025-12-07 12:44:43.079Z debug(replica): 2n: commit_start_journal: cached prepare op=36 checksum=238875211410305915178031641343879996928
2025-12-07 12:44:43.079Z debug(replica): 2n: repair_prepare: op=37 checksum=256040641953740902299633398237491859755 (already writing)
2025-12-07 12:44:43.079Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=35)
2025-12-07 12:44:43.079Z debug(replica): 2n: execute_op: executing view=1 primary=false op=36 checksum=238875211410305915178031641343879996928 (lookup_accounts)
2025-12-07 12:44:43.079Z debug(replica): 2n: execute_op: commit_timestamp=1765111481505553348 prepare.header.timestamp=1765111482471866398
2025-12-07 12:44:43.080Z debug(replica): 2n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=34
2025-12-07 12:44:43.080Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 101242688849568941386140975785512591926, .checksum_padding = 0, .checksum_body = 135507521567594955949836790787384546610, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 195733741345923175235940755205463443404, .request_checksum_padding = 0, .context = 18124691114262502011199125918491031931, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit = 36, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.080Z debug(replica): 2n: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 101242688849568941386140975785512591926, .checksum_padding = 0, .checksum_body = 135507521567594955949836790787384546610, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 195733741345923175235940755205463443404, .request_checksum_padding = 0, .context = 18124691114262502011199125918491031931, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 36, .commit = 36, .timestamp = 1765111482471866398, .request = 34, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.080Z debug(forest): entering forest.compact() op=36 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.080Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:43.080Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:43.081Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=38797312 len=45056 unlocked
2025-12-07 12:44:43.081Z debug(journal): 0: write_header: op=37 sectors[8192..12288]
2025-12-07 12:44:43.081Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.081Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.081Z debug(journal): 0: write: view=1 slot=37 op=37 len=41280: 256040641953740902299633398237491859755 complete, marking clean
2025-12-07 12:44:43.081Z debug(replica): 0n: send_prepare_ok: op=37 checksum=256040641953740902299633398237491859755
2025-12-07 12:44:43.081Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 87230053081080571651046281265465490104, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 238875211410305915178031641343879996928, .parent_padding = 0, .prepare_checksum = 256040641953740902299633398237491859755, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit_min = 36, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.082Z debug(client_replies): 2: write_reply: wrote (client=308518150047980233983200754653755704418 request=34)
2025-12-07 12:44:43.082Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=38797312 len=45056 unlocked
2025-12-07 12:44:43.082Z debug(journal): 2: write_header: op=37 sectors[8192..12288]
2025-12-07 12:44:43.082Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.082Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.082Z debug(journal): 2: write: view=1 slot=37 op=37 len=41280: 256040641953740902299633398237491859755 complete, marking clean
2025-12-07 12:44:43.082Z debug(replica): 2n: send_prepare_ok: op=37 checksum=256040641953740902299633398237491859755
2025-12-07 12:44:43.082Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 260299616012721480916248775181704494566, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 238875211410305915178031641343879996928, .parent_padding = 0, .prepare_checksum = 256040641953740902299633398237491859755, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit_min = 36, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.082Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 260299616012721480916248775181704494566, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 238875211410305915178031641343879996928, .parent_padding = 0, .prepare_checksum = 256040641953740902299633398237491859755, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit_min = 36, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.082Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:43.082Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-12-07 12:44:43.082Z debug(replica): 1N: on_prepare_ok: quorum received, context=256040641953740902299633398237491859755
2025-12-07 12:44:43.082Z debug(vsr): 1: prepare_timeout stopped
2025-12-07 12:44:43.082Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-12-07 12:44:43.084Z debug(replica): 1N: execute_op: executing view=1 primary=true op=37 checksum=256040641953740902299633398237491859755 (lookup_transfers)
2025-12-07 12:44:43.084Z debug(replica): 1N: execute_op: commit_timestamp=1765111482471866398 prepare.header.timestamp=1765111482498703643
2025-12-07 12:44:43.085Z debug(replica): 1N: execute_op: advancing commit_max=36..37
2025-12-07 12:44:43.086Z debug(replica): 1N: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=35
2025-12-07 12:44:43.086Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 312800377260108180616355923452506297571, .checksum_padding = 0, .checksum_body = 291628704974223942082566320260493325553, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 328448, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 122829339514885198021462345860929232069, .request_checksum_padding = 0, .context = 183425624027570135853387932100962161606, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit = 37, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.086Z debug(replica): 1N: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 312800377260108180616355923452506297571, .checksum_padding = 0, .checksum_body = 291628704974223942082566320260493325553, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 328448, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 122829339514885198021462345860929232069, .request_checksum_padding = 0, .context = 183425624027570135853387932100962161606, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit = 37, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.086Z debug(forest): entering forest.compact() op=37 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.087Z debug(client_replies): 1: write_reply: wrote (client=308518150047980233983200754653755704418 request=35)
warning(client): 308518150047980233983200754653755704418: on_reply: slow request, request=35 op=37 size=41280 lookup_transfers time=590ms
2025-12-07 12:44:43.089Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 332171652621535710428799087491044152427, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 183425624027570135853387932100962161606, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 36, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 590442279, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.089Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 332171652621535710428799087491044152427, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 183425624027570135853387932100962161606, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 36, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 590442279, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.089Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-07 12:44:43.089Z debug(replica): 1N: on_request: new request
2025-12-07 12:44:43.089Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 332171652621535710428799087491044152427, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 183425624027570135853387932100962161606, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 36, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 590442279, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.089Z debug(replica): 1N: primary_pipeline_prepare: request checksum=332171652621535710428799087491044152427 client=308518150047980233983200754653755704418
2025-12-07 12:44:43.089Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=338466082336437125028226566829440593561 op=38
2025-12-07 12:44:43.089Z debug(vsr): 1: prepare_timeout started
2025-12-07 12:44:43.089Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-07 12:44:43.089Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:43.089Z debug(replica): 1N: replicate: replicating op=38 to replica 0
2025-12-07 12:44:43.089Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 338466082336437125028226566829440593561, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .request_checksum = 332171652621535710428799087491044152427, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit = 37, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.089Z debug(replica): 1N: replicate: replicating op=38 to replica 2
2025-12-07 12:44:43.089Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 338466082336437125028226566829440593561, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .request_checksum = 332171652621535710428799087491044152427, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit = 37, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.089Z debug(replica): 1N: on_prepare: advancing: op=37..38 checksum=256040641953740902299633398237491859755..338466082336437125028226566829440593561
2025-12-07 12:44:43.089Z debug(journal): 1: set_header_as_dirty: op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.089Z debug(replica): 1N: append: appending to journal op=38
2025-12-07 12:44:43.089Z debug(journal): 1: write: view=1 slot=38 op=38 len=1216: 338466082336437125028226566829440593561 starting
2025-12-07 12:44:43.089Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=39845888 len=4096 locked
2025-12-07 12:44:43.089Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 338466082336437125028226566829440593561, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .request_checksum = 332171652621535710428799087491044152427, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit = 37, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.089Z debug(replica): 2n: on_prepare: advancing commit_max=36..37
2025-12-07 12:44:43.089Z debug(replica): 2n: on_prepare: caching prepare.op=38 (commit_min=36 op=37 commit_max=37 prepare_max=1007)
2025-12-07 12:44:43.090Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=39845888 len=4096 unlocked
2025-12-07 12:44:43.090Z debug(journal): 1: write_header: op=38 sectors[8192..12288]
2025-12-07 12:44:43.090Z debug(replica): 2n: on_prepare: advancing: op=37..38 checksum=256040641953740902299633398237491859755..338466082336437125028226566829440593561
2025-12-07 12:44:43.090Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.090Z debug(journal): 2: set_header_as_dirty: op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.090Z debug(replica): 2n: append: appending to journal op=38
2025-12-07 12:44:43.090Z debug(journal): 2: write: view=1 slot=38 op=38 len=1216: 338466082336437125028226566829440593561 starting
2025-12-07 12:44:43.090Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=39845888 len=4096 locked
2025-12-07 12:44:43.090Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.090Z debug(replica): 2n: commit_start_journal: cached prepare op=37 checksum=256040641953740902299633398237491859755
2025-12-07 12:44:43.090Z debug(journal): 1: write: view=1 slot=38 op=38 len=1216: 338466082336437125028226566829440593561 complete, marking clean
2025-12-07 12:44:43.090Z debug(replica): 1N: send_prepare_ok: op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.090Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 105510657087889581359301918652374706343, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .prepare_checksum = 338466082336437125028226566829440593561, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit_min = 37, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.090Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 105510657087889581359301918652374706343, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .prepare_checksum = 338466082336437125028226566829440593561, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit_min = 37, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.090Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:43.090Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-07 12:44:43.090Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-07 12:44:43.090Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:43.090Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:43.091Z debug(replica): 2n: repair_prepare: op=38 checksum=338466082336437125028226566829440593561 (already writing)
2025-12-07 12:44:43.091Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=36)
2025-12-07 12:44:43.091Z debug(replica): 2n: execute_op: executing view=1 primary=false op=37 checksum=256040641953740902299633398237491859755 (lookup_transfers)
2025-12-07 12:44:43.091Z debug(replica): 2n: execute_op: commit_timestamp=1765111482471866398 prepare.header.timestamp=1765111482498703643
2025-12-07 12:44:43.094Z debug(replica): 2n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=35
2025-12-07 12:44:43.094Z debug(forest): entering forest.compact() op=37 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.095Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=39845888 len=4096 unlocked
2025-12-07 12:44:43.095Z debug(journal): 2: write_header: op=38 sectors[8192..12288]
2025-12-07 12:44:43.095Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.095Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-07 12:44:43.095Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-07 12:44:43.095Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.095Z debug(journal): 2: write: view=1 slot=38 op=38 len=1216: 338466082336437125028226566829440593561 complete, marking clean
2025-12-07 12:44:43.095Z debug(replica): 2n: send_prepare_ok: op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.095Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 261000644669433376369280664376215211270, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .prepare_checksum = 338466082336437125028226566829440593561, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit_min = 37, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.095Z debug(client_replies): 2: write_reply: wrote (client=308518150047980233983200754653755704418 request=35)
2025-12-07 12:44:43.095Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 261000644669433376369280664376215211270, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .prepare_checksum = 338466082336437125028226566829440593561, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit_min = 37, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.095Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:43.095Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-12-07 12:44:43.095Z debug(replica): 1N: on_prepare_ok: quorum received, context=338466082336437125028226566829440593561
2025-12-07 12:44:43.095Z debug(vsr): 1: prepare_timeout stopped
2025-12-07 12:44:43.095Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-12-07 12:44:43.095Z debug(replica): 1N: execute_op: executing view=1 primary=true op=38 checksum=338466082336437125028226566829440593561 (lookup_accounts)
2025-12-07 12:44:43.095Z debug(replica): 1N: execute_op: commit_timestamp=1765111482498703643 prepare.header.timestamp=1765111483089649924
2025-12-07 12:44:43.095Z debug(replica): 1N: execute_op: advancing commit_max=37..38
2025-12-07 12:44:43.095Z debug(replica): 1N: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=36
2025-12-07 12:44:43.095Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 127389848716175152096900763629003123837, .checksum_padding = 0, .checksum_body = 135507521567594955949836790787384546610, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 332171652621535710428799087491044152427, .request_checksum_padding = 0, .context = 245562320400712436195115824806322763527, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit = 38, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.095Z debug(replica): 1N: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 127389848716175152096900763629003123837, .checksum_padding = 0, .checksum_body = 135507521567594955949836790787384546610, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 332171652621535710428799087491044152427, .request_checksum_padding = 0, .context = 245562320400712436195115824806322763527, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit = 38, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.095Z debug(forest): entering forest.compact() op=38 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.096Z info(workload): accounts created = 59, transfers = 2584, pending transfers = 0, commands run = 18
2025-12-07 12:44:43.096Z debug(client_replies): 1: write_reply: wrote (client=308518150047980233983200754653755704418 request=36)
2025-12-07 12:44:43.097Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 261306942258224040353065558433072861127, .checksum_padding = 0, .checksum_body = 96965288366835005664658955306459873383, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 42880, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 245562320400712436195115824806322763527, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 6640631, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.097Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 261306942258224040353065558433072861127, .checksum_padding = 0, .checksum_body = 96965288366835005664658955306459873383, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 42880, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 245562320400712436195115824806322763527, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 6640631, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.097Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-07 12:44:43.097Z debug(replica): 1N: on_request: new request
2025-12-07 12:44:43.097Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 261306942258224040353065558433072861127, .checksum_padding = 0, .checksum_body = 96965288366835005664658955306459873383, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 42880, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 245562320400712436195115824806322763527, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 37, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 6640631, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.097Z debug(replica): 1N: primary_pipeline_prepare: request checksum=261306942258224040353065558433072861127 client=308518150047980233983200754653755704418
2025-12-07 12:44:43.097Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=209184889614926110049600513064513212044 op=39
2025-12-07 12:44:43.097Z debug(vsr): 1: prepare_timeout started
2025-12-07 12:44:43.097Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-07 12:44:43.097Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:43.097Z debug(replica): 1N: replicate: replicating op=39 to replica 0
2025-12-07 12:44:43.097Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 209184889614926110049600513064513212044, .checksum_padding = 0, .checksum_body = 96965288366835005664658955306459873383, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 42880, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 338466082336437125028226566829440593561, .parent_padding = 0, .request_checksum = 261306942258224040353065558433072861127, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit = 38, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.097Z debug(replica): 1N: replicate: replicating op=39 to replica 2
2025-12-07 12:44:43.097Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 209184889614926110049600513064513212044, .checksum_padding = 0, .checksum_body = 96965288366835005664658955306459873383, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 42880, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 338466082336437125028226566829440593561, .parent_padding = 0, .request_checksum = 261306942258224040353065558433072861127, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit = 38, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.097Z debug(replica): 1N: on_prepare: advancing: op=38..39 checksum=338466082336437125028226566829440593561..209184889614926110049600513064513212044
2025-12-07 12:44:43.097Z debug(journal): 1: set_header_as_dirty: op=39 checksum=209184889614926110049600513064513212044
2025-12-07 12:44:43.097Z debug(replica): 1N: append: appending to journal op=39
2025-12-07 12:44:43.097Z debug(journal): 1: write: view=1 slot=39 op=39 len=42880: 209184889614926110049600513064513212044 starting
2025-12-07 12:44:43.097Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=45056 locked
2025-12-07 12:44:43.097Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 209184889614926110049600513064513212044, .checksum_padding = 0, .checksum_body = 96965288366835005664658955306459873383, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 42880, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 338466082336437125028226566829440593561, .parent_padding = 0, .request_checksum = 261306942258224040353065558433072861127, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit = 38, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.097Z debug(replica): 0n: on_prepare: advancing commit_max=36..38
2025-12-07 12:44:43.097Z debug(replica): 0n: on_prepare: caching prepare.op=39 (commit_min=36 op=37 commit_max=38 prepare_max=1007)
2025-12-07 12:44:43.097Z debug(replica): 0n: on_prepare: newer op
2025-12-07 12:44:43.097Z debug(replica): 0n: jump_to_newer_op: advancing: op=37..38 checksum=256040641953740902299633398237491859755..338466082336437125028226566829440593561
2025-12-07 12:44:43.097Z debug(replica): 0n: on_prepare: advancing: op=38..39 checksum=338466082336437125028226566829440593561..209184889614926110049600513064513212044
2025-12-07 12:44:43.097Z debug(journal): 0: set_header_as_dirty: op=39 checksum=209184889614926110049600513064513212044
2025-12-07 12:44:43.097Z debug(replica): 0n: append: appending to journal op=39
2025-12-07 12:44:43.097Z debug(journal): 0: write: view=1 slot=39 op=39 len=42880: 209184889614926110049600513064513212044 starting
2025-12-07 12:44:43.097Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=45056 locked
2025-12-07 12:44:43.097Z debug(replica): 0n: valid_hash_chain_between: missing op=38
2025-12-07 12:44:43.097Z debug(replica): 0n: commit_start_journal: waiting for repair (hash chain)
2025-12-07 12:44:43.097Z debug(replica): 0n: commit_start_journal: cached prepare op=37 checksum=256040641953740902299633398237491859755
2025-12-07 12:44:43.097Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 209184889614926110049600513064513212044, .checksum_padding = 0, .checksum_body = 96965288366835005664658955306459873383, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 42880, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 338466082336437125028226566829440593561, .parent_padding = 0, .request_checksum = 261306942258224040353065558433072861127, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit = 38, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.097Z debug(replica): 2n: on_prepare: advancing commit_max=37..38
2025-12-07 12:44:43.097Z debug(replica): 2n: on_prepare: caching prepare.op=39 (commit_min=37 op=38 commit_max=38 prepare_max=1007)
2025-12-07 12:44:43.097Z debug(replica): 2n: on_prepare: advancing: op=38..39 checksum=338466082336437125028226566829440593561..209184889614926110049600513064513212044
2025-12-07 12:44:43.097Z debug(journal): 2: set_header_as_dirty: op=39 checksum=209184889614926110049600513064513212044
2025-12-07 12:44:43.097Z debug(replica): 2n: append: appending to journal op=39
2025-12-07 12:44:43.097Z debug(journal): 2: write: view=1 slot=39 op=39 len=42880: 209184889614926110049600513064513212044 starting
2025-12-07 12:44:43.097Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=45056 locked
2025-12-07 12:44:43.098Z debug(replica): 2n: commit_start_journal: cached prepare op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.098Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=45056 unlocked
2025-12-07 12:44:43.098Z debug(journal): 1: write_header: op=39 sectors[8192..12288]
2025-12-07 12:44:43.098Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.098Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.098Z debug(journal): 1: write: view=1 slot=39 op=39 len=42880: 209184889614926110049600513064513212044 complete, marking clean
2025-12-07 12:44:43.098Z debug(replica): 1N: send_prepare_ok: op=39 checksum=209184889614926110049600513064513212044
2025-12-07 12:44:43.098Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 314182497324938505515120343748394111595, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 338466082336437125028226566829440593561, .parent_padding = 0, .prepare_checksum = 209184889614926110049600513064513212044, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit_min = 38, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.098Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 314182497324938505515120343748394111595, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 338466082336437125028226566829440593561, .parent_padding = 0, .prepare_checksum = 209184889614926110049600513064513212044, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit_min = 38, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.098Z debug(replica): 2n: repair_prepare: op=39 checksum=209184889614926110049600513064513212044 (already writing)
2025-12-07 12:44:43.098Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:43.098Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-07 12:44:43.098Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-07 12:44:43.098Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=37)
2025-12-07 12:44:43.098Z debug(replica): 2n: execute_op: executing view=1 primary=false op=38 checksum=338466082336437125028226566829440593561 (lookup_accounts)
2025-12-07 12:44:43.098Z debug(replica): 2n: execute_op: commit_timestamp=1765111482498703643 prepare.header.timestamp=1765111483089649924
2025-12-07 12:44:43.098Z debug(replica): 2n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=36
2025-12-07 12:44:43.098Z debug(forest): entering forest.compact() op=38 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.098Z debug(client_replies): 2: write_reply: wrote (client=308518150047980233983200754653755704418 request=36)
2025-12-07 12:44:43.099Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=45056 unlocked
2025-12-07 12:44:43.099Z debug(journal): 2: write_header: op=39 sectors[8192..12288]
2025-12-07 12:44:43.099Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.099Z debug(replica): 0n: repair: break: view=1 break=38..38 (commit=36..38 op=39)
2025-12-07 12:44:43.099Z debug(replica): 0n: sending request_headers to replica 1: vsr.message_header.Header.RequestHeaders{ .checksum = 44874284482224932039189864856621613638, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.0, .protocol = 0, .command = vsr.Command.request_headers, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .op_min = 0, .op_max = 38, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.099Z debug(replica): 0n: repair_prepare: op=38 checksum=0 replica=1 latency=1ms (committed, not present, normal)
2025-12-07 12:44:43.099Z debug(replica): 0n: sending request_prepare to replica 1: vsr.message_header.Header.RequestPrepare{ .checksum = 48443765190173170555198295292827068129, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.request_prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .prepare_checksum = 0, .prepare_checksum_padding = 0, .prepare_op = 38, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.099Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.099Z debug(journal): 2: write: view=1 slot=39 op=39 len=42880: 209184889614926110049600513064513212044 complete, marking clean
2025-12-07 12:44:43.099Z debug(replica): 2n: send_prepare_ok: op=39 checksum=209184889614926110049600513064513212044
2025-12-07 12:44:43.099Z debug(replica): 0n: repair_prepare: op=39 checksum=209184889614926110049600513064513212044 (already writing)
2025-12-07 12:44:43.099Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 91926686886942780706084840003495241412, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 338466082336437125028226566829440593561, .parent_padding = 0, .prepare_checksum = 209184889614926110049600513064513212044, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit_min = 38, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.099Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=36)
2025-12-07 12:44:43.099Z debug(replica): 0n: execute_op: executing view=1 primary=false op=37 checksum=256040641953740902299633398237491859755 (lookup_transfers)
2025-12-07 12:44:43.099Z debug(replica): 0n: execute_op: commit_timestamp=1765111482471866398 prepare.header.timestamp=1765111482498703643
2025-12-07 12:44:43.099Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 91926686886942780706084840003495241412, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 338466082336437125028226566829440593561, .parent_padding = 0, .prepare_checksum = 209184889614926110049600513064513212044, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit_min = 38, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.099Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:43.099Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-12-07 12:44:43.099Z debug(replica): 1N: on_prepare_ok: quorum received, context=209184889614926110049600513064513212044
2025-12-07 12:44:43.099Z debug(vsr): 1: prepare_timeout stopped
2025-12-07 12:44:43.099Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-12-07 12:44:43.099Z debug(replica): 1N: execute_op: executing view=1 primary=true op=39 checksum=209184889614926110049600513064513212044 (create_transfers)
2025-12-07 12:44:43.101Z debug(replica): 0n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=35
2025-12-07 12:44:43.115Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-07 12:44:43.546Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-07 12:44:43.546Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 312800377260108180616355923452506297571, .checksum_padding = 0, .checksum_body = 291628704974223942082566320260493325553, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 328448, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 122829339514885198021462345860929232069, .request_checksum_padding = 0, .context = 183425624027570135853387932100962161606, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit = 37, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.546Z debug(replica): 0n: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 312800377260108180616355923452506297571, .checksum_padding = 0, .checksum_body = 291628704974223942082566320260493325553, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 328448, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 122829339514885198021462345860929232069, .request_checksum_padding = 0, .context = 183425624027570135853387932100962161606, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 37, .commit = 37, .timestamp = 1765111482498703643, .request = 35, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.099Z debug(replica): 1N: execute_op: commit_timestamp=1765111483089649924 prepare.header.timestamp=1765111483097255412
2025-12-07 12:44:43.546Z debug(forest): entering forest.compact() op=37 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.547Z warning(replica): 0n: commit_dispatch: slow request, request=35 size=41280 lookup_transfers time=449ms
2025-12-07 12:44:43.547Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=40894464 len=45056 unlocked
2025-12-07 12:44:43.547Z debug(journal): 0: write_header: op=39 sectors[8192..12288]
2025-12-07 12:44:43.547Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.548Z debug(client_replies): 0: write_reply: wrote (client=308518150047980233983200754653755704418 request=35)
2025-12-07 12:44:43.548Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.548Z debug(journal): 0: write: view=1 slot=39 op=39 len=42880: 209184889614926110049600513064513212044 complete, marking clean
2025-12-07 12:44:43.548Z debug(replica): 0n: send_prepare_ok: op=39 checksum=209184889614926110049600513064513212044
2025-12-07 12:44:43.548Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 230386863093678356731270054333118807166, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 338466082336437125028226566829440593561, .parent_padding = 0, .prepare_checksum = 209184889614926110049600513064513212044, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit_min = 37, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.551Z debug(replica): 1N: execute_op: advancing commit_max=38..39
2025-12-07 12:44:43.551Z debug(replica): 1N: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=37
2025-12-07 12:44:43.551Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 84809199215624751237545935048658185206, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 261306942258224040353065558433072861127, .request_checksum_padding = 0, .context = 4705306448028604170785792023902478197, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit = 39, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.551Z debug(replica): 1N: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 84809199215624751237545935048658185206, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 261306942258224040353065558433072861127, .request_checksum_padding = 0, .context = 4705306448028604170785792023902478197, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit = 39, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.551Z debug(forest): entering forest.compact() op=39 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 308518150047980233983200754653755704418: on_reply: slow request, request=37 op=39 size=42880 create_transfers time=454ms
2025-12-07 12:44:43.554Z warning(replica): 1N: commit_dispatch: slow request, request=37 size=42880 create_transfers time=455ms
2025-12-07 12:44:43.555Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 115896615589171071381275115656110708871, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 4705306448028604170785792023902478197, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 38, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 454915005, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.555Z debug(replica): 1N: on_request: new request
2025-12-07 12:44:43.555Z debug(replica): 1N: primary_pipeline_prepare: request checksum=115896615589171071381275115656110708871 client=308518150047980233983200754653755704418
2025-12-07 12:44:43.555Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=165077665185696621985649682213798088050 op=40
2025-12-07 12:44:43.555Z debug(vsr): 1: prepare_timeout started
2025-12-07 12:44:43.555Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-07 12:44:43.555Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:43.555Z debug(replica): 1N: replicate: replicating op=40 to replica 0
2025-12-07 12:44:43.555Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 165077665185696621985649682213798088050, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 209184889614926110049600513064513212044, .parent_padding = 0, .request_checksum = 115896615589171071381275115656110708871, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit = 39, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.555Z debug(replica): 1N: replicate: replicating op=40 to replica 2
2025-12-07 12:44:43.555Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 165077665185696621985649682213798088050, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 209184889614926110049600513064513212044, .parent_padding = 0, .request_checksum = 115896615589171071381275115656110708871, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit = 39, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.555Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 165077665185696621985649682213798088050, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 209184889614926110049600513064513212044, .parent_padding = 0, .request_checksum = 115896615589171071381275115656110708871, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit = 39, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.555Z debug(replica): 0n: on_prepare: advancing commit_max=38..39
2025-12-07 12:44:43.555Z debug(replica): 1N: on_prepare: advancing: op=39..40 checksum=209184889614926110049600513064513212044..165077665185696621985649682213798088050
2025-12-07 12:44:43.555Z debug(replica): 0n: on_prepare: caching prepare.op=40 (commit_min=37 op=39 commit_max=39 prepare_max=1007)
2025-12-07 12:44:43.555Z debug(journal): 1: set_header_as_dirty: op=40 checksum=165077665185696621985649682213798088050
2025-12-07 12:44:43.555Z debug(replica): 1N: append: appending to journal op=40
2025-12-07 12:44:43.555Z debug(replica): 0n: on_prepare: advancing: op=39..40 checksum=209184889614926110049600513064513212044..165077665185696621985649682213798088050
2025-12-07 12:44:43.555Z debug(journal): 0: set_header_as_dirty: op=40 checksum=165077665185696621985649682213798088050
2025-12-07 12:44:43.555Z debug(replica): 0n: append: appending to journal op=40
2025-12-07 12:44:43.555Z debug(journal): 1: write: view=1 slot=40 op=40 len=1216: 165077665185696621985649682213798088050 starting
2025-12-07 12:44:43.555Z debug(journal): 0: write: view=1 slot=40 op=40 len=1216: 165077665185696621985649682213798088050 starting
2025-12-07 12:44:43.555Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 locked
2025-12-07 12:44:43.555Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 locked
2025-12-07 12:44:43.555Z debug(replica): 0n: repair: break: view=1 break=38..38 (commit=37..39 op=40)
2025-12-07 12:44:43.555Z debug(client_replies): 1: write_reply: wrote (client=308518150047980233983200754653755704418 request=37)
2025-12-07 12:44:43.555Z debug(replica): 0n: sending request_headers to replica 2: vsr.message_header.Header.RequestHeaders{ .checksum = 224804965419434535459951928278972405121, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.0, .protocol = 0, .command = vsr.Command.request_headers, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .op_min = 38, .op_max = 38, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.555Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 165077665185696621985649682213798088050, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 209184889614926110049600513064513212044, .parent_padding = 0, .request_checksum = 115896615589171071381275115656110708871, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit = 39, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.555Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:43.555Z debug(replica): 2n: on_prepare: advancing commit_max=38..39
2025-12-07 12:44:43.555Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:43.555Z debug(replica): 2n: on_prepare: caching prepare.op=40 (commit_min=38 op=39 commit_max=39 prepare_max=1007)
2025-12-07 12:44:43.555Z debug(replica): 0n: repair_prepare: op=38 checksum=0 replica=2 latency=1ms (committed, not present, normal)
2025-12-07 12:44:43.555Z debug(replica): 2n: on_prepare: advancing: op=39..40 checksum=209184889614926110049600513064513212044..165077665185696621985649682213798088050
2025-12-07 12:44:43.555Z debug(journal): 2: set_header_as_dirty: op=40 checksum=165077665185696621985649682213798088050
2025-12-07 12:44:43.555Z debug(replica): 2n: append: appending to journal op=40
2025-12-07 12:44:43.555Z debug(journal): 2: write: view=1 slot=40 op=40 len=1216: 165077665185696621985649682213798088050 starting
2025-12-07 12:44:43.555Z debug(replica): 0n: sending request_prepare to replica 2: vsr.message_header.Header.RequestPrepare{ .checksum = 48443765190173170555198295292827068129, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.request_prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .prepare_checksum = 0, .prepare_checksum_padding = 0, .prepare_op = 38, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.555Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 locked
2025-12-07 12:44:43.555Z debug(replica): 2n: commit_start_journal: cached prepare op=39 checksum=209184889614926110049600513064513212044
2025-12-07 12:44:43.555Z debug(replica): 0n: repair_prepare: op=40 checksum=165077665185696621985649682213798088050 (already writing)
2025-12-07 12:44:43.555Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 unlocked
2025-12-07 12:44:43.555Z debug(journal): 1: write_header: op=40 sectors[8192..12288]
2025-12-07 12:44:43.555Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.556Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 unlocked
2025-12-07 12:44:43.556Z debug(journal): 0: write_header: op=40 sectors[8192..12288]
2025-12-07 12:44:43.556Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.556Z debug(replica): 2n: repair_prepare: op=40 checksum=165077665185696621985649682213798088050 (already writing)
2025-12-07 12:44:43.556Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.556Z debug(journal): 0: write: view=1 slot=40 op=40 len=1216: 165077665185696621985649682213798088050 complete, marking clean
2025-12-07 12:44:43.556Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.556Z debug(replica): 0n: send_prepare_ok: op=40 checksum=165077665185696621985649682213798088050
2025-12-07 12:44:43.556Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=38)
2025-12-07 12:44:43.556Z debug(journal): 1: write: view=1 slot=40 op=40 len=1216: 165077665185696621985649682213798088050 complete, marking clean
2025-12-07 12:44:43.556Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 27746292804270390994581388417387466204, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 209184889614926110049600513064513212044, .parent_padding = 0, .prepare_checksum = 165077665185696621985649682213798088050, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit_min = 37, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.556Z debug(replica): 1N: send_prepare_ok: op=40 checksum=165077665185696621985649682213798088050
2025-12-07 12:44:43.556Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 169465922565179533303375673145539589470, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 209184889614926110049600513064513212044, .parent_padding = 0, .prepare_checksum = 165077665185696621985649682213798088050, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit_min = 39, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.556Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 169465922565179533303375673145539589470, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 209184889614926110049600513064513212044, .parent_padding = 0, .prepare_checksum = 165077665185696621985649682213798088050, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit_min = 39, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.556Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:43.556Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-07 12:44:43.556Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-07 12:44:43.556Z debug(replica): 2n: execute_op: executing view=1 primary=false op=39 checksum=209184889614926110049600513064513212044 (create_transfers)
2025-12-07 12:44:43.556Z debug(replica): 2n: execute_op: commit_timestamp=1765111483089649924 prepare.header.timestamp=1765111483097255412
2025-12-07 12:44:43.557Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:43.557Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:43.557Z debug(vsr): 0: journal_repair_timeout fired
2025-12-07 12:44:43.557Z debug(vsr): 0: journal_repair_timeout reset
2025-12-07 12:44:43.557Z debug(replica): 0n: repair: break: view=1 break=38..38 (commit=37..39 op=40)
2025-12-07 12:44:43.557Z debug(replica): 0n: sending request_headers to replica 2: vsr.message_header.Header.RequestHeaders{ .checksum = 224804965419434535459951928278972405121, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.0, .protocol = 0, .command = vsr.Command.request_headers, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .op_min = 38, .op_max = 38, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.557Z debug(replica): 0n: repair_prepare: op=38 checksum=0 replica=1 latency=92ms (committed, not present, normal)
2025-12-07 12:44:43.557Z debug(replica): 0n: sending request_prepare to replica 1: vsr.message_header.Header.RequestPrepare{ .checksum = 48443765190173170555198295292827068129, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.request_prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .prepare_checksum = 0, .prepare_checksum_padding = 0, .prepare_op = 38, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.557Z debug(replica): 2n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=37
2025-12-07 12:44:43.557Z debug(forest): entering forest.compact() op=39 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.559Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.RequestHeaders{ .checksum = 224804965419434535459951928278972405121, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.0, .protocol = 0, .command = vsr.Command.request_headers, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .op_min = 38, .op_max = 38, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.559Z debug(journal): 2: copy_latest_headers_between: op_min=38 op_max=38 dest.len=1 copied=1
2025-12-07 12:44:43.559Z debug(replica): 2n: sending headers to replica 0: vsr.message_header.Header.Headers{ .checksum = 31702815169021827706546681324239596578, .checksum_padding = 0, .checksum_body = 317629097082513684800676262295825873331, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.headers, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.559Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.RequestPrepare{ .checksum = 48443765190173170555198295292827068129, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.request_prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .prepare_checksum = 0, .prepare_checksum_padding = 0, .prepare_op = 38, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.559Z debug(replica): 2n: on_request_prepare: op=38 checksum=338466082336437125028226566829440593561 reply from pipeline
2025-12-07 12:44:43.559Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Headers{ .checksum = 31702815169021827706546681324239596578, .checksum_padding = 0, .checksum_body = 317629097082513684800676262295825873331, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.headers, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.559Z debug(replica): 2n: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 338466082336437125028226566829440593561, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .request_checksum = 332171652621535710428799087491044152427, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit = 37, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.559Z debug(replica): 0n: repair_header: op=38 checksum=338466082336437125028226566829440593561 (gap)
2025-12-07 12:44:43.559Z debug(journal): 0: set_header_as_dirty: op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.559Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=41943040 len=4096 unlocked
2025-12-07 12:44:43.559Z debug(journal): 2: write_header: op=40 sectors[8192..12288]
2025-12-07 12:44:43.559Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.559Z debug(client_replies): 2: write_reply: wrote (client=308518150047980233983200754653755704418 request=37)
2025-12-07 12:44:43.559Z debug(vsr): 2: journal_repair_timeout fired
2025-12-07 12:44:43.559Z debug(vsr): 2: journal_repair_timeout reset
2025-12-07 12:44:43.559Z info(journal): 0: read_prepare: op=38 checksum=338466082336437125028226566829440593561: no matching prepare
2025-12-07 12:44:43.559Z debug(replica): 0n: commit_start_journal_callback: prepare == null
2025-12-07 12:44:43.559Z debug(replica): 2n: repair_prepare: op=40 checksum=165077665185696621985649682213798088050 (already writing)
2025-12-07 12:44:43.559Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 338466082336437125028226566829440593561, .checksum_padding = 0, .checksum_body = 127247978300680311275717352710447464924, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1216, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .request_checksum = 332171652621535710428799087491044152427, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit = 37, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.559Z debug(replica): 0n: on_prepare: ignoring (repair)
2025-12-07 12:44:43.559Z debug(replica): 0n: repair_header: op=38 checksum=338466082336437125028226566829440593561 (checksum dirty)
2025-12-07 12:44:43.559Z debug(journal): 0: set_header_as_dirty: op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.559Z debug(journal): 0: write: view=1 slot=38 op=38 len=1216: 338466082336437125028226566829440593561 starting
2025-12-07 12:44:43.559Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=39845888 len=4096 locked
2025-12-07 12:44:43.559Z debug(replica): 0n: on_repair: repairing journal op=38
2025-12-07 12:44:43.560Z debug(replica): 0n: commit_start_journal: cached prepare op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.559Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.RequestHeaders{ .checksum = 224804965419434535459951928278972405121, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 0, .release = 0.0.0, .protocol = 0, .command = vsr.Command.request_headers, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .op_min = 38, .op_max = 38, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.560Z debug(journal): 2: copy_latest_headers_between: op_min=38 op_max=38 dest.len=1 copied=1
2025-12-07 12:44:43.560Z debug(replica): 2n: sending headers to replica 0: vsr.message_header.Header.Headers{ .checksum = 31702815169021827706546681324239596578, .checksum_padding = 0, .checksum_body = 317629097082513684800676262295825873331, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.headers, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.560Z debug(replica): 0n: repair_prepare: op=38 checksum=338466082336437125028226566829440593561 (already writing)
2025-12-07 12:44:43.560Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.560Z debug(journal): 2: write: view=1 slot=40 op=40 len=1216: 165077665185696621985649682213798088050 complete, marking clean
2025-12-07 12:44:43.560Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=37)
2025-12-07 12:44:43.560Z debug(replica): 2n: send_prepare_ok: op=40 checksum=165077665185696621985649682213798088050
2025-12-07 12:44:43.560Z debug(replica): 0n: execute_op: executing view=1 primary=false op=38 checksum=338466082336437125028226566829440593561 (lookup_accounts)
2025-12-07 12:44:43.560Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 182426312981379852579701366575433128540, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 209184889614926110049600513064513212044, .parent_padding = 0, .prepare_checksum = 165077665185696621985649682213798088050, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit_min = 39, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.560Z debug(replica): 0n: execute_op: commit_timestamp=1765111482498703643 prepare.header.timestamp=1765111483089649924
2025-12-07 12:44:43.560Z debug(replica): 0n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=36
2025-12-07 12:44:43.560Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 182426312981379852579701366575433128540, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 209184889614926110049600513064513212044, .parent_padding = 0, .prepare_checksum = 165077665185696621985649682213798088050, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit_min = 39, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.560Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:43.560Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 127389848716175152096900763629003123837, .checksum_padding = 0, .checksum_body = 135507521567594955949836790787384546610, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 332171652621535710428799087491044152427, .request_checksum_padding = 0, .context = 245562320400712436195115824806322763527, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit = 38, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.560Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-12-07 12:44:43.560Z debug(replica): 1N: on_prepare_ok: quorum received, context=165077665185696621985649682213798088050
2025-12-07 12:44:43.560Z debug(vsr): 1: prepare_timeout stopped
2025-12-07 12:44:43.560Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-12-07 12:44:43.560Z debug(replica): 0n: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 127389848716175152096900763629003123837, .checksum_padding = 0, .checksum_body = 135507521567594955949836790787384546610, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 332171652621535710428799087491044152427, .request_checksum_padding = 0, .context = 245562320400712436195115824806322763527, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit = 38, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.560Z debug(forest): entering forest.compact() op=38 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.560Z debug(replica): 1N: execute_op: executing view=1 primary=true op=40 checksum=165077665185696621985649682213798088050 (lookup_accounts)
2025-12-07 12:44:43.560Z debug(replica): 1N: execute_op: commit_timestamp=1765111483097255412 prepare.header.timestamp=1765111483555140259
2025-12-07 12:44:43.560Z debug(replica): 1N: execute_op: advancing commit_max=39..40
2025-12-07 12:44:43.560Z debug(replica): 1N: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=38
2025-12-07 12:44:43.560Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 227665858549559138844585523912079017089, .checksum_padding = 0, .checksum_body = 189095072987502742472058629479738211346, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 115896615589171071381275115656110708871, .request_checksum_padding = 0, .context = 214488337369804816506518267774916516491, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit = 40, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.560Z debug(replica): 1N: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 227665858549559138844585523912079017089, .checksum_padding = 0, .checksum_body = 189095072987502742472058629479738211346, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 115896615589171071381275115656110708871, .request_checksum_padding = 0, .context = 214488337369804816506518267774916516491, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit = 40, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.560Z debug(forest): entering forest.compact() op=40 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.560Z debug(replica): 0n: commit_start_journal: cached prepare op=39 checksum=209184889614926110049600513064513212044
2025-12-07 12:44:43.560Z info(workload): accounts created = 59, transfers = 2916, pending transfers = 0, commands run = 19
2025-12-07 12:44:43.560Z debug(replica): 0n: execute_op: executing view=1 primary=false op=39 checksum=209184889614926110049600513064513212044 (create_transfers)
2025-12-07 12:44:43.560Z debug(replica): 0n: execute_op: commit_timestamp=1765111483089649924 prepare.header.timestamp=1765111483097255412
2025-12-07 12:44:43.560Z debug(client_replies): 1: write_reply: wrote (client=308518150047980233983200754653755704418 request=38)
2025-12-07 12:44:43.561Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 144682003518333206317566273969009573192, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 214488337369804816506518267774916516491, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 7263314, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.561Z debug(replica): 1N: on_request: new request
2025-12-07 12:44:43.561Z debug(replica): 1N: primary_pipeline_prepare: request checksum=144682003518333206317566273969009573192 client=308518150047980233983200754653755704418
2025-12-07 12:44:43.561Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=152263654467012904408604332125766854297 op=41
2025-12-07 12:44:43.561Z debug(vsr): 1: prepare_timeout started
2025-12-07 12:44:43.561Z debug(vsr): 1: primary_abdicate_timeout started
2025-12-07 12:44:43.561Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:43.561Z debug(replica): 1N: replicate: replicating op=41 to replica 0
2025-12-07 12:44:43.561Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 152263654467012904408604332125766854297, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .request_checksum = 144682003518333206317566273969009573192, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.561Z debug(replica): 1N: replicate: replicating op=41 to replica 2
2025-12-07 12:44:43.561Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 152263654467012904408604332125766854297, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .request_checksum = 144682003518333206317566273969009573192, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.561Z debug(replica): 1N: on_prepare: advancing: op=40..41 checksum=165077665185696621985649682213798088050..152263654467012904408604332125766854297
2025-12-07 12:44:43.561Z debug(journal): 1: set_header_as_dirty: op=41 checksum=152263654467012904408604332125766854297
2025-12-07 12:44:43.561Z debug(replica): 1N: append: appending to journal op=41
2025-12-07 12:44:43.561Z debug(journal): 1: write: view=1 slot=41 op=41 len=6400: 152263654467012904408604332125766854297 starting
2025-12-07 12:44:43.561Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=8192 locked
2025-12-07 12:44:43.561Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=8192 unlocked
2025-12-07 12:44:43.561Z debug(journal): 1: write_header: op=41 sectors[8192..12288]
2025-12-07 12:44:43.561Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.561Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.561Z debug(journal): 1: write: view=1 slot=41 op=41 len=6400: 152263654467012904408604332125766854297 complete, marking clean
2025-12-07 12:44:43.561Z debug(replica): 1N: send_prepare_ok: op=41 checksum=152263654467012904408604332125766854297
2025-12-07 12:44:43.561Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 261543850382694320580731487698159386923, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .prepare_checksum = 152263654467012904408604332125766854297, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit_min = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.561Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 261543850382694320580731487698159386923, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .prepare_checksum = 152263654467012904408604332125766854297, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit_min = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.561Z debug(vsr): 1: primary_abdicate_timeout reset
2025-12-07 12:44:43.561Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-12-07 12:44:43.561Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-12-07 12:44:43.562Z debug(replica): 0n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=37
2025-12-07 12:44:43.562Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 84809199215624751237545935048658185206, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 261306942258224040353065558433072861127, .request_checksum_padding = 0, .context = 4705306448028604170785792023902478197, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit = 39, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.562Z debug(replica): 0n: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 84809199215624751237545935048658185206, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 261306942258224040353065558433072861127, .request_checksum_padding = 0, .context = 4705306448028604170785792023902478197, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 39, .commit = 39, .timestamp = 1765111483097255412, .request = 37, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.562Z debug(forest): entering forest.compact() op=39 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.563Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 144682003518333206317566273969009573192, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 214488337369804816506518267774916516491, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 7263314, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.564Z debug(replica): 0n: on_request: forwarding new request to primary (view=1)
2025-12-07 12:44:43.564Z debug(replica): 0n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 144682003518333206317566273969009573192, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 214488337369804816506518267774916516491, .parent_padding = 0, .client = 308518150047980233983200754653755704418, .session = 2, .timestamp = 0, .request = 39, .operation = vsr.Operation(138), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 7263314, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.564Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 152263654467012904408604332125766854297, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .request_checksum = 144682003518333206317566273969009573192, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.564Z debug(replica): 0n: on_prepare: advancing commit_max=39..40
2025-12-07 12:44:43.564Z debug(replica): 0n: on_prepare: caching prepare.op=41 (commit_min=39 op=40 commit_max=40 prepare_max=1007)
2025-12-07 12:44:43.564Z debug(replica): 0n: on_prepare: advancing: op=40..41 checksum=165077665185696621985649682213798088050..152263654467012904408604332125766854297
2025-12-07 12:44:43.564Z debug(journal): 0: set_header_as_dirty: op=41 checksum=152263654467012904408604332125766854297
2025-12-07 12:44:43.564Z debug(replica): 0n: append: appending to journal op=41
2025-12-07 12:44:43.564Z debug(journal): 0: write: view=1 slot=41 op=41 len=6400: 152263654467012904408604332125766854297 starting
2025-12-07 12:44:43.564Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=8192 locked
2025-12-07 12:44:43.564Z debug(replica): 0n: commit_start_journal: cached prepare op=40 checksum=165077665185696621985649682213798088050
2025-12-07 12:44:43.564Z debug(replica): 0n: repair_prepare: op=41 checksum=152263654467012904408604332125766854297 (already writing)
2025-12-07 12:44:43.564Z debug(replica): 0n: repair_prepare: op=38 checksum=338466082336437125028226566829440593561 (already writing)
2025-12-07 12:44:43.564Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=39)
2025-12-07 12:44:43.564Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Headers{ .checksum = 31702815169021827706546681324239596578, .checksum_padding = 0, .checksum_body = 317629097082513684800676262295825873331, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.headers, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.564Z debug(replica): 0n: repair_header: op=38 checksum=338466082336437125028226566829440593561 (checksum dirty)
2025-12-07 12:44:43.564Z debug(journal): 0: set_header_as_dirty: op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.564Z debug(replica): 0n: repair_prepare: op=41 checksum=152263654467012904408604332125766854297 (already writing)
2025-12-07 12:44:43.564Z debug(replica): 0n: repair_prepare: op=38 checksum=338466082336437125028226566829440593561 (already writing)
2025-12-07 12:44:43.564Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=39)
2025-12-07 12:44:43.564Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=39845888 len=4096 unlocked
2025-12-07 12:44:43.564Z debug(journal): 0: write_header: op=38 sectors[8192..12288]
2025-12-07 12:44:43.564Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.564Z debug(client_replies): 0: write_reply: wrote (client=308518150047980233983200754653755704418 request=36)
2025-12-07 12:44:43.564Z debug(replica): 0n: execute_op: executing view=1 primary=false op=40 checksum=165077665185696621985649682213798088050 (lookup_accounts)
2025-12-07 12:44:43.564Z debug(replica): 0n: execute_op: commit_timestamp=1765111483097255412 prepare.header.timestamp=1765111483555140259
2025-12-07 12:44:43.564Z debug(replica): 0n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=38
2025-12-07 12:44:43.564Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 227665858549559138844585523912079017089, .checksum_padding = 0, .checksum_body = 189095072987502742472058629479738211346, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 115896615589171071381275115656110708871, .request_checksum_padding = 0, .context = 214488337369804816506518267774916516491, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit = 40, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.564Z debug(replica): 0n: sending reply to client 308518150047980233983200754653755704418: vsr.message_header.Header.Reply{ .checksum = 227665858549559138844585523912079017089, .checksum_padding = 0, .checksum_body = 189095072987502742472058629479738211346, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 7936, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 115896615589171071381275115656110708871, .request_checksum_padding = 0, .context = 214488337369804816506518267774916516491, .context_padding = 0, .client = 308518150047980233983200754653755704418, .op = 40, .commit = 40, .timestamp = 1765111483555140259, .request = 38, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:43.564Z debug(forest): entering forest.compact() op=40 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:43.565Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=8192 unlocked
2025-12-07 12:44:43.565Z debug(journal): 0: write_header: op=41 sectors[8192..12288]
2025-12-07 12:44:43.565Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.565Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:43.565Z debug(journal): 0: write: view=1 slot=38 op=38 len=1216: 338466082336437125028226566829440593561 complete, marking clean
2025-12-07 12:44:43.565Z debug(replica): 0n: send_prepare_ok: op=38 checksum=338466082336437125028226566829440593561
2025-12-07 12:44:43.565Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 87970779746109025022906418958303260109, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 256040641953740902299633398237491859755, .parent_padding = 0, .prepare_checksum = 338466082336437125028226566829440593561, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 38, .commit_min = 40, .timestamp = 1765111483089649924, .request = 36, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.565Z debug(client_replies): 0: write_reply: wrote (client=308518150047980233983200754653755704418 request=37)
2025-12-07 12:44:43.565Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:43.565Z debug(journal): 0: write: view=1 slot=41 op=41 len=6400: 152263654467012904408604332125766854297 complete, marking clean
2025-12-07 12:44:43.565Z debug(replica): 0n: send_prepare_ok: op=41 checksum=152263654467012904408604332125766854297
2025-12-07 12:44:43.565Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 48854683770890466566240415544615050578, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .prepare_checksum = 152263654467012904408604332125766854297, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit_min = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.575Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:43.565Z debug(client_replies): 0: write_reply: wrote (client=308518150047980233983200754653755704418 request=38)
2025-12-07 12:44:43.569Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 152263654467012904408604332125766854297, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .request_checksum = 144682003518333206317566273969009573192, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:43.575Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.023Z info(supervisor): injecting network delays: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 415, .jitter_ms = 50 }, .lose = 3/100, .corrupt = null }
2025-12-07 12:44:44.171Z debug(replica): 2n: on_prepare: advancing commit_max=39..40
2025-12-07 12:44:44.172Z debug(replica): 2n: on_prepare: caching prepare.op=41 (commit_min=39 op=40 commit_max=40 prepare_max=1007)
2025-12-07 12:44:44.172Z debug(replica): 2n: on_prepare: advancing: op=40..41 checksum=165077665185696621985649682213798088050..152263654467012904408604332125766854297
2025-12-07 12:44:44.172Z debug(journal): 2: set_header_as_dirty: op=41 checksum=152263654467012904408604332125766854297
2025-12-07 12:44:44.172Z debug(replica): 2n: append: appending to journal op=41
2025-12-07 12:44:44.172Z debug(journal): 2: write: view=1 slot=41 op=41 len=6400: 152263654467012904408604332125766854297 starting
2025-12-07 12:44:44.172Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=8192 locked
2025-12-07 12:44:44.172Z debug(replica): 2n: commit_start_journal: cached prepare op=40 checksum=165077665185696621985649682213798088050
2025-12-07 12:44:44.172Z debug(replica): 2n: repair_prepare: op=41 checksum=152263654467012904408604332125766854297 (already writing)
2025-12-07 12:44:44.172Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=39)
2025-12-07 12:44:44.172Z debug(replica): 2n: execute_op: executing view=1 primary=false op=40 checksum=165077665185696621985649682213798088050 (lookup_accounts)
2025-12-07 12:44:44.172Z debug(replica): 2n: execute_op: commit_timestamp=1765111483097255412 prepare.header.timestamp=1765111483555140259
2025-12-07 12:44:44.172Z debug(replica): 2n: client_table_entry_update: client=308518150047980233983200754653755704418 session=2 request=38
2025-12-07 12:44:44.172Z debug(forest): entering forest.compact() op=40 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-12-07 12:44:44.177Z debug(client_replies): 2: write_reply: wrote (client=308518150047980233983200754653755704418 request=38)
2025-12-07 12:44:44.177Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-12-07 12:44:44.177Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-12-07 12:44:44.177Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=42991616 len=8192 unlocked
2025-12-07 12:44:44.177Z debug(journal): 2: write_header: op=41 sectors[8192..12288]
2025-12-07 12:44:44.177Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 locked
2025-12-07 12:44:44.177Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=8192 len=4096 unlocked
2025-12-07 12:44:44.177Z debug(journal): 2: write: view=1 slot=41 op=41 len=6400: 152263654467012904408604332125766854297 complete, marking clean
2025-12-07 12:44:44.177Z debug(replica): 2n: send_prepare_ok: op=41 checksum=152263654467012904408604332125766854297
2025-12-07 12:44:44.177Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 139495585022063710154608722784604832160, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .prepare_checksum = 152263654467012904408604332125766854297, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit_min = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:44.179Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.179Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.181Z info(supervisor): 2: terminating replica
2025-12-07 12:44:44.191Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.191Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.191Z debug(vsr): 1: journal_repair_timeout fired
2025-12-07 12:44:44.191Z debug(vsr): 1: journal_repair_timeout reset
2025-12-07 12:44:44.199Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.199Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.199Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-12-07 12:44:44.209Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=93ms
2025-12-07 12:44:44.210Z info(supervisor): 2: starting replica
2025-12-07 12:44:44.211Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.211Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.212Z info(io): opening "0_2.tigerbeetle"...
2025-12-07 12:44:44.219Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.219Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.220Z info(supervisor): injecting network corruption: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 415, .jitter_ms = 50 }, .lose = 3/100, .corrupt = 1/100 }
2025-12-07 12:44:44.230Z info(supervisor): 0: pausing replica
2025-12-07 12:44:44.231Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.231Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.240Z info(supervisor): 0: unpausing replica
2025-12-07 12:44:44.240Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.241Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.251Z info(supervisor): healing network
2025-12-07 12:44:44.251Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.251Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.251Z debug(vsr): 1: pulse_timeout fired
2025-12-07 12:44:44.251Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:44.252Z info(main): multiversioning: upgrades disabled for development (0.0.1) release.
2025-12-07 12:44:44.252Z info(main): release=0.0.1
2025-12-07 12:44:44.252Z info(main): release_client_min=0.0.1
2025-12-07 12:44:44.252Z info(main): releases_bundled={ 0.0.1 }
2025-12-07 12:44:44.252Z info(main): git_commit=cb2d50e177764f6bbefcd20e02a39a069dab4a7d
2025-12-07 12:44:44.252Z debug(superblock): null: open: started
2025-12-07 12:44:44.252Z debug(superblock): null: open: read_header: copy=0 size=8192 offset=0
2025-12-07 12:44:44.252Z debug(superblock): null: open: read_header: copy=1 size=8192 offset=24576
2025-12-07 12:44:44.252Z debug(superblock): null: open: read_header: copy=2 size=8192 offset=49152
2025-12-07 12:44:44.252Z debug(superblock): null: open: read_header: copy=3 size=8192 offset=73728
2025-12-07 12:44:44.252Z debug(superblock_quorums): copy: 0/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-07 12:44:44.252Z debug(superblock_quorums): copy: 1/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-07 12:44:44.252Z debug(superblock_quorums): copy: 2/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-07 12:44:44.253Z debug(superblock_quorums): copy: 3/4: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3
2025-12-07 12:44:44.253Z debug(superblock_quorums): quorum: checksum=7b32c8d46df0002da29939ed96939def parent=645496bdd28292132a94347ec0c43410 sequence=3 count=4 valid=true
2025-12-07 12:44:44.253Z debug(superblock): null: open: installed working superblock: checksum=7b32c8d46df0002da29939ed96939def sequence=3 release=0.0.1 cluster=00000000000000000000000000000000 replica_id=134075807420264837279280775697797238201 size=1141374976 free_set_blocks_acquired_size=0 free_set_blocks_released_size=0 client_sessions_size=0 checkpoint_id=f222e9ce156b309eaeb4af665242ac18 commit_min_checksum=108034676951432761169128540124443993015 commit_min=0 commit_max=0 log_view=1 view=1 sync_op_min=0 sync_op_max=0 manifest_oldest_checksum=0 manifest_oldest_address=0 manifest_newest_checksum=0 manifest_newest_address=0 manifest_block_count=0 snapshots_block_checksum=0 snapshots_block_address=0
2025-12-07 12:44:44.253Z debug(superblock): null: open: vsr_header: op=0 checksum=108034676951432761169128540124443993015
2025-12-07 12:44:44.253Z debug(superblock): null: open: complete
2025-12-07 12:44:44.253Z debug(journal): 2: slot_count=1024 size=1.000244140625GiB headers_size=256KiB prepares_size=1GiB
2025-12-07 12:44:44.261Z info(supervisor): 1: pausing replica
2025-12-07 12:44:44.261Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.261Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.261Z debug(vsr): 0: journal_repair_timeout fired
2025-12-07 12:44:44.261Z debug(vsr): 0: journal_repair_timeout reset
2025-12-07 12:44:44.271Z info(supervisor): 1: unpausing replica
2025-12-07 12:44:44.281Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.281Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.281Z info(supervisor): sleeping for 5.693s
2025-12-07 12:44:44.281Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.281Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.285Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-12-07 12:44:44.301Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.301Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.301Z debug(vsr): 1: journal_repair_timeout fired
2025-12-07 12:44:44.301Z debug(vsr): 1: journal_repair_timeout reset
2025-12-07 12:44:44.301Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.301Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.302Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-12-07 12:44:44.302Z info(message_bus): 0: on_connect: connected to=2
2025-12-07 12:44:44.303Z warning(faulty_network): connect failed (2,6): error.ConnectionRefused
2025-12-07 12:44:44.303Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-12-07 12:44:44.311Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=65ms
2025-12-07 12:44:44.321Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.321Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.321Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.321Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.341Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.341Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.341Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.341Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.353Z warning(faulty_network): connect failed (2,7): error.ConnectionRefused
2025-12-07 12:44:44.361Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.361Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.361Z debug(vsr): 1: pulse_timeout fired
2025-12-07 12:44:44.361Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:44.361Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.361Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.361Z debug(vsr): 0: journal_repair_timeout fired
2025-12-07 12:44:44.361Z debug(vsr): 0: journal_repair_timeout reset
2025-12-07 12:44:44.376Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-12-07 12:44:44.376Z info(message_bus): 0: on_connect: connected to=2
2025-12-07 12:44:44.376Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-12-07 12:44:44.376Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-12-07 12:44:44.381Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.381Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.382Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=53ms
2025-12-07 12:44:44.382Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.382Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.387Z debug(manifest_log): 2: Manifest.Pace.half_bar_append_blocks_max = 1
2025-12-07 12:44:44.387Z debug(manifest_log): 2: Manifest.Pace.half_bar_compact_blocks_max = 2
2025-12-07 12:44:44.387Z debug(manifest_log): 2: Manifest.Pace.log_blocks_full_max = 586
2025-12-07 12:44:44.387Z debug(manifest_log): 2: Manifest.Pace.log_blocks_cycle_max = 1172
2025-12-07 12:44:44.387Z debug(manifest_log): 2: Manifest.Pace.log_blocks_max = 1466
2025-12-07 12:44:44.387Z debug(manifest_log): 2: Manifest.Pace.tables_max = 2396744
2025-12-07 12:44:44.401Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.401Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.401Z debug(vsr): 1: journal_repair_timeout fired
2025-12-07 12:44:44.401Z debug(vsr): 1: journal_repair_timeout reset
2025-12-07 12:44:44.402Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.402Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.411Z debug(vsr): 1: prepare_timeout fired
2025-12-07 12:44:44.411Z debug(vsr): 1: prepare_timeout backing off
2025-12-07 12:44:44.411Z debug(vsr): 1: prepare_timeout after=25..5 (rtt=1 min=1 max=1000 attempts=1)
2025-12-07 12:44:44.411Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-12-07 12:44:44.411Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-12-07 12:44:44.411Z debug(replica): 1N: on_prepare_timeout: replicating to replica 0
2025-12-07 12:44:44.411Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 152263654467012904408604332125766854297, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .request_checksum = 144682003518333206317566273969009573192, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:44.412Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 152263654467012904408604332125766854297, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .request_checksum = 144682003518333206317566273969009573192, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:44.412Z warning(replica): 0n: on_prepare: not replicating op=41 commit_min=40 present=true
2025-12-07 12:44:44.412Z debug(replica): 0n: on_prepare: ignoring (repair)
2025-12-07 12:44:44.412Z debug(replica): 0n: on_repair: ignoring (duplicate)
2025-12-07 12:44:44.412Z debug(replica): 0n: send_prepare_ok: op=41 checksum=152263654467012904408604332125766854297
2025-12-07 12:44:44.412Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 48854683770890466566240415544615050578, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .prepare_checksum = 152263654467012904408604332125766854297, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit_min = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:44.421Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.421Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.422Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.422Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.435Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-12-07 12:44:44.435Z info(message_bus): 0: on_connect: connected to=2
2025-12-07 12:44:44.435Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-12-07 12:44:44.435Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-12-07 12:44:44.441Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.441Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.442Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=66ms
2025-12-07 12:44:44.442Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.442Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.444Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-12-07 12:44:44.461Z debug(vsr): 1: prepare_timeout fired
2025-12-07 12:44:44.462Z debug(vsr): 1: prepare_timeout backing off
2025-12-07 12:44:44.462Z debug(vsr): 1: prepare_timeout after=5..5 (rtt=1 min=1 max=1000 attempts=2)
2025-12-07 12:44:44.462Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-12-07 12:44:44.462Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-12-07 12:44:44.462Z debug(replica): 1N: on_prepare_timeout: replicating to replica 2
2025-12-07 12:44:44.462Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 152263654467012904408604332125766854297, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .request_checksum = 144682003518333206317566273969009573192, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:44.462Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.462Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.462Z debug(vsr): 1: pulse_timeout fired
2025-12-07 12:44:44.462Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:44.462Z debug(vsr): 0: start_view_change_message_timeout fired
2025-12-07 12:44:44.462Z debug(vsr): 0: start_view_change_message_timeout reset
2025-12-07 12:44:44.462Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.462Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.462Z debug(vsr): 0: journal_repair_timeout fired
2025-12-07 12:44:44.462Z debug(vsr): 0: journal_repair_timeout reset
2025-12-07 12:44:44.462Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-12-07 12:44:44.462Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-12-07 12:44:44.482Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.482Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.482Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.482Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.502Z debug(vsr): 1: commit_message_timeout fired
2025-12-07 12:44:44.502Z debug(vsr): 1: commit_message_timeout reset
2025-12-07 12:44:44.502Z debug(replica): 1N: sending commit to replica 0: vsr.message_header.Header.Commit{ .checksum = 316500252821743204811669919578555315572, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 165077665185696621985649682213798088050, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 40, .timestamp_monotonic = 16595408171038017, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:44.502Z debug(replica): 1N: sending commit to replica 2: vsr.message_header.Header.Commit{ .checksum = 316500252821743204811669919578555315572, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 165077665185696621985649682213798088050, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 40, .timestamp_monotonic = 16595408171038017, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:44.502Z warning(faulty_network): send error (2,4): error.BrokenPipe
2025-12-07 12:44:44.502Z debug(vsr): 1: start_view_change_message_timeout fired
2025-12-07 12:44:44.502Z debug(vsr): 1: start_view_change_message_timeout reset
2025-12-07 12:44:44.502Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.502Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.502Z debug(vsr): 1: journal_repair_timeout fired
2025-12-07 12:44:44.502Z debug(vsr): 1: journal_repair_timeout reset
2025-12-07 12:44:44.502Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 316500252821743204811669919578555315572, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 165077665185696621985649682213798088050, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 40, .timestamp_monotonic = 16595408171038017, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-12-07 12:44:44.502Z debug(vsr): 0: normal_heartbeat_timeout reset
2025-12-07 12:44:44.502Z debug(replica): 0n: on_commit: checksum verified
2025-12-07 12:44:44.502Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-12-07 12:44:44.502Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-12-07 12:44:44.502Z info(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-12-07 12:44:44.502Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.502Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.507Z debug(replica): 2r: init: replica_count=3 quorum_view_change=2 quorum_replication=2 release=0.0.1
2025-12-07 12:44:44.507Z info(replica): superblock release=0.0.1
2025-12-07 12:44:44.507Z debug(journal): 2: recover: recovering
2025-12-07 12:44:44.507Z debug(journal): 2: recover_headers: offset=0 size=262144 recovering
2025-12-07 12:44:44.508Z debug(journal): 2: recover_headers: offset=0 size=262144 recovered
2025-12-07 12:44:44.508Z debug(journal): 2: recover_headers: complete
2025-12-07 12:44:44.508Z debug(journal): 2: recover_prepare: recovering slot=0
2025-12-07 12:44:44.508Z debug(journal): 2: recover_prepare: recovering slot=1
2025-12-07 12:44:44.508Z debug(journal): 2: recover_prepare: recovering slot=2
2025-12-07 12:44:44.508Z debug(journal): 2: recover_prepare: recovering slot=3
2025-12-07 12:44:44.508Z debug(journal): 2: recover_prepare: recovering slot=4
2025-12-07 12:44:44.508Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-12-07 12:44:44.508Z debug(journal): 2: recover_prepare: recovering slot=5
2025-12-07 12:44:44.508Z debug(journal): 2: recover_prepare: recovering slot=6
2025-12-07 12:44:44.508Z debug(journal): 2: recover_prepare: recovering slot=7
2025-12-07 12:44:44.508Z info(message_bus): 0: on_connect: connected to=2
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=8
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=9
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=10
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=11
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=12
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=13
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=14
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=15
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=16
2025-12-07 12:44:44.509Z debug(journal): 2: recover_prepare: recovering slot=17
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=18
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=19
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=20
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=21
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=22
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=23
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=24
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=25
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=26
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=27
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=28
2025-12-07 12:44:44.510Z debug(journal): 2: recover_prepare: recovering slot=29
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=30
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=31
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=32
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=33
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=34
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=35
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=36
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=37
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=38
2025-12-07 12:44:44.511Z debug(journal): 2: recover_prepare: recovering slot=39
2025-12-07 12:44:44.512Z debug(journal): 2: recover_prepare: recovering slot=40
2025-12-07 12:44:44.512Z debug(journal): 2: recover_prepare: recovering slot=41
2025-12-07 12:44:44.512Z debug(journal): 2: recover_prepare: recovering slot=42
2025-12-07 12:44:44.512Z debug(journal): 2: recover_prepare: recovering slot=43
2025-12-07 12:44:44.512Z debug(journal): 2: recover_prepare: recovering slot=44
2025-12-07 12:44:44.512Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=88ms
2025-12-07 12:44:44.512Z debug(vsr): 1: prepare_timeout fired
2025-12-07 12:44:44.512Z debug(vsr): 1: prepare_timeout backing off
2025-12-07 12:44:44.512Z debug(journal): 2: recover_prepare: recovering slot=45
2025-12-07 12:44:44.512Z debug(vsr): 1: prepare_timeout after=5..11 (rtt=1 min=1 max=1000 attempts=3)
2025-12-07 12:44:44.512Z debug(replica): 1N: on_prepare_timeout: waiting for replica 2
2025-12-07 12:44:44.512Z debug(replica): 1N: on_prepare_timeout: waiting for replica 0
2025-12-07 12:44:44.512Z debug(replica): 1N: on_prepare_timeout: replicating to replica 0
2025-12-07 12:44:44.512Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 152263654467012904408604332125766854297, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .request_checksum = 144682003518333206317566273969009573192, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:44.512Z debug(journal): 2: recover_prepare: recovering slot=46
2025-12-07 12:44:44.513Z debug(journal): 2: recover_prepare: recovering slot=47
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=48
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=49
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=50
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=51
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=52
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=53
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=54
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=55
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=56
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=57
2025-12-07 12:44:44.514Z debug(journal): 2: recover_prepare: recovering slot=58
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=59
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=60
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=61
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=62
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=63
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=64
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=65
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=66
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=67
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=68
2025-12-07 12:44:44.515Z debug(journal): 2: recover_prepare: recovering slot=69
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=70
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=71
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=72
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=73
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=74
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=75
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=76
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=77
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=78
2025-12-07 12:44:44.516Z debug(journal): 2: recover_prepare: recovering slot=79
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=80
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=81
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=82
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=83
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=84
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=85
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=86
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=87
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=88
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=89
2025-12-07 12:44:44.517Z debug(journal): 2: recover_prepare: recovering slot=90
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=91
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=92
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=93
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=94
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=95
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=96
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=97
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=98
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=99
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=100
2025-12-07 12:44:44.518Z debug(journal): 2: recover_prepare: recovering slot=101
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=102
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=103
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=104
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=105
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=106
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=107
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=108
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=109
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=110
2025-12-07 12:44:44.519Z debug(journal): 2: recover_prepare: recovering slot=111
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=112
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=113
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=114
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=115
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=116
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=117
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=118
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=119
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=120
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=121
2025-12-07 12:44:44.520Z debug(journal): 2: recover_prepare: recovering slot=122
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=123
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=124
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=125
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=126
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=127
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=128
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=129
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=130
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=131
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=132
2025-12-07 12:44:44.521Z debug(journal): 2: recover_prepare: recovering slot=133
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=134
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=135
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=136
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=137
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=138
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=139
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=140
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=141
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=142
2025-12-07 12:44:44.522Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:44.522Z debug(journal): 2: recover_prepare: recovering slot=143
2025-12-07 12:44:44.522Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:44.523Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.523Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=144
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=145
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=146
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=147
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=148
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=149
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=150
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=151
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=152
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=153
2025-12-07 12:44:44.523Z debug(journal): 2: recover_prepare: recovering slot=154
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=155
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=156
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=157
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=158
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=159
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=160
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=161
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=162
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=163
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=164
2025-12-07 12:44:44.524Z debug(journal): 2: recover_prepare: recovering slot=165
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=166
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=167
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=168
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=169
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=170
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=171
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=172
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=173
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=174
2025-12-07 12:44:44.525Z debug(journal): 2: recover_prepare: recovering slot=175
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=176
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=177
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=178
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=179
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=180
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=181
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=182
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=183
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=184
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=185
2025-12-07 12:44:44.526Z debug(journal): 2: recover_prepare: recovering slot=186
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=187
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=188
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=189
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=190
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=191
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=192
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=193
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=194
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=195
2025-12-07 12:44:44.527Z debug(journal): 2: recover_prepare: recovering slot=196
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=197
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=198
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=199
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=200
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=201
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=202
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=203
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=204
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=205
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=206
2025-12-07 12:44:44.528Z debug(journal): 2: recover_prepare: recovering slot=207
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=208
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=209
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=210
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=211
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=212
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=213
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=214
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=215
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=216
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=217
2025-12-07 12:44:44.529Z debug(journal): 2: recover_prepare: recovering slot=218
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=219
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=220
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=221
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=222
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=223
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=224
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=225
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=226
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=227
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=228
2025-12-07 12:44:44.530Z debug(journal): 2: recover_prepare: recovering slot=229
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=230
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=231
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=232
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=233
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=234
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=235
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=236
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=237
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=238
2025-12-07 12:44:44.531Z debug(journal): 2: recover_prepare: recovering slot=239
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=240
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=241
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=242
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=243
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=244
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=245
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=246
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=247
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=248
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=249
2025-12-07 12:44:44.532Z debug(journal): 2: recover_prepare: recovering slot=250
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=251
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=252
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=253
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=254
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=255
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=256
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=257
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=258
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=259
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=260
2025-12-07 12:44:44.533Z debug(journal): 2: recover_prepare: recovering slot=261
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=262
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=263
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=264
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=265
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=266
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=267
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=268
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=269
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=270
2025-12-07 12:44:44.534Z debug(journal): 2: recover_prepare: recovering slot=271
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=272
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=273
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=274
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=275
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=276
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=277
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=278
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=279
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=280
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=281
2025-12-07 12:44:44.535Z debug(journal): 2: recover_prepare: recovering slot=282
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=283
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=284
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=285
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=286
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=287
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=288
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=289
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=290
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=291
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=292
2025-12-07 12:44:44.536Z debug(journal): 2: recover_prepare: recovering slot=293
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=294
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=295
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=296
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=297
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=298
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=299
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=300
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=301
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=302
2025-12-07 12:44:44.537Z debug(journal): 2: recover_prepare: recovering slot=303
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=304
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=305
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=306
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=307
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=308
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=309
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=310
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=311
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=312
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=313
2025-12-07 12:44:44.538Z debug(journal): 2: recover_prepare: recovering slot=314
thread 1 panic: reached unreachable code
2025-12-07 12:44:44.539Z debug(journal): 2: recover_prepare: recovering slot=315
2025-12-07 12:44:44.543Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:44.542Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:45.005Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:45.005Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:45.005Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-12-07 12:44:45.005Z debug(journal): 2: recover_prepare: recovering slot=316
2025-12-07 12:44:45.005Z debug(replica): 0n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 152263654467012904408604332125766854297, .checksum_padding = 0, .checksum_body = 86105784186857841043536129431676857443, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 6400, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .request_checksum = 144682003518333206317566273969009573192, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
2025-12-07 12:44:45.005Z debug(journal): 2: recover_prepare: recovering slot=317
2025-12-07 12:44:45.005Z warning(replica): 0n: on_prepare: not replicating op=41 commit_min=40 present=true
2025-12-07 12:44:45.005Z debug(replica): 0n: on_prepare: ignoring (repair)
2025-12-07 12:44:45.005Z debug(replica): 0n: on_repair: ignoring (duplicate)
2025-12-07 12:44:45.005Z info(message_bus): 1: on_connect: connected to=2
2025-12-07 12:44:45.005Z debug(replica): 0n: send_prepare_ok: op=41 checksum=152263654467012904408604332125766854297
2025-12-07 12:44:45.005Z debug(journal): 2: recover_prepare: recovering slot=318
2025-12-07 12:44:45.005Z debug(replica): 0n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 48854683770890466566240415544615050578, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 165077665185696621985649682213798088050, .parent_padding = 0, .prepare_checksum = 152263654467012904408604332125766854297, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 308518150047980233983200754653755704418, .op = 41, .commit_min = 40, .timestamp = 1765111483561092893, .request = 39, .operation = vsr.Operation(138), .reserved = { 0, 0, 0 } }
/root/tigerbeetle/zig/lib/std/debug.zig2025-12-07 12:44:45.005Z debug(journal): 2: recover_prepare: recovering slot=319
:550:14: 0x126197b in assert (vortex)
    if (!ok) unreachable; // assertion failure
             ^
/root/tigerbeetle/working/release/src/testing/vortex/faulty_network.zig:88:15: 0x13ac7a7 in recv (vortex)
        assert(!pipe.recv_inflight);
              ^
/root/tigerbeetle/working/release/src/testing/vortex/faulty_network.zig:233:22: 0x13af827 in on_send (vortex)
            pipe.recv();
                     ^
2025-12-07 12:44:45.006Z debug(journal): 2: recover_prepare: recovering slot=320
2025-12-07 12:44:45.006Z debug(journal): 2: recover_prepare: recovering slot=321
/root/tigerbeetle/working/release/src/io/linux.zig:1856:25: 0x13af2bf in erased (vortex)
2025-12-07 12:44:45.006Z debug(journal): 2: recover_prepare: recovering slot=322
                callback(ctx, completion, result.*);
                        ^
2025-12-07 12:44:45.006Z debug(journal): 2: recover_prepare: recovering slot=323
/root/tigerbeetle/working/release/src/io/linux.zig:738:40: 0x1311823 in complete (vortex)
2025-12-07 12:44:45.006Z debug(journal): 2: recover_prepare: recovering slot=324
2025-12-07 12:44:45.006Z debug(journal): 2: recover_prepare: recovering slot=325
                    completion.callback(completion.context, completion, &result);
                                       ^
2025-12-07 12:44:45.006Z debug(journal): 2: recover_prepare: recovering slot=326
/root/tigerbeetle/working/release/src/io/linux.zig:194:49: 0x130fe6b in flush (vortex)
                .inactive => completion.complete(),
                                                ^
2025-12-07 12:44:45.006Z debug(journal): 2: recover_prepare: recovering slot=327
/root/tigerbeetle/working/release/src/io/linux.zig:149:27: 0x1312253 in run_for_ns (vortex)
            try self.flush(1, &timeouts, &etime);
                          ^
/root/tigerbeetle/working/release/src/testing/vortex/supervisor.zig:263:41: 0x1312c6f in run (vortex)
            try supervisor.io.run_for_ns(constants.vsr.tick_ms * std.time.ns_per_ms);
                                        ^
/root/tigerbeetle/working/release/src/testing/vortex/supervisor.zig:207:23: 0x1316dc3 in main (vortex)
    try supervisor.run();
                      ^
/root/tigerbeetle/working/release/src/vortex.zig:61:61: 0x1328a13 in main (vortex)
        .supervisor => |supervisor_args| try Supervisor.main(allocator, supervisor_args),
                                                            ^
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=328
/root/tigerbeetle/zig/lib/std/start.zig:660:37: 0x132940f in main (vortex)
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=329
            const result = root.main() catch |err| {
                                    ^
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=330
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=331
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=332
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=333
/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c:95:7: 0x1546b67 in libc_start_main_stage2 (/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c)
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=334
 exit(main(argc, argv, envp));
      ^
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=335
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=336
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=337
2025-12-07 12:44:45.007Z debug(journal): 2: recover_prepare: recovering slot=338
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=339
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=340
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=341
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=342
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=343
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=344
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=345
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=346
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=347
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=348
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=349
2025-12-07 12:44:45.008Z debug(journal): 2: recover_prepare: recovering slot=350
2025-12-07 12:44:45.009Z debug(journal): 2: recover_prepare: recovering slot=351
2025-12-07 12:44:45.009Z debug(journal): 2: recover_prepare: recovering slot=352
2025-12-07 12:44:45.009Z debug(journal): 2: recover_prepare: recovering slot=353
2025-12-07 12:44:45.009Z debug(journal): 2: recover_prepare: recovering slot=354
2025-12-07 12:44:45.009Z debug(journal): 2: recover_prepare: recovering slot=355
2025-12-07 12:44:45.009Z debug(journal): 2: recover_prepare: recovering slot=356
2025-12-07 12:44:45.009Z debug(journal): 2: recover_prepare: recovering slot=357
2025-12-07 12:44:45.009Z debug(journal): 2: recover_prepare: recovering slot=358
2025-12-07 12:44:45.009Z debug(journal): 2: recover_prepare: recovering slot=359
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=360
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=361
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=362
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=363
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=364
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=365
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=366
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=367
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=368
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=369
2025-12-07 12:44:45.010Z debug(journal): 2: recover_prepare: recovering slot=370
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=371
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=372
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=373
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=374
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=375
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=376
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=377
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=378
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=379
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=380
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=381
2025-12-07 12:44:45.011Z debug(journal): 2: recover_prepare: recovering slot=382
2025-12-07 12:44:45.012Z debug(journal): 2: recover_prepare: recovering slot=383
2025-12-07 12:44:45.012Z debug(journal): 2: recover_prepare: recovering slot=384
2025-12-07 12:44:45.012Z debug(journal): 2: recover_prepare: recovering slot=385
2025-12-07 12:44:45.012Z debug(journal): 2: recover_prepare: recovering slot=386
2025-12-07 12:44:45.012Z debug(journal): 2: recover_prepare: recovering slot=387
2025-12-07 12:44:45.012Z debug(journal): 2: recover_prepare: recovering slot=388
2025-12-07 12:44:45.012Z debug(journal): 2: recover_prepare: recovering slot=389
2025-12-07 12:44:45.012Z debug(journal): 2: recover_prepare: recovering slot=390
2025-12-07 12:44:45.012Z debug(journal): 2: recover_prepare: recovering slot=391
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=392
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=393
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=394
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=395
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=396
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=397
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=398
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=399
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=400
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=401
2025-12-07 12:44:45.013Z debug(journal): 2: recover_prepare: recovering slot=402
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=403
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=404
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=405
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=406
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=407
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=408
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=409
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=410
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=411
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=412
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=413
2025-12-07 12:44:45.014Z debug(journal): 2: recover_prepare: recovering slot=414
2025-12-07 12:44:45.015Z debug(journal): 2: recover_prepare: recovering slot=415
2025-12-07 12:44:45.015Z debug(journal): 2: recover_prepare: recovering slot=416
2025-12-07 12:44:45.015Z debug(journal): 2: recover_prepare: recovering slot=417
2025-12-07 12:44:45.015Z debug(journal): 2: recover_prepare: recovering slot=418
2025-12-07 12:44:45.015Z debug(journal): 2: recover_prepare: recovering slot=419
2025-12-07 12:44:45.015Z debug(journal): 2: recover_prepare: recovering slot=420
2025-12-07 12:44:45.015Z debug(journal): 2: recover_prepare: recovering slot=421
2025-12-07 12:44:45.015Z debug(journal): 2: recover_prepare: recovering slot=422
2025-12-07 12:44:45.015Z debug(journal): 2: recover_prepare: recovering slot=423
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=424
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=425
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=426
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=427
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=428
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=429
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=430
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=431
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=432
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=433
2025-12-07 12:44:45.016Z debug(journal): 2: recover_prepare: recovering slot=434
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=435
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=436
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=437
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=438
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=439
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=440
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=441
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=442
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=443
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=444
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=445
2025-12-07 12:44:45.017Z debug(journal): 2: recover_prepare: recovering slot=446
2025-12-07 12:44:45.018Z debug(journal): 2: recover_prepare: recovering slot=447
2025-12-07 12:44:45.018Z debug(journal): 2: recover_prepare: recovering slot=448
2025-12-07 12:44:45.018Z debug(journal): 2: recover_prepare: recovering slot=449
2025-12-07 12:44:45.018Z debug(journal): 2: recover_prepare: recovering slot=450
2025-12-07 12:44:45.018Z debug(journal): 2: recover_prepare: recovering slot=451
2025-12-07 12:44:45.018Z debug(journal): 2: recover_prepare: recovering slot=452
2025-12-07 12:44:45.018Z debug(journal): 2: recover_prepare: recovering slot=453
2025-12-07 12:44:45.018Z debug(journal): 2: recover_prepare: recovering slot=454
2025-12-07 12:44:45.018Z debug(journal): 2: recover_prepare: recovering slot=455
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=456
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=457
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=458
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=459
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=460
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=461
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=462
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=463
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=464
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=465
2025-12-07 12:44:45.019Z debug(journal): 2: recover_prepare: recovering slot=466
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=467
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=468
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=469
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=470
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=471
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=472
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=473
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=474
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=475
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=476
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=477
2025-12-07 12:44:45.020Z debug(journal): 2: recover_prepare: recovering slot=478
2025-12-07 12:44:45.021Z debug(journal): 2: recover_prepare: recovering slot=479
2025-12-07 12:44:45.021Z debug(journal): 2: recover_prepare: recovering slot=480
2025-12-07 12:44:45.021Z debug(journal): 2: recover_prepare: recovering slot=481
2025-12-07 12:44:45.021Z debug(journal): 2: recover_prepare: recovering slot=482
2025-12-07 12:44:45.021Z debug(journal): 2: recover_prepare: recovering slot=483
2025-12-07 12:44:45.021Z debug(journal): 2: recover_prepare: recovering slot=484
2025-12-07 12:44:45.021Z debug(journal): 2: recover_prepare: recovering slot=485
2025-12-07 12:44:45.021Z debug(journal): 2: recover_prepare: recovering slot=486
2025-12-07 12:44:45.021Z debug(journal): 2: recover_prepare: recovering slot=487
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=488
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=489
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=490
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=491
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=492
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=493
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=494
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=495
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=496
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=497
2025-12-07 12:44:45.022Z debug(journal): 2: recover_prepare: recovering slot=498
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=499
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=500
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=501
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=502
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=503
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=504
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=505
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=506
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=507
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=508
2025-12-07 12:44:45.023Z debug(journal): 2: recover_prepare: recovering slot=509
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=510
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=511
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=512
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=513
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=514
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=515
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=516
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=517
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=518
2025-12-07 12:44:45.024Z debug(journal): 2: recover_prepare: recovering slot=519
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=520
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=521
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=522
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=523
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=524
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=525
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=526
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=527
2025-12-07 12:44:45.025Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:45.025Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:45.025Z debug(vsr): 1: pulse_timeout fired
2025-12-07 12:44:45.025Z debug(vsr): 1: pulse_timeout reset
2025-12-07 12:44:45.025Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:45.025Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:45.025Z debug(vsr): 0: journal_repair_timeout fired
2025-12-07 12:44:45.025Z debug(vsr): 0: journal_repair_timeout reset
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=528
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=529
2025-12-07 12:44:45.025Z debug(journal): 2: recover_prepare: recovering slot=530
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=531
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=532
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=533
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=534
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=535
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=536
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=537
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=538
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=539
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=540
2025-12-07 12:44:45.026Z debug(journal): 2: recover_prepare: recovering slot=541
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=542
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=543
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=544
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=545
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=546
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=547
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=548
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=549
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=550
2025-12-07 12:44:45.027Z debug(journal): 2: recover_prepare: recovering slot=551
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=552
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=553
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=554
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=555
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=556
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=557
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=558
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=559
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=560
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=561
2025-12-07 12:44:45.028Z debug(journal): 2: recover_prepare: recovering slot=562
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=563
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=564
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=565
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=566
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=567
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=568
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=569
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=570
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=571
2025-12-07 12:44:45.029Z debug(journal): 2: recover_prepare: recovering slot=572
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=573
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=574
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=575
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=576
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=577
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=578
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=579
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=580
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=581
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=582
2025-12-07 12:44:45.030Z debug(journal): 2: recover_prepare: recovering slot=583
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=584
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=585
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=586
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=587
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=588
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=589
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=590
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=591
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=592
2025-12-07 12:44:45.031Z debug(journal): 2: recover_prepare: recovering slot=593
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=594
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=595
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=596
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=597
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=598
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=599
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=600
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=601
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=602
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=603
2025-12-07 12:44:45.032Z debug(journal): 2: recover_prepare: recovering slot=604
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=605
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=606
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=607
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=608
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=609
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=610
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=611
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=612
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=613
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=614
2025-12-07 12:44:45.033Z debug(journal): 2: recover_prepare: recovering slot=615
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=616
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=617
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=618
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=619
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=620
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=621
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=622
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=623
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=624
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=625
2025-12-07 12:44:45.034Z debug(journal): 2: recover_prepare: recovering slot=626
2025-12-07 12:44:45.035Z debug(journal): 2: recover_prepare: recovering slot=627
2025-12-07 12:44:45.035Z debug(journal): 2: recover_prepare: recovering slot=628
2025-12-07 12:44:45.035Z debug(journal): 2: recover_prepare: recovering slot=629
2025-12-07 12:44:45.035Z debug(journal): 2: recover_prepare: recovering slot=630
2025-12-07 12:44:45.035Z debug(journal): 2: recover_prepare: recovering slot=631
2025-12-07 12:44:45.035Z debug(journal): 2: recover_prepare: recovering slot=632
2025-12-07 12:44:45.035Z debug(journal): 2: recover_prepare: recovering slot=633
2025-12-07 12:44:45.035Z debug(journal): 2: recover_prepare: recovering slot=634
2025-12-07 12:44:45.035Z debug(journal): 2: recover_prepare: recovering slot=635
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=636
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=637
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=638
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=639
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=640
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=641
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=642
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=643
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=644
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=645
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=646
2025-12-07 12:44:45.036Z debug(journal): 2: recover_prepare: recovering slot=647
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=648
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=649
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=650
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=651
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=652
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=653
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=654
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=655
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=656
2025-12-07 12:44:45.037Z debug(journal): 2: recover_prepare: recovering slot=657
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=658
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=659
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=660
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=661
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=662
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=663
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=664
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=665
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=666
2025-12-07 12:44:45.038Z debug(journal): 2: recover_prepare: recovering slot=667
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=668
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=669
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=670
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=671
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=672
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=673
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=674
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=675
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=676
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=677
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=678
2025-12-07 12:44:45.039Z debug(journal): 2: recover_prepare: recovering slot=679
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=680
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=681
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=682
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=683
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=684
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=685
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=686
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=687
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=688
2025-12-07 12:44:45.040Z debug(journal): 2: recover_prepare: recovering slot=689
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=690
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=691
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=692
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=693
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=694
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=695
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=696
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=697
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=698
2025-12-07 12:44:45.041Z debug(journal): 2: recover_prepare: recovering slot=699
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=700
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=701
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=702
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=703
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=704
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=705
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=706
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=707
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=708
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=709
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=710
2025-12-07 12:44:45.042Z debug(journal): 2: recover_prepare: recovering slot=711
2025-12-07 12:44:45.043Z debug(journal): 2: recover_prepare: recovering slot=712
2025-12-07 12:44:45.043Z debug(journal): 2: recover_prepare: recovering slot=713
2025-12-07 12:44:45.043Z debug(journal): 2: recover_prepare: recovering slot=714
2025-12-07 12:44:45.043Z debug(journal): 2: recover_prepare: recovering slot=715
2025-12-07 12:44:45.043Z debug(journal): 2: recover_prepare: recovering slot=716
2025-12-07 12:44:45.043Z debug(journal): 2: recover_prepare: recovering slot=717
2025-12-07 12:44:45.043Z debug(journal): 2: recover_prepare: recovering slot=718
2025-12-07 12:44:45.043Z debug(journal): 2: recover_prepare: recovering slot=719
2025-12-07 12:44:45.043Z debug(journal): 2: recover_prepare: recovering slot=720
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=721
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=722
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=723
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=724
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=725
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=726
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=727
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=728
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=729
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=730
2025-12-07 12:44:45.044Z debug(journal): 2: recover_prepare: recovering slot=731
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=732
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=733
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=734
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=735
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=736
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=737
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=738
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=739
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=740
2025-12-07 12:44:45.045Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-12-07 12:44:45.045Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=741
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=742
2025-12-07 12:44:45.045Z debug(journal): 2: recover_prepare: recovering slot=743
2025-12-07 12:44:45.045Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-12-07 12:44:45.045Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=744
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=745
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=746
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=747
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=748
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=749
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=750
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=751
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=752
2025-12-07 12:44:45.046Z debug(journal): 2: recover_prepare: recovering slot=753
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=754
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=755
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=756
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=757
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=758
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=759
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=760
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=761
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=762
2025-12-07 12:44:45.047Z debug(journal): 2: recover_prepare: recovering slot=763
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=764
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=765
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=766
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=767
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=768
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=769
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=770
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=771
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=772
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=773
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=774
2025-12-07 12:44:45.048Z debug(journal): 2: recover_prepare: recovering slot=775
2025-12-07 12:44:45.049Z debug(journal): 2: recover_prepare: recovering slot=776
2025-12-07 12:44:45.049Z debug(journal): 2: recover_prepare: recovering slot=777
2025-12-07 12:44:45.049Z debug(journal): 2: recover_prepare: recovering slot=778
2025-12-07 12:44:45.049Z debug(journal): 2: recover_prepare: recovering slot=779
2025-12-07 12:44:45.049Z debug(journal): 2: recover_prepare: recovering slot=780
2025-12-07 12:44:45.049Z debug(journal): 2: recover_prepare: recovering slot=781
2025-12-07 12:44:45.049Z debug(journal): 2: recover_prepare: recovering slot=782
2025-12-07 12:44:45.049Z debug(journal): 2: recover_prepare: recovering slot=783
2025-12-07 12:44:45.049Z debug(journal): 2: recover_prepare: recovering slot=784
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=785
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=786
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=787
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=788
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=789
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=790
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=791
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=792
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=793
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=794
2025-12-07 12:44:45.050Z debug(journal): 2: recover_prepare: recovering slot=795
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=796
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=797
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=798
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=799
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=800
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=801
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=802
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=803
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=804
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=805
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=806
2025-12-07 12:44:45.051Z debug(journal): 2: recover_prepare: recovering slot=807
2025-12-07 12:44:45.052Z debug(journal): 2: recover_prepare: recovering slot=808
2025-12-07 12:44:45.052Z debug(journal): 2: recover_prepare: recovering slot=809
2025-12-07 12:44:45.052Z debug(journal): 2: recover_prepare: recovering slot=810
2025-12-07 12:44:45.052Z debug(journal): 2: recover_prepare: recovering slot=811
2025-12-07 12:44:45.052Z debug(journal): 2: recover_prepare: recovering slot=812
2025-12-07 12:44:45.052Z debug(journal): 2: recover_prepare: recovering slot=813
2025-12-07 12:44:45.052Z debug(journal): 2: recover_prepare: recovering slot=814
2025-12-07 12:44:45.052Z debug(journal): 2: recover_prepare: recovering slot=815
2025-12-07 12:44:45.052Z debug(journal): 2: recover_prepare: recovering slot=816
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=817
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=818
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=819
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=820
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=821
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=822
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=823
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=824
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=825
2025-12-07 12:44:45.053Z debug(journal): 2: recover_prepare: recovering slot=826
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=827
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=828
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=829
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=830
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=831
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=832
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=833
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=834
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=835
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=836
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=837
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=838
2025-12-07 12:44:45.054Z debug(journal): 2: recover_prepare: recovering slot=839
2025-12-07 12:44:45.055Z debug(journal): 2: recover_prepare: recovering slot=840
2025-12-07 12:44:45.055Z debug(journal): 2: recover_prepare: recovering slot=841
2025-12-07 12:44:45.055Z debug(journal): 2: recover_prepare: recovering slot=842
2025-12-07 12:44:45.055Z debug(journal): 2: recover_prepare: recovering slot=843
2025-12-07 12:44:45.055Z debug(journal): 2: recover_prepare: recovering slot=844
2025-12-07 12:44:45.055Z debug(journal): 2: recover_prepare: recovering slot=845
2025-12-07 12:44:45.055Z debug(journal): 2: recover_prepare: recovering slot=846
2025-12-07 12:44:45.055Z debug(journal): 2: recover_prepare: recovering slot=847
2025-12-07 12:44:45.055Z debug(journal): 2: recover_prepare: recovering slot=848
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=849
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=850
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=851
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=852
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=853
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=854
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=855
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=856
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=857
2025-12-07 12:44:45.056Z debug(journal): 2: recover_prepare: recovering slot=858
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=859
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=860
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=861
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=862
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=863
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=864
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=865
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=866
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=867
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=868
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=869
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=870
2025-12-07 12:44:45.057Z debug(journal): 2: recover_prepare: recovering slot=871
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=872
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=873
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=874
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=875
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=876
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=877
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=878
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=879
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=880
2025-12-07 12:44:45.058Z debug(journal): 2: recover_prepare: recovering slot=881
2025-12-07 12:44:45.059Z debug(journal): 2: recover_prepare: recovering slot=882
2025-12-07 12:44:45.059Z debug(journal): 2: recover_prepare: recovering slot=883
2025-12-07 12:44:45.059Z debug(journal): 2: recover_prepare: recovering slot=884
2025-12-07 12:44:45.059Z debug(journal): 2: recover_prepare: recovering slot=885
2025-12-07 12:44:45.059Z debug(journal): 2: recover_prepare: recovering slot=886
2025-12-07 12:44:45.059Z debug(journal): 2: recover_prepare: recovering slot=887
2025-12-07 12:44:45.059Z debug(journal): 2: recover_prepare: recovering slot=888
2025-12-07 12:44:45.059Z debug(journal): 2: recover_prepare: recovering slot=889
2025-12-07 12:44:45.059Z debug(journal): 2: recover_prepare: recovering slot=890
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=891
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=892
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=893
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=894
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=895
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=896
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=897
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=898
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=899
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=900
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=901
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=902
2025-12-07 12:44:45.060Z debug(journal): 2: recover_prepare: recovering slot=903
2025-12-07 12:44:45.061Z debug(journal): 2: recover_prepare: recovering slot=904
2025-12-07 12:44:45.061Z debug(journal): 2: recover_prepare: recovering slot=905
2025-12-07 12:44:45.061Z debug(journal): 2: recover_prepare: recovering slot=906
2025-12-07 12:44:45.061Z debug(journal): 2: recover_prepare: recovering slot=907
2025-12-07 12:44:45.061Z debug(journal): 2: recover_prepare: recovering slot=908
2025-12-07 12:44:45.061Z debug(journal): 2: recover_prepare: recovering slot=909
2025-12-07 12:44:45.061Z debug(journal): 2: recover_prepare: recovering slot=910
2025-12-07 12:44:45.061Z debug(journal): 2: recover_prepare: recovering slot=911
2025-12-07 12:44:45.061Z debug(journal): 2: recover_prepare: recovering slot=912
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=913
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=914
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=915
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=916
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=917
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=918
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=919
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=920
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=921
2025-12-07 12:44:45.062Z debug(journal): 2: recover_prepare: recovering slot=922
2025-12-07 12:44:45.063Z debug(journal): 2: recover_prepare: recovering slot=923
2025-12-07 12:44:45.063Z debug(journal): 2: recover_prepare: recovering slot=924
2025-12-07 12:44:45.063Z debug(journal): 2: recover_prepare: recovering slot=925
2025-12-07 12:44:45.063Z debug(journal): 2: recover_prepare: recovering slot=926
2025-12-07 12:44:45.063Z debug(journal): 2: recover_prepare: recovering slot=927
2025-12-07 12:44:45.063Z debug(journal): 2: recover_prepare: recovering slot=928
2025-12-07 12:44:45.063Z debug(journal): 2: recover_prepare: recovering slot=929
2025-12-07 12:44:45.063Z debug(journal): 2: recover_prepare: recovering slot=930
