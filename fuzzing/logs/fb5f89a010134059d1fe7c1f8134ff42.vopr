N: execute_op: replying to client: Reply{ .checksum=559e7b464b02cab0897edc56f38b740f, .checksum_body=ea896edcb0f638197c53a5b6a59e1ab2, .cluster=0, .size=264, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f1b4745855ee2d14decbc19dbf0e6c35, .context=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .op=129, .commit=129, .timestamp=1767635373322092837, .request=127, .operation=vsr.Operation(139) }
2026-01-05 17:49:35.364Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=559e7b464b02cab0897edc56f38b740f, .checksum_body=ea896edcb0f638197c53a5b6a59e1ab2, .cluster=0, .size=264, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f1b4745855ee2d14decbc19dbf0e6c35, .context=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .op=129, .commit=129, .timestamp=1767635373322092837, .request=127, .operation=vsr.Operation(139) }
2026-01-05 17:49:35.364Z debug(forest): entering forest.compact() op=129 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:35.366Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:35.366Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:35.368Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:35.368Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:35.381Z debug(client_replies): 2: write_reply: wrote (client=89448119447425700321927633112927827628 request=127)
2026-01-05 17:49:35.386Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f1b4745855ee2d14decbc19dbf0e6c35, .checksum_body=28ec7072d7d4887a2ce1c29ae7b4cb2f, .cluster=0, .size=805632, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=0826d90daf15ee810ec40be531463a0b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=127, .operation=vsr.Operation(139), .previous_request_latency=2248033848 }
2026-01-05 17:49:35.386Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:35.386Z debug(client_replies): 2: read_reply: start (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.386Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:35.386Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:35.388Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:35.388Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:35.390Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f1b4745855ee2d14decbc19dbf0e6c35, .checksum_body=28ec7072d7d4887a2ce1c29ae7b4cb2f, .cluster=0, .size=805632, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=0826d90daf15ee810ec40be531463a0b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=127, .operation=vsr.Operation(139), .previous_request_latency=2248033848 }
2026-01-05 17:49:35.390Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:35.390Z debug(client_replies): 2: read_reply: busy (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.390Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2026-01-05 17:49:35.391Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:35.391Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:35.391Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:35.395Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f1b4745855ee2d14decbc19dbf0e6c35, .checksum_body=28ec7072d7d4887a2ce1c29ae7b4cb2f, .cluster=0, .size=805632, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=0826d90daf15ee810ec40be531463a0b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=127, .operation=vsr.Operation(139), .previous_request_latency=2248033848 }
2026-01-05 17:49:35.395Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:35.395Z debug(client_replies): 2: read_reply: busy (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.395Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2026-01-05 17:49:35.395Z debug(client_replies): 2: read_reply: done (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.395Z debug(replica): 2N: on_request: repeat reply (client=89448119447425700321927633112927827628 request=127)
2026-01-05 17:49:35.395Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=559e7b464b02cab0897edc56f38b740f, .checksum_body=ea896edcb0f638197c53a5b6a59e1ab2, .cluster=0, .size=264, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f1b4745855ee2d14decbc19dbf0e6c35, .context=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .op=129, .commit=129, .timestamp=1767635373322092837, .request=127, .operation=vsr.Operation(139) }
2026-01-05 17:49:35.399Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f1b4745855ee2d14decbc19dbf0e6c35, .checksum_body=28ec7072d7d4887a2ce1c29ae7b4cb2f, .cluster=0, .size=805632, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=0826d90daf15ee810ec40be531463a0b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=127, .operation=vsr.Operation(139), .previous_request_latency=2248033848 }
2026-01-05 17:49:35.399Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:35.399Z debug(client_replies): 2: read_reply: start (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.403Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f1b4745855ee2d14decbc19dbf0e6c35, .checksum_body=28ec7072d7d4887a2ce1c29ae7b4cb2f, .cluster=0, .size=805632, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=0826d90daf15ee810ec40be531463a0b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=127, .operation=vsr.Operation(139), .previous_request_latency=2248033848 }
2026-01-05 17:49:35.403Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:35.403Z debug(client_replies): 2: read_reply: busy (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.403Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2026-01-05 17:49:35.403Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:35.403Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:35.403Z debug(replica): 2N: primary_pipeline_prepare: request checksum=d61359834690ad86dda240c877bd0dc7 client=89448119447425700321927633112927827628
2026-01-05 17:49:35.404Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=aefe2344f99c5c1bf2ea92b5146e2b8d op=130
2026-01-05 17:49:35.404Z debug(vsr): 2: prepare_timeout started
2026-01-05 17:49:35.404Z debug(vsr): 2: primary_abdicate_timeout started
2026-01-05 17:49:35.404Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:35.404Z debug(replica): 2N: replicate: replicating op=130 to replica 1
2026-01-05 17:49:35.404Z debug(replica): 2N: sending prepare to replica 1: Prepare{ .checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=42091096d67417bfd9e86e834087c921, .request_checksum=d61359834690ad86dda240c877bd0dc7, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:35.404Z debug(replica): 2N: replicate: replicating op=130 to replica 0
2026-01-05 17:49:35.404Z debug(replica): 2N: sending prepare to replica 0: Prepare{ .checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=42091096d67417bfd9e86e834087c921, .request_checksum=d61359834690ad86dda240c877bd0dc7, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:35.404Z debug(replica): 2N: on_prepare: advancing: op=129..130 checksum=42091096d67417bfd9e86e834087c921..aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:35.404Z debug(journal): 2: set_header_as_dirty: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:35.404Z debug(replica): 2N: append: appending to journal op=130
2026-01-05 17:49:35.404Z debug(journal): 2: write: view=2 slot=130 op=130 len=1008: aefe2344f99c5c1bf2ea92b5146e2b8d starting
2026-01-05 17:49:35.404Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=136314880 len=4096 locked
2026-01-05 17:49:35.404Z debug(replica): 0n: on_message: view=2 status=normal Prepare{ .checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=42091096d67417bfd9e86e834087c921, .request_checksum=d61359834690ad86dda240c877bd0dc7, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:35.404Z debug(replica): 0n: on_prepare: advancing commit_max=128..129
2026-01-05 17:49:35.404Z debug(replica): 1n: on_message: view=2 status=normal Prepare{ .checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=42091096d67417bfd9e86e834087c921, .request_checksum=d61359834690ad86dda240c877bd0dc7, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:35.404Z debug(replica): 0n: on_prepare: caching prepare.op=130 (commit_min=128 op=129 commit_max=129 prepare_max=1007)
2026-01-05 17:49:35.404Z debug(replica): 1n: on_prepare: advancing commit_max=128..129
2026-01-05 17:49:35.404Z debug(replica): 1n: on_prepare: caching prepare.op=130 (commit_min=128 op=129 commit_max=129 prepare_max=1007)
2026-01-05 17:49:35.404Z debug(replica): 0n: on_prepare: advancing: op=129..130 checksum=42091096d67417bfd9e86e834087c921..aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:35.404Z debug(journal): 0: set_header_as_dirty: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:35.404Z debug(replica): 0n: append: appending to journal op=130
2026-01-05 17:49:35.404Z debug(journal): 0: write: view=2 slot=130 op=130 len=1008: aefe2344f99c5c1bf2ea92b5146e2b8d starting
2026-01-05 17:49:35.404Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=136314880 len=4096 locked
2026-01-05 17:49:35.404Z debug(replica): 0n: commit_start_journal: cached prepare op=129 checksum=42091096d67417bfd9e86e834087c921
2026-01-05 17:49:35.404Z debug(replica): 1n: on_prepare: advancing: op=129..130 checksum=42091096d67417bfd9e86e834087c921..aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:35.404Z debug(journal): 1: set_header_as_dirty: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:35.404Z debug(replica): 1n: append: appending to journal op=130
2026-01-05 17:49:35.404Z debug(journal): 1: write: view=2 slot=130 op=130 len=1008: aefe2344f99c5c1bf2ea92b5146e2b8d starting
2026-01-05 17:49:35.404Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=136314880 len=4096 locked
2026-01-05 17:49:35.404Z debug(replica): 1n: commit_start_journal: cached prepare op=129 checksum=42091096d67417bfd9e86e834087c921
2026-01-05 17:49:35.404Z debug(replica): 0n: repair_prepare: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d (already writing)
2026-01-05 17:49:35.404Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=128)
2026-01-05 17:49:35.405Z debug(replica): 1n: repair_prepare: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d (already writing)
2026-01-05 17:49:35.405Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=128)
2026-01-05 17:49:35.406Z debug(replica): 0n: execute_op: executing view=2 primary=false op=129 checksum=42091096d67417bfd9e86e834087c921 (create_transfers)
2026-01-05 17:49:35.406Z debug(replica): 0n: execute_op: commit_timestamp=1767635371061500883 prepare.header.timestamp=1767635373322092837
2026-01-05 17:49:35.407Z debug(replica): 1n: execute_op: executing view=2 primary=false op=129 checksum=42091096d67417bfd9e86e834087c921 (create_transfers)
2026-01-05 17:49:35.407Z debug(replica): 1n: execute_op: commit_timestamp=1767635371061500883 prepare.header.timestamp=1767635373322092837
2026-01-05 17:49:35.407Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f1b4745855ee2d14decbc19dbf0e6c35, .checksum_body=28ec7072d7d4887a2ce1c29ae7b4cb2f, .cluster=0, .size=805632, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=0826d90daf15ee810ec40be531463a0b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=127, .operation=vsr.Operation(139), .previous_request_latency=2248033848 }
2026-01-05 17:49:35.407Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:35.407Z debug(client_replies): 2: read_reply: busy (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.407Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2026-01-05 17:49:35.408Z debug(client_replies): 2: read_reply: done (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.408Z debug(replica): 2N: on_request: repeat reply (client=89448119447425700321927633112927827628 request=127)
2026-01-05 17:49:35.408Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=559e7b464b02cab0897edc56f38b740f, .checksum_body=ea896edcb0f638197c53a5b6a59e1ab2, .cluster=0, .size=264, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f1b4745855ee2d14decbc19dbf0e6c35, .context=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .op=129, .commit=129, .timestamp=1767635373322092837, .request=127, .operation=vsr.Operation(139) }
2026-01-05 17:49:35.408Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=136314880 len=4096 unlocked
2026-01-05 17:49:35.408Z debug(journal): 2: write_header: op=130 sectors[32768..36864]
2026-01-05 17:49:35.408Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:35.412Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f1b4745855ee2d14decbc19dbf0e6c35, .checksum_body=28ec7072d7d4887a2ce1c29ae7b4cb2f, .cluster=0, .size=805632, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=0826d90daf15ee810ec40be531463a0b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=127, .operation=vsr.Operation(139), .previous_request_latency=2248033848 }
2026-01-05 17:49:35.412Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:35.412Z debug(client_replies): 2: read_reply: start (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.412Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:35.412Z debug(journal): 2: write: view=2 slot=130 op=130 len=1008: aefe2344f99c5c1bf2ea92b5146e2b8d complete, marking clean
2026-01-05 17:49:35.412Z debug(replica): 2N: send_prepare_ok: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:35.412Z debug(replica): 2N: sending prepare_ok to replica 2: PrepareOk{ .checksum=b44347898546e4674043a859716d7fe2, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=42091096d67417bfd9e86e834087c921, .prepare_checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit_min=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:35.412Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=b44347898546e4674043a859716d7fe2, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=42091096d67417bfd9e86e834087c921, .prepare_checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit_min=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:35.412Z debug(vsr): 2: primary_abdicate_timeout reset
2026-01-05 17:49:35.412Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2026-01-05 17:49:35.412Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2026-01-05 17:49:35.412Z debug(client_replies): 2: read_reply: done (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.412Z debug(replica): 2N: on_request: repeat reply (client=89448119447425700321927633112927827628 request=127)
2026-01-05 17:49:35.412Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=559e7b464b02cab0897edc56f38b740f, .checksum_body=ea896edcb0f638197c53a5b6a59e1ab2, .cluster=0, .size=264, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f1b4745855ee2d14decbc19dbf0e6c35, .context=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .op=129, .commit=129, .timestamp=1767635373322092837, .request=127, .operation=vsr.Operation(139) }
2026-01-05 17:49:35.416Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f1b4745855ee2d14decbc19dbf0e6c35, .checksum_body=28ec7072d7d4887a2ce1c29ae7b4cb2f, .cluster=0, .size=805632, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=0826d90daf15ee810ec40be531463a0b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=127, .operation=vsr.Operation(139), .previous_request_latency=2248033848 }
2026-01-05 17:49:35.416Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:35.416Z debug(client_replies): 2: read_reply: start (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.416Z debug(client_replies): 2: read_reply: done (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:35.416Z debug(replica): 2N: on_request: repeat reply (client=89448119447425700321927633112927827628 request=127)
2026-01-05 17:49:35.416Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=559e7b464b02cab0897edc56f38b740f, .checksum_body=ea896edcb0f638197c53a5b6a59e1ab2, .cluster=0, .size=264, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f1b4745855ee2d14decbc19dbf0e6c35, .context=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .op=129, .commit=129, .timestamp=1767635373322092837, .request=127, .operation=vsr.Operation(139) }
2026-01-05 17:49:35.427Z debug(replica): 0n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=127
2026-01-05 17:49:35.420Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f1b4745855ee2d14decbc19dbf0e6c35, .checksum_body=28ec7072d7d4887a2ce1c29ae7b4cb2f, .cluster=0, .size=805632, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=0826d90daf15ee810ec40be531463a0b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=127, .operation=vsr.Operation(139), .previous_request_latency=2248033848 }
2026-01-05 17:49:35.427Z debug(replica): 0n: execute_op: replying to client: Reply{ .checksum=559e7b464b02cab0897edc56f38b740f, .checksum_body=ea896edcb0f638197c53a5b6a59e1ab2, .cluster=0, .size=264, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f1b4745855ee2d14decbc19dbf0e6c35, .context=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .op=129, .commit=129, .timestamp=1767635373322092837, .request=127, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.073Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:37.073Z debug(client_replies): 2: read_reply: start (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:37.073Z debug(replica): 0n: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=559e7b464b02cab0897edc56f38b740f, .checksum_body=ea896edcb0f638197c53a5b6a59e1ab2, .cluster=0, .size=264, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f1b4745855ee2d14decbc19dbf0e6c35, .context=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .op=129, .commit=129, .timestamp=1767635373322092837, .request=127, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.073Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.073Z debug(forest): entering forest.compact() op=129 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:37.073Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.073Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.074Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.074Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.074Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:35.436Z debug(replica): 1n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=127
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.074Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.074Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.074Z debug(forest): entering forest.compact() op=129 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.074Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.074Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.074Z debug(client_replies): 2: read_reply: done (client=89448119447425700321927633112927827628 reply=559e7b464b02cab0897edc56f38b740f)
2026-01-05 17:49:37.074Z debug(replica): 2N: on_request: repeat reply (client=89448119447425700321927633112927827628 request=127)
2026-01-05 17:49:37.074Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=559e7b464b02cab0897edc56f38b740f, .checksum_body=ea896edcb0f638197c53a5b6a59e1ab2, .cluster=0, .size=264, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f1b4745855ee2d14decbc19dbf0e6c35, .context=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .op=129, .commit=129, .timestamp=1767635373322092837, .request=127, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.074Z debug(vsr): 2: ping_timeout fired
2026-01-05 17:49:37.074Z debug(vsr): 2: ping_timeout reset
2026-01-05 17:49:37.074Z debug(replica): 2N: sending ping to replica 0: Ping{ .checksum=987f8ab2bde76aab60a1901959e8f886, .checksum_body=74730330cb75ecd4a42c51968e55e4bd, .cluster=0, .size=512, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.ping, .replica=2, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .ping_timestamp_monotonic=39755157899751707, .release_count=1, .route=18446744073692774913 }
2026-01-05 17:49:37.074Z debug(replica): 2N: sending ping to replica 1: Ping{ .checksum=987f8ab2bde76aab60a1901959e8f886, .checksum_body=74730330cb75ecd4a42c51968e55e4bd, .cluster=0, .size=512, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.ping, .replica=2, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .ping_timestamp_monotonic=39755157899751707, .release_count=1, .route=18446744073692774913 }
2026-01-05 17:49:37.074Z debug(vsr): 2: commit_message_timeout fired
2026-01-05 17:49:37.074Z debug(vsr): 2: commit_message_timeout reset
2026-01-05 17:49:37.074Z debug(replica): 2N: sending commit to replica 0: Commit{ .checksum=39cb3538646ee6b4664f06c8a2979cc2, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.commit, .replica=2, .commit_checksum=42091096d67417bfd9e86e834087c921, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .commit=129, .timestamp_monotonic=39755157899863648 }
2026-01-05 17:49:37.074Z debug(replica): 2N: sending commit to replica 1: Commit{ .checksum=39cb3538646ee6b4664f06c8a2979cc2, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.commit, .replica=2, .commit_checksum=42091096d67417bfd9e86e834087c921, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .commit=129, .timestamp_monotonic=39755157899863648 }
2026-01-05 17:49:37.074Z debug(vsr): 2: start_view_change_message_timeout fired
2026-01-05 17:49:37.074Z debug(vsr): 2: start_view_change_message_timeout reset
2026-01-05 17:49:37.074Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:37.074Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:37.074Z debug(vsr): 2: journal_repair_timeout fired
2026-01-05 17:49:37.074Z debug(vsr): 2: journal_repair_timeout reset
2026-01-05 17:49:37.074Z debug(vsr): 2: grid_repair_budget_timeout fired
2026-01-05 17:49:37.074Z debug(vsr): 2: grid_repair_budget_timeout reset
2026-01-05 17:49:37.074Z debug(vsr): 2: upgrade_timeout fired
2026-01-05 17:49:37.074Z debug(vsr): 2: upgrade_timeout reset
2026-01-05 17:49:37.091Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:37.091Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 0n: on_message: view=2 status=normal Ping{ .checksum=987f8ab2bde76aab60a1901959e8f886, .checksum_body=74730330cb75ecd4a42c51968e55e4bd, .cluster=0, .size=512, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.ping, .replica=2, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .ping_timestamp_monotonic=39755157899751707, .release_count=1, .route=18446744073692774913 }
2026-01-05 17:49:37.091Z debug(replica): 0n: sending pong to replica 2: Pong{ .checksum=2260bfb192102b2927927439a69699fb, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.pong, .replica=0, .ping_timestamp_monotonic=39755157899751707, .pong_timestamp_wall=1767635377091258084 }
2026-01-05 17:49:37.091Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.091Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.091Z debug(replica): 0n: on_message: view=2 status=normal Commit{ .checksum=39cb3538646ee6b4664f06c8a2979cc2, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.commit, .replica=2, .commit_checksum=42091096d67417bfd9e86e834087c921, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .commit=129, .timestamp_monotonic=39755157899863648 }
2026-01-05 17:49:37.091Z debug(vsr): 0: normal_heartbeat_timeout reset
2026-01-05 17:49:37.091Z debug(replica): 0n: on_commit: checksum verified
2026-01-05 17:49:37.091Z debug(replica): 1n: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:37.091Z debug(replica): 1n: sending request to replica 2: Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=136314880 len=4096 unlocked
2026-01-05 17:49:37.091Z debug(journal): 0: write_header: op=130 sectors[32768..36864]
2026-01-05 17:49:37.091Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:37.091Z debug(client_replies): 0: write_reply: wrote (client=89448119447425700321927633112927827628 request=127)
2026-01-05 17:49:37.091Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.091Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.091Z debug(replica): 1n: on_message: view=2 status=normal Ping{ .checksum=987f8ab2bde76aab60a1901959e8f886, .checksum_body=74730330cb75ecd4a42c51968e55e4bd, .cluster=0, .size=512, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.ping, .replica=2, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .ping_timestamp_monotonic=39755157899751707, .release_count=1, .route=18446744073692774913 }
2026-01-05 17:49:37.091Z debug(replica): 1n: sending pong to replica 2: Pong{ .checksum=1b86edf427d44f5bbb31b86116c55e0c, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.pong, .replica=1, .ping_timestamp_monotonic=39755157899751707, .pong_timestamp_wall=1767635377091590597 }
2026-01-05 17:49:37.091Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:37.091Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 2N: on_message: view=2 status=normal Pong{ .checksum=1b86edf427d44f5bbb31b86116c55e0c, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.pong, .replica=1, .ping_timestamp_monotonic=39755157899751707, .pong_timestamp_wall=1767635377091590597 }
2026-01-05 17:49:37.091Z debug(clock): 2: learn: replica=1 m0=39755157899751707 t1=1767635377091590597 m2=39755157916867678 t2=1767635377091686248 one_way_delay=8557985 asymmetric_delay=0 clock_offset=8462334
2026-01-05 17:49:37.091Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:37.091Z debug(replica): 1n: on_message: view=2 status=normal Commit{ .checksum=39cb3538646ee6b4664f06c8a2979cc2, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.commit, .replica=2, .commit_checksum=42091096d67417bfd9e86e834087c921, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .commit=129, .timestamp_monotonic=39755157899863648 }
2026-01-05 17:49:37.091Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(vsr): 1: normal_heartbeat_timeout reset
2026-01-05 17:49:37.091Z debug(replica): 1n: on_commit: checksum verified
2026-01-05 17:49:37.091Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:37.091Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=136314880 len=4096 unlocked
2026-01-05 17:49:37.091Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(journal): 1: write_header: op=130 sectors[32768..36864]
2026-01-05 17:49:37.091Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:37.091Z debug(client_replies): 1: write_reply: wrote (client=89448119447425700321927633112927827628 request=127)
2026-01-05 17:49:37.091Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:37.091Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 1n: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:37.091Z debug(replica): 1n: sending request to replica 2: Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(vsr): 0: start_view_change_message_timeout fired
2026-01-05 17:49:37.091Z debug(vsr): 0: start_view_change_message_timeout reset
2026-01-05 17:49:37.091Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:37.091Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:37.091Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.091Z debug(vsr): 0: journal_repair_timeout fired
2026-01-05 17:49:37.091Z debug(vsr): 0: journal_repair_timeout reset
2026-01-05 17:49:37.091Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.092Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.092Z debug(vsr): 1: start_view_change_message_timeout fired
2026-01-05 17:49:37.092Z debug(vsr): 1: start_view_change_message_timeout reset
2026-01-05 17:49:37.092Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:37.092Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:37.092Z debug(replica): 0n: repair_prepare: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d (already writing)
2026-01-05 17:49:37.092Z debug(vsr): 1: journal_repair_timeout fired
2026-01-05 17:49:37.092Z debug(vsr): 1: journal_repair_timeout reset
2026-01-05 17:49:37.092Z debug(vsr): 0: grid_repair_budget_timeout fired
2026-01-05 17:49:37.092Z debug(vsr): 0: grid_repair_budget_timeout reset
2026-01-05 17:49:37.092Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:37.092Z debug(replica): 1n: repair_prepare: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d (already writing)
2026-01-05 17:49:37.092Z debug(journal): 0: write: view=2 slot=130 op=130 len=1008: aefe2344f99c5c1bf2ea92b5146e2b8d complete, marking clean
2026-01-05 17:49:37.092Z debug(replica): 0n: send_prepare_ok: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:37.092Z debug(replica): 0n: sending prepare_ok to replica 2: PrepareOk{ .checksum=1781b0a5dc493fe22c95b0d496ccce1d, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=42091096d67417bfd9e86e834087c921, .prepare_checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit_min=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.092Z debug(vsr): 1: grid_repair_budget_timeout fired
2026-01-05 17:49:37.092Z debug(vsr): 1: grid_repair_budget_timeout reset
2026-01-05 17:49:37.092Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:37.092Z debug(journal): 1: write: view=2 slot=130 op=130 len=1008: aefe2344f99c5c1bf2ea92b5146e2b8d complete, marking clean
2026-01-05 17:49:37.092Z debug(replica): 1n: send_prepare_ok: op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:37.092Z debug(replica): 1n: sending prepare_ok to replica 2: PrepareOk{ .checksum=498ef36406c46ff2b9851865fc503a75, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=1, .parent=42091096d67417bfd9e86e834087c921, .prepare_checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit_min=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.092Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=498ef36406c46ff2b9851865fc503a75, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=1, .parent=42091096d67417bfd9e86e834087c921, .prepare_checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit_min=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.092Z debug(vsr): 2: primary_abdicate_timeout reset
2026-01-05 17:49:37.092Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2026-01-05 17:49:37.092Z debug(replica): 2N: on_prepare_ok: quorum received, prepare_checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:37.092Z debug(vsr): 2: prepare_timeout stopped
2026-01-05 17:49:37.092Z debug(vsr): 2: primary_abdicate_timeout stopped
2026-01-05 17:49:37.092Z debug(replica): 2N: execute_op: executing view=2 primary=true op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d (lookup_accounts)
2026-01-05 17:49:37.092Z debug(replica): 2N: execute_op: commit_timestamp=1767635373322092837 prepare.header.timestamp=1767635375403979890
2026-01-05 17:49:37.092Z debug(replica): 2N: execute_op: advancing commit_max=129..130
2026-01-05 17:49:37.092Z debug(replica): 2N: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=128
2026-01-05 17:49:37.092Z debug(replica): 2N: execute_op: replying to client: Reply{ .checksum=2b64e98f3934a67f9495dc08399fd5bc, .checksum_body=c59fac8a3599e75291f3f733952a3bfe, .cluster=0, .size=6272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=d61359834690ad86dda240c877bd0dc7, .context=41653709f181abf181794298f6d98f9b, .client=89448119447425700321927633112927827628, .op=130, .commit=130, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.092Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=2b64e98f3934a67f9495dc08399fd5bc, .checksum_body=c59fac8a3599e75291f3f733952a3bfe, .cluster=0, .size=6272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=d61359834690ad86dda240c877bd0dc7, .context=41653709f181abf181794298f6d98f9b, .client=89448119447425700321927633112927827628, .op=130, .commit=130, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.092Z debug(forest): entering forest.compact() op=130 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:37.092Z info(workload): accounts created = 46, transfers = 100947, pending transfers = 0, commands run = 64
2026-01-05 17:49:37.092Z debug(client_replies): 2: write_reply: wrote (client=89448119447425700321927633112927827628 request=128)
2026-01-05 17:49:37.094Z debug(clock): 2: synchronized: truechimers=2/3 clock_offset=0ns..0ns accuracy=0ns
2026-01-05 17:49:37.094Z debug(clock): 2: system time is 150ns behind
2026-01-05 17:49:37.095Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:37.095Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:37.107Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=a922ce948239e554f1c19d8cc01f87ef, .checksum_body=a9a524525ad95fca3fc234b638b0730a, .cluster=0, .size=900352, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=41653709f181abf181794298f6d98f9b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=129, .operation=vsr.Operation(139), .previous_request_latency=1702090278 }
2026-01-05 17:49:37.107Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.107Z debug(replica): 2N: primary_pipeline_prepare: request checksum=a922ce948239e554f1c19d8cc01f87ef client=89448119447425700321927633112927827628
2026-01-05 17:49:37.107Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=a922ce948239e554f1c19d8cc01f87ef, .checksum_body=a9a524525ad95fca3fc234b638b0730a, .cluster=0, .size=900352, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=41653709f181abf181794298f6d98f9b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=129, .operation=vsr.Operation(139), .previous_request_latency=1702090278 }
2026-01-05 17:49:37.107Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:37.107Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=a922ce948239e554f1c19d8cc01f87ef, .checksum_body=a9a524525ad95fca3fc234b638b0730a, .cluster=0, .size=900352, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=41653709f181abf181794298f6d98f9b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=129, .operation=vsr.Operation(139), .previous_request_latency=1702090278 }
2026-01-05 17:49:37.111Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=8fbb5458dcf58f558a41c764b02ba5ff op=131
2026-01-05 17:49:37.111Z debug(vsr): 2: prepare_timeout started
2026-01-05 17:49:37.111Z debug(vsr): 2: primary_abdicate_timeout started
2026-01-05 17:49:37.111Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:37.111Z debug(replica): 2N: replicate: replicating op=131 to replica 1
2026-01-05 17:49:37.111Z debug(replica): 2N: sending prepare to replica 1: Prepare{ .checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checksum_body=a9a524525ad95fca3fc234b638b0730a, .cluster=0, .size=900352, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .request_checksum=a922ce948239e554f1c19d8cc01f87ef, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.111Z debug(replica): 2N: replicate: replicating op=131 to replica 0
2026-01-05 17:49:37.111Z debug(replica): 2N: sending prepare to replica 0: Prepare{ .checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checksum_body=a9a524525ad95fca3fc234b638b0730a, .cluster=0, .size=900352, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .request_checksum=a922ce948239e554f1c19d8cc01f87ef, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.112Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:37.112Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:37.112Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:37.112Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:37.112Z debug(replica): 2N: on_prepare: advancing: op=130..131 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d..8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.112Z debug(journal): 2: set_header_as_dirty: op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.112Z debug(replica): 2N: append: appending to journal op=131
2026-01-05 17:49:37.112Z debug(journal): 2: write: view=2 slot=131 op=131 len=900352: 8fbb5458dcf58f558a41c764b02ba5ff starting
2026-01-05 17:49:37.112Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=137363456 len=901120 locked
2026-01-05 17:49:37.113Z debug(replica): 2N: on_message: view=2 status=normal Pong{ .checksum=2260bfb192102b2927927439a69699fb, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.pong, .replica=0, .ping_timestamp_monotonic=39755157899751707, .pong_timestamp_wall=1767635377091258084 }
2026-01-05 17:49:37.113Z debug(clock): 2: learn: m0=39755157899751707 < window.monotonic=39755157920162573
2026-01-05 17:49:37.113Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.113Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:37.113Z debug(client_replies): 2: read_reply: start (client=89448119447425700321927633112927827628 reply=2b64e98f3934a67f9495dc08399fd5bc)
2026-01-05 17:49:37.113Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.113Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:37.113Z debug(client_replies): 2: read_reply: busy (client=89448119447425700321927633112927827628 reply=2b64e98f3934a67f9495dc08399fd5bc)
2026-01-05 17:49:37.113Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2026-01-05 17:49:37.113Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.113Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:37.113Z debug(client_replies): 2: read_reply: busy (client=89448119447425700321927633112927827628 reply=2b64e98f3934a67f9495dc08399fd5bc)
2026-01-05 17:49:37.113Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2026-01-05 17:49:37.113Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=d61359834690ad86dda240c877bd0dc7, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=bd740e4bf78ca68bf9ac83b4587bdd1b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=128, .operation=vsr.Operation(140), .previous_request_latency=2050818216 }
2026-01-05 17:49:37.113Z debug(replica): 2N: on_request: replying to duplicate request
2026-01-05 17:49:37.113Z debug(client_replies): 2: read_reply: busy (client=89448119447425700321927633112927827628 reply=2b64e98f3934a67f9495dc08399fd5bc)
2026-01-05 17:49:37.113Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2026-01-05 17:49:37.113Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=1781b0a5dc493fe22c95b0d496ccce1d, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=42091096d67417bfd9e86e834087c921, .prepare_checksum=aefe2344f99c5c1bf2ea92b5146e2b8d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=130, .commit_min=129, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.113Z debug(replica): 2N: on_prepare_ok: not preparing op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:37.113Z debug(client_replies): 2: read_reply: done (client=89448119447425700321927633112927827628 reply=2b64e98f3934a67f9495dc08399fd5bc)
2026-01-05 17:49:37.113Z debug(replica): 2N: on_request: repeat reply (client=89448119447425700321927633112927827628 request=128)
2026-01-05 17:49:37.113Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=2b64e98f3934a67f9495dc08399fd5bc, .checksum_body=c59fac8a3599e75291f3f733952a3bfe, .cluster=0, .size=6272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=d61359834690ad86dda240c877bd0dc7, .context=41653709f181abf181794298f6d98f9b, .client=89448119447425700321927633112927827628, .op=130, .commit=130, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.113Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=137363456 len=901120 unlocked
2026-01-05 17:49:37.113Z debug(journal): 2: write_header: op=131 sectors[32768..36864]
2026-01-05 17:49:37.113Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:37.114Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:37.114Z debug(journal): 2: write: view=2 slot=131 op=131 len=900352: 8fbb5458dcf58f558a41c764b02ba5ff complete, marking clean
2026-01-05 17:49:37.114Z debug(replica): 2N: send_prepare_ok: op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.114Z debug(replica): 2N: sending prepare_ok to replica 2: PrepareOk{ .checksum=88e430a1b6415fb2897670f9bbc3e83a, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .prepare_checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit_min=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.114Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=88e430a1b6415fb2897670f9bbc3e83a, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .prepare_checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit_min=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.114Z debug(vsr): 2: primary_abdicate_timeout reset
2026-01-05 17:49:37.114Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2026-01-05 17:49:37.114Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2026-01-05 17:49:37.116Z debug(replica): 1n: on_message: view=2 status=normal Prepare{ .checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checksum_body=a9a524525ad95fca3fc234b638b0730a, .cluster=0, .size=900352, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .request_checksum=a922ce948239e554f1c19d8cc01f87ef, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.116Z debug(replica): 1n: on_prepare: advancing commit_max=129..130
2026-01-05 17:49:37.116Z debug(replica): 1n: on_prepare: caching prepare.op=131 (commit_min=129 op=130 commit_max=130 prepare_max=1007)
2026-01-05 17:49:37.116Z debug(replica): 1n: on_prepare: advancing: op=130..131 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d..8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.116Z debug(journal): 1: set_header_as_dirty: op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.116Z debug(replica): 1n: append: appending to journal op=131
2026-01-05 17:49:37.116Z debug(journal): 1: write: view=2 slot=131 op=131 len=900352: 8fbb5458dcf58f558a41c764b02ba5ff starting
2026-01-05 17:49:37.116Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=137363456 len=901120 locked
2026-01-05 17:49:37.116Z debug(replica): 1n: commit_start_journal: cached prepare op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:37.117Z debug(replica): 1n: repair_prepare: op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff (already writing)
2026-01-05 17:49:37.117Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=129)
2026-01-05 17:49:37.117Z debug(replica): 1n: execute_op: executing view=2 primary=false op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d (lookup_accounts)
2026-01-05 17:49:37.117Z debug(replica): 1n: execute_op: commit_timestamp=1767635373322092837 prepare.header.timestamp=1767635375403979890
2026-01-05 17:49:37.117Z debug(replica): 1n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=128
2026-01-05 17:49:37.117Z debug(forest): entering forest.compact() op=130 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:37.117Z debug(replica): 0n: on_message: view=2 status=normal Prepare{ .checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checksum_body=a9a524525ad95fca3fc234b638b0730a, .cluster=0, .size=900352, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .request_checksum=a922ce948239e554f1c19d8cc01f87ef, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.117Z debug(replica): 0n: on_prepare: advancing commit_max=129..130
2026-01-05 17:49:37.117Z debug(replica): 0n: on_prepare: caching prepare.op=131 (commit_min=129 op=130 commit_max=130 prepare_max=1007)
2026-01-05 17:49:37.117Z debug(replica): 0n: on_prepare: advancing: op=130..131 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d..8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.117Z debug(journal): 0: set_header_as_dirty: op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.117Z debug(replica): 0n: append: appending to journal op=131
2026-01-05 17:49:37.117Z debug(journal): 0: write: view=2 slot=131 op=131 len=900352: 8fbb5458dcf58f558a41c764b02ba5ff starting
2026-01-05 17:49:37.117Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=137363456 len=901120 locked
2026-01-05 17:49:37.117Z debug(replica): 0n: commit_start_journal: cached prepare op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d
2026-01-05 17:49:37.117Z debug(replica): 0n: repair_prepare: op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff (already writing)
2026-01-05 17:49:37.117Z debug(client_replies): 1: write_reply: wrote (client=89448119447425700321927633112927827628 request=128)
2026-01-05 17:49:37.117Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=129)
2026-01-05 17:49:37.117Z debug(replica): 0n: execute_op: executing view=2 primary=false op=130 checksum=aefe2344f99c5c1bf2ea92b5146e2b8d (lookup_accounts)
2026-01-05 17:49:37.117Z debug(replica): 0n: execute_op: commit_timestamp=1767635373322092837 prepare.header.timestamp=1767635375403979890
2026-01-05 17:49:37.117Z debug(replica): 0n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=128
2026-01-05 17:49:37.117Z debug(replica): 0n: execute_op: replying to client: Reply{ .checksum=2b64e98f3934a67f9495dc08399fd5bc, .checksum_body=c59fac8a3599e75291f3f733952a3bfe, .cluster=0, .size=6272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=d61359834690ad86dda240c877bd0dc7, .context=41653709f181abf181794298f6d98f9b, .client=89448119447425700321927633112927827628, .op=130, .commit=130, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.117Z debug(replica): 0n: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=2b64e98f3934a67f9495dc08399fd5bc, .checksum_body=c59fac8a3599e75291f3f733952a3bfe, .cluster=0, .size=6272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=d61359834690ad86dda240c877bd0dc7, .context=41653709f181abf181794298f6d98f9b, .client=89448119447425700321927633112927827628, .op=130, .commit=130, .timestamp=1767635375403979890, .request=128, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.117Z debug(forest): entering forest.compact() op=130 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:37.117Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=137363456 len=901120 unlocked
2026-01-05 17:49:37.118Z debug(journal): 1: write_header: op=131 sectors[32768..36864]
2026-01-05 17:49:37.118Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:37.118Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:37.118Z debug(journal): 1: write: view=2 slot=131 op=131 len=900352: 8fbb5458dcf58f558a41c764b02ba5ff complete, marking clean
2026-01-05 17:49:37.118Z debug(client_replies): 0: write_reply: wrote (client=89448119447425700321927633112927827628 request=128)
2026-01-05 17:49:37.118Z debug(replica): 1n: send_prepare_ok: op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.118Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=a922ce948239e554f1c19d8cc01f87ef, .checksum_body=a9a524525ad95fca3fc234b638b0730a, .cluster=0, .size=900352, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=41653709f181abf181794298f6d98f9b, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=129, .operation=vsr.Operation(139), .previous_request_latency=1702090278 }
2026-01-05 17:49:37.118Z debug(replica): 1n: sending prepare_ok to replica 2: PrepareOk{ .checksum=6d6529501f6ef2a177375e3d80a5a2f3, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=1, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .prepare_checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit_min=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.118Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.118Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:37.118Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=6d6529501f6ef2a177375e3d80a5a2f3, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=1, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .prepare_checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit_min=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.118Z debug(vsr): 2: primary_abdicate_timeout reset
2026-01-05 17:49:37.118Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2026-01-05 17:49:37.118Z debug(replica): 2N: on_prepare_ok: quorum received, prepare_checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.118Z debug(vsr): 2: prepare_timeout stopped
2026-01-05 17:49:37.118Z debug(vsr): 2: primary_abdicate_timeout stopped
2026-01-05 17:49:37.118Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=137363456 len=901120 unlocked
2026-01-05 17:49:37.118Z debug(journal): 0: write_header: op=131 sectors[32768..36864]
2026-01-05 17:49:37.118Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:37.118Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:37.118Z debug(journal): 0: write: view=2 slot=131 op=131 len=900352: 8fbb5458dcf58f558a41c764b02ba5ff complete, marking clean
2026-01-05 17:49:37.118Z debug(replica): 0n: send_prepare_ok: op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.118Z debug(replica): 0n: sending prepare_ok to replica 2: PrepareOk{ .checksum=743becc358182bcbc098fc716bd76f31, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .prepare_checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit_min=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.121Z debug(replica): 2N: execute_op: executing view=2 primary=true op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff (create_transfers)
2026-01-05 17:49:37.121Z debug(replica): 2N: execute_op: commit_timestamp=1767635375403979890 prepare.header.timestamp=1767635377107856452
2026-01-05 17:49:37.132Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:37.132Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:37.132Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:37.132Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:37.147Z debug(replica): 2N: execute_op: advancing commit_max=130..131
2026-01-05 17:49:37.147Z debug(replica): 2N: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=129
2026-01-05 17:49:37.147Z debug(replica): 2N: execute_op: replying to client: Reply{ .checksum=9af1af93932d517ac0e831424dabf1ca, .checksum_body=84c4e949e472ec8a9094f8d9c9d79b7e, .cluster=0, .size=272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=a922ce948239e554f1c19d8cc01f87ef, .context=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .op=131, .commit=131, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.147Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=9af1af93932d517ac0e831424dabf1ca, .checksum_body=84c4e949e472ec8a9094f8d9c9d79b7e, .cluster=0, .size=272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=a922ce948239e554f1c19d8cc01f87ef, .context=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .op=131, .commit=131, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.147Z debug(forest): entering forest.compact() op=131 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:37.152Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:37.152Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:37.152Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:37.152Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:37.160Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=743becc358182bcbc098fc716bd76f31, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=aefe2344f99c5c1bf2ea92b5146e2b8d, .prepare_checksum=8fbb5458dcf58f558a41c764b02ba5ff, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=131, .commit_min=130, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:37.160Z debug(replica): 2N: on_prepare_ok: not preparing op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:37.160Z debug(client_replies): 2: write_reply: wrote (client=89448119447425700321927633112927827628 request=129)
2026-01-05 17:49:37.170Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:37.170Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:37.172Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:37.172Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:37.172Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:37.172Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:37.183Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:37.183Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:37.183Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:37.183Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:37.183Z debug(replica): 2N: primary_pipeline_prepare: request checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b client=89448119447425700321927633112927827628
2026-01-05 17:49:37.183Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:37.183Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=51e5d453cc4ea823baccfb72b57e94ea op=132
2026-01-05 17:49:37.183Z debug(vsr): 2: prepare_timeout started
2026-01-05 17:49:37.183Z debug(vsr): 2: primary_abdicate_timeout started
2026-01-05 17:49:37.183Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:37.183Z debug(replica): 2N: replicate: replicating op=132 to replica 1
2026-01-05 17:49:37.183Z debug(replica): 2N: sending prepare to replica 1: Prepare{ .checksum=51e5d453cc4ea823baccfb72b57e94ea, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .request_checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.184Z debug(replica): 2N: replicate: replicating op=132 to replica 0
2026-01-05 17:49:37.184Z debug(replica): 2N: sending prepare to replica 0: Prepare{ .checksum=51e5d453cc4ea823baccfb72b57e94ea, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .request_checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.184Z debug(replica): 2N: on_prepare: advancing: op=131..132 checksum=8fbb5458dcf58f558a41c764b02ba5ff..51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:37.184Z debug(journal): 2: set_header_as_dirty: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:37.184Z debug(replica): 2N: append: appending to journal op=132
2026-01-05 17:49:37.184Z debug(journal): 2: write: view=2 slot=132 op=132 len=1008: 51e5d453cc4ea823baccfb72b57e94ea starting
2026-01-05 17:49:37.184Z debug(replica): 0n: on_message: view=2 status=normal Prepare{ .checksum=51e5d453cc4ea823baccfb72b57e94ea, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .request_checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.184Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=138412032 len=4096 locked
2026-01-05 17:49:37.184Z debug(replica): 0n: on_prepare: advancing commit_max=130..131
2026-01-05 17:49:37.184Z debug(replica): 1n: on_message: view=2 status=normal Prepare{ .checksum=51e5d453cc4ea823baccfb72b57e94ea, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .request_checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:37.184Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:37.184Z debug(replica): 1n: on_prepare: advancing commit_max=130..131
2026-01-05 17:49:37.184Z debug(replica): 0n: on_prepare: caching prepare.op=132 (commit_min=130 op=131 commit_max=131 prepare_max=1007)
2026-01-05 17:49:39.154Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.154Z debug(replica): 1n: on_prepare: caching prepare.op=132 (commit_min=130 op=131 commit_max=131 prepare_max=1007)
2026-01-05 17:49:39.154Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.154Z debug(replica): 0n: on_prepare: advancing: op=131..132 checksum=8fbb5458dcf58f558a41c764b02ba5ff..51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.154Z debug(replica): 1n: on_prepare: advancing: op=131..132 checksum=8fbb5458dcf58f558a41c764b02ba5ff..51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.154Z debug(journal): 0: set_header_as_dirty: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.154Z debug(journal): 1: set_header_as_dirty: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.154Z debug(replica): 0n: append: appending to journal op=132
2026-01-05 17:49:39.154Z debug(replica): 1n: append: appending to journal op=132
2026-01-05 17:49:39.154Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=138412032 len=4096 unlocked
2026-01-05 17:49:39.154Z debug(journal): 0: write: view=2 slot=132 op=132 len=1008: 51e5d453cc4ea823baccfb72b57e94ea starting
2026-01-05 17:49:39.154Z debug(journal): 1: write: view=2 slot=132 op=132 len=1008: 51e5d453cc4ea823baccfb72b57e94ea starting
2026-01-05 17:49:39.154Z debug(journal): 2: write_header: op=132 sectors[32768..36864]
2026-01-05 17:49:39.154Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:39.154Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=138412032 len=4096 locked
2026-01-05 17:49:39.154Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=138412032 len=4096 locked
2026-01-05 17:49:39.155Z debug(replica): 1n: commit_start_journal: cached prepare op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:39.155Z debug(replica): 0n: commit_start_journal: cached prepare op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff
2026-01-05 17:49:39.155Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.155Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:39.155Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:39.155Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.155Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.155Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.155Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.155Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.155Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.155Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.156Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.156Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.156Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:39.156Z debug(journal): 2: write: view=2 slot=132 op=132 len=1008: 51e5d453cc4ea823baccfb72b57e94ea complete, marking clean
2026-01-05 17:49:39.156Z debug(replica): 2N: send_prepare_ok: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.156Z debug(replica): 2N: sending prepare_ok to replica 2: PrepareOk{ .checksum=2c0b972348a750c966c311768c0ef106, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .prepare_checksum=51e5d453cc4ea823baccfb72b57e94ea, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit_min=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.156Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=2c0b972348a750c966c311768c0ef106, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .prepare_checksum=51e5d453cc4ea823baccfb72b57e94ea, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit_min=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.156Z debug(vsr): 2: primary_abdicate_timeout reset
2026-01-05 17:49:39.156Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2026-01-05 17:49:39.156Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2026-01-05 17:49:39.157Z debug(replica): 1n: repair_prepare: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea (already writing)
2026-01-05 17:49:39.157Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=130)
2026-01-05 17:49:39.157Z debug(replica): 0n: repair_prepare: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea (already writing)
2026-01-05 17:49:39.157Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=130)
2026-01-05 17:49:39.159Z debug(replica): 0n: execute_op: executing view=2 primary=false op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff (create_transfers)
2026-01-05 17:49:39.159Z debug(replica): 0n: execute_op: commit_timestamp=1767635375403979890 prepare.header.timestamp=1767635377107856452
2026-01-05 17:49:39.159Z debug(replica): 1n: execute_op: executing view=2 primary=false op=131 checksum=8fbb5458dcf58f558a41c764b02ba5ff (create_transfers)
2026-01-05 17:49:39.159Z debug(replica): 1n: execute_op: commit_timestamp=1767635375403979890 prepare.header.timestamp=1767635377107856452
2026-01-05 17:49:39.168Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.168Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.168Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.175Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:39.175Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:39.185Z debug(replica): 0n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=129
2026-01-05 17:49:39.185Z debug(forest): entering forest.compact() op=131 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:39.185Z debug(replica): 1n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=129
2026-01-05 17:49:39.185Z debug(replica): 1n: execute_op: replying to client: Reply{ .checksum=9af1af93932d517ac0e831424dabf1ca, .checksum_body=84c4e949e472ec8a9094f8d9c9d79b7e, .cluster=0, .size=272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=a922ce948239e554f1c19d8cc01f87ef, .context=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .op=131, .commit=131, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:39.185Z debug(replica): 1n: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=9af1af93932d517ac0e831424dabf1ca, .checksum_body=84c4e949e472ec8a9094f8d9c9d79b7e, .cluster=0, .size=272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=a922ce948239e554f1c19d8cc01f87ef, .context=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .op=131, .commit=131, .timestamp=1767635377107856452, .request=129, .operation=vsr.Operation(139) }
2026-01-05 17:49:39.185Z debug(forest): entering forest.compact() op=131 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:39.195Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:39.195Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:39.195Z debug(vsr): 2: journal_repair_timeout fired
2026-01-05 17:49:39.195Z debug(vsr): 2: journal_repair_timeout reset
2026-01-05 17:49:39.197Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.197Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.197Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.197Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:39.197Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:39.197Z debug(vsr): 0: journal_repair_timeout fired
2026-01-05 17:49:39.197Z debug(vsr): 0: journal_repair_timeout reset
2026-01-05 17:49:39.197Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.197Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.197Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.197Z debug(replica): 0n: repair_prepare: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea (already writing)
2026-01-05 17:49:39.197Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=138412032 len=4096 unlocked
2026-01-05 17:49:39.197Z debug(journal): 0: write_header: op=132 sectors[32768..36864]
2026-01-05 17:49:39.197Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:39.197Z debug(client_replies): 0: write_reply: wrote (client=89448119447425700321927633112927827628 request=129)
2026-01-05 17:49:39.197Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.197Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.197Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.197Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.197Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.197Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.197Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.198Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(replica): 1n: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.198Z debug(replica): 1n: sending request to replica 2: Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:39.198Z debug(journal): 0: write: view=2 slot=132 op=132 len=1008: 51e5d453cc4ea823baccfb72b57e94ea complete, marking clean
2026-01-05 17:49:39.198Z debug(replica): 0n: send_prepare_ok: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.198Z debug(replica): 0n: sending prepare_ok to replica 2: PrepareOk{ .checksum=522d2556cb251d071125fb09fdf5705f, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .prepare_checksum=51e5d453cc4ea823baccfb72b57e94ea, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit_min=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.198Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:39.198Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:39.198Z debug(vsr): 1: journal_repair_timeout fired
2026-01-05 17:49:39.198Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.198Z debug(vsr): 1: journal_repair_timeout reset
2026-01-05 17:49:39.198Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.198Z debug(replica): 1n: repair_prepare: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea (already writing)
2026-01-05 17:49:39.198Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=138412032 len=4096 unlocked
2026-01-05 17:49:39.198Z debug(journal): 1: write_header: op=132 sectors[32768..36864]
2026-01-05 17:49:39.198Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:39.198Z debug(client_replies): 1: write_reply: wrote (client=89448119447425700321927633112927827628 request=129)
2026-01-05 17:49:39.198Z debug(replica): 1n: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.198Z debug(replica): 1n: sending request to replica 2: Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(replica): 1n: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.198Z debug(replica): 1n: sending request to replica 2: Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.198Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.198Z debug(replica): 1n: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.198Z debug(replica): 1n: sending request to replica 2: Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.198Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:39.198Z debug(journal): 1: write: view=2 slot=132 op=132 len=1008: 51e5d453cc4ea823baccfb72b57e94ea complete, marking clean
2026-01-05 17:49:39.198Z debug(replica): 1n: send_prepare_ok: op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.198Z debug(replica): 1n: sending prepare_ok to replica 2: PrepareOk{ .checksum=0672c9b5db6d60f4d14aad24ea9f10fd, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=1, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .prepare_checksum=51e5d453cc4ea823baccfb72b57e94ea, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit_min=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.215Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:39.215Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:39.217Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:39.217Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:39.218Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:39.218Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:39.235Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:39.235Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:39.237Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:39.237Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:39.238Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:39.238Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:39.238Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.239Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.239Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.239Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=522d2556cb251d071125fb09fdf5705f, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .prepare_checksum=51e5d453cc4ea823baccfb72b57e94ea, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit_min=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.239Z debug(vsr): 2: primary_abdicate_timeout reset
2026-01-05 17:49:39.239Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2026-01-05 17:49:39.239Z debug(replica): 2N: on_prepare_ok: quorum received, prepare_checksum=51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.239Z debug(vsr): 2: prepare_timeout stopped
2026-01-05 17:49:39.239Z debug(vsr): 2: primary_abdicate_timeout stopped
2026-01-05 17:49:39.239Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.239Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=051d86655f800383bfd6f75a45bc0340, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=130, .operation=vsr.Operation(140), .previous_request_latency=60519042 }
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.239Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.239Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=0672c9b5db6d60f4d14aad24ea9f10fd, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=1, .parent=8fbb5458dcf58f558a41c764b02ba5ff, .prepare_checksum=51e5d453cc4ea823baccfb72b57e94ea, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=132, .commit_min=131, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.239Z debug(replica): 2N: on_prepare_ok: 3 message(s)
2026-01-05 17:49:39.239Z debug(replica): 2N: on_prepare_ok: ignoring (quorum received already)
2026-01-05 17:49:39.239Z debug(replica): 2N: execute_op: executing view=2 primary=true op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea (lookup_accounts)
2026-01-05 17:49:39.239Z debug(replica): 2N: execute_op: commit_timestamp=1767635377107856452 prepare.header.timestamp=1767635377183927122
2026-01-05 17:49:39.239Z debug(replica): 2N: execute_op: advancing commit_max=131..132
2026-01-05 17:49:39.239Z debug(replica): 2N: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=130
2026-01-05 17:49:39.239Z debug(replica): 2N: execute_op: replying to client: Reply{ .checksum=2f5f582fb47eff0dbb06a17a7c4c147d, .checksum_body=a8e13ab86bb166db0bb3ce0fa4218a5c, .cluster=0, .size=6272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .context=ab0fe69e5f22c8f36eaa786fc072cdf9, .client=89448119447425700321927633112927827628, .op=132, .commit=132, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.239Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=2f5f582fb47eff0dbb06a17a7c4c147d, .checksum_body=a8e13ab86bb166db0bb3ce0fa4218a5c, .cluster=0, .size=6272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .context=ab0fe69e5f22c8f36eaa786fc072cdf9, .client=89448119447425700321927633112927827628, .op=132, .commit=132, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.239Z debug(forest): entering forest.compact() op=132 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:39.239Z info(workload): accounts created = 46, transfers = 107978, pending transfers = 0, commands run = 65
2026-01-05 17:49:39.239Z debug(client_replies): 2: write_reply: wrote (client=89448119447425700321927633112927827628 request=130)
2026-01-05 17:49:39.241Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=8e180ba0a24ccc77c0b90a214e63561d, .checksum_body=514c2d1f4be99fecc3782c04f9db415e, .cluster=0, .size=122240, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=ab0fe69e5f22c8f36eaa786fc072cdf9, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=131, .operation=vsr.Operation(141), .previous_request_latency=2056088602 }
2026-01-05 17:49:39.241Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.241Z debug(replica): 2N: primary_pipeline_prepare: request checksum=8e180ba0a24ccc77c0b90a214e63561d client=89448119447425700321927633112927827628
2026-01-05 17:49:39.241Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=8e180ba0a24ccc77c0b90a214e63561d, .checksum_body=514c2d1f4be99fecc3782c04f9db415e, .cluster=0, .size=122240, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=ab0fe69e5f22c8f36eaa786fc072cdf9, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=131, .operation=vsr.Operation(141), .previous_request_latency=2056088602 }
2026-01-05 17:49:39.241Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.241Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=8e180ba0a24ccc77c0b90a214e63561d, .checksum_body=514c2d1f4be99fecc3782c04f9db415e, .cluster=0, .size=122240, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=ab0fe69e5f22c8f36eaa786fc072cdf9, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=131, .operation=vsr.Operation(141), .previous_request_latency=2056088602 }
2026-01-05 17:49:39.242Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=d04003e88954be486c5d033509811f22 op=133
2026-01-05 17:49:39.242Z debug(vsr): 2: prepare_timeout started
2026-01-05 17:49:39.242Z debug(vsr): 2: primary_abdicate_timeout started
2026-01-05 17:49:39.242Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:39.242Z debug(replica): 2N: replicate: replicating op=133 to replica 1
2026-01-05 17:49:39.242Z debug(replica): 2N: sending prepare to replica 1: Prepare{ .checksum=d04003e88954be486c5d033509811f22, .checksum_body=514c2d1f4be99fecc3782c04f9db415e, .cluster=0, .size=122240, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=51e5d453cc4ea823baccfb72b57e94ea, .request_checksum=8e180ba0a24ccc77c0b90a214e63561d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.242Z debug(replica): 2N: replicate: replicating op=133 to replica 0
2026-01-05 17:49:39.242Z debug(replica): 2N: sending prepare to replica 0: Prepare{ .checksum=d04003e88954be486c5d033509811f22, .checksum_body=514c2d1f4be99fecc3782c04f9db415e, .cluster=0, .size=122240, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=51e5d453cc4ea823baccfb72b57e94ea, .request_checksum=8e180ba0a24ccc77c0b90a214e63561d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.242Z debug(replica): 2N: on_prepare: advancing: op=132..133 checksum=51e5d453cc4ea823baccfb72b57e94ea..d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.242Z debug(journal): 2: set_header_as_dirty: op=133 checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.242Z debug(replica): 2N: append: appending to journal op=133
2026-01-05 17:49:39.242Z debug(journal): 2: write: view=2 slot=133 op=133 len=122240: d04003e88954be486c5d033509811f22 starting
2026-01-05 17:49:39.242Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=139460608 len=122880 locked
2026-01-05 17:49:39.242Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=8e180ba0a24ccc77c0b90a214e63561d, .checksum_body=514c2d1f4be99fecc3782c04f9db415e, .cluster=0, .size=122240, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=ab0fe69e5f22c8f36eaa786fc072cdf9, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=131, .operation=vsr.Operation(141), .previous_request_latency=2056088602 }
2026-01-05 17:49:39.242Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.242Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:39.242Z debug(replica): 1n: on_message: view=2 status=normal Prepare{ .checksum=d04003e88954be486c5d033509811f22, .checksum_body=514c2d1f4be99fecc3782c04f9db415e, .cluster=0, .size=122240, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=51e5d453cc4ea823baccfb72b57e94ea, .request_checksum=8e180ba0a24ccc77c0b90a214e63561d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.242Z debug(replica): 1n: on_prepare: advancing commit_max=131..132
2026-01-05 17:49:39.242Z debug(replica): 1n: on_prepare: caching prepare.op=133 (commit_min=131 op=132 commit_max=132 prepare_max=1007)
2026-01-05 17:49:39.242Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=139460608 len=122880 unlocked
2026-01-05 17:49:39.242Z debug(journal): 2: write_header: op=133 sectors[32768..36864]
2026-01-05 17:49:39.242Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:39.243Z debug(replica): 1n: on_prepare: advancing: op=132..133 checksum=51e5d453cc4ea823baccfb72b57e94ea..d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.243Z debug(journal): 1: set_header_as_dirty: op=133 checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.243Z debug(replica): 1n: append: appending to journal op=133
2026-01-05 17:49:39.243Z debug(journal): 1: write: view=2 slot=133 op=133 len=122240: d04003e88954be486c5d033509811f22 starting
2026-01-05 17:49:39.243Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=139460608 len=122880 locked
2026-01-05 17:49:39.243Z debug(replica): 1n: commit_start_journal: cached prepare op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.243Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:39.243Z debug(journal): 2: write: view=2 slot=133 op=133 len=122240: d04003e88954be486c5d033509811f22 complete, marking clean
2026-01-05 17:49:39.243Z debug(replica): 2N: send_prepare_ok: op=133 checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.243Z debug(replica): 0n: on_message: view=2 status=normal Prepare{ .checksum=d04003e88954be486c5d033509811f22, .checksum_body=514c2d1f4be99fecc3782c04f9db415e, .cluster=0, .size=122240, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=51e5d453cc4ea823baccfb72b57e94ea, .request_checksum=8e180ba0a24ccc77c0b90a214e63561d, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.243Z debug(replica): 2N: sending prepare_ok to replica 2: PrepareOk{ .checksum=d2390a7c7b3ed00a9ad1af14dbd09792, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=51e5d453cc4ea823baccfb72b57e94ea, .prepare_checksum=d04003e88954be486c5d033509811f22, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit_min=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.243Z debug(replica): 0n: on_prepare: advancing commit_max=131..132
2026-01-05 17:49:39.243Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=d2390a7c7b3ed00a9ad1af14dbd09792, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=51e5d453cc4ea823baccfb72b57e94ea, .prepare_checksum=d04003e88954be486c5d033509811f22, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit_min=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.243Z debug(vsr): 2: primary_abdicate_timeout reset
2026-01-05 17:49:39.243Z debug(replica): 0n: on_prepare: caching prepare.op=133 (commit_min=131 op=132 commit_max=132 prepare_max=1007)
2026-01-05 17:49:39.243Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2026-01-05 17:49:39.243Z debug(replica): 1n: repair_prepare: op=133 checksum=d04003e88954be486c5d033509811f22 (already writing)
2026-01-05 17:49:39.243Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2026-01-05 17:49:39.243Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=131)
2026-01-05 17:49:39.243Z debug(replica): 0n: on_prepare: advancing: op=132..133 checksum=51e5d453cc4ea823baccfb72b57e94ea..d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.243Z debug(replica): 1n: execute_op: executing view=2 primary=false op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea (lookup_accounts)
2026-01-05 17:49:39.243Z debug(replica): 1n: execute_op: commit_timestamp=1767635377107856452 prepare.header.timestamp=1767635377183927122
2026-01-05 17:49:39.243Z debug(journal): 0: set_header_as_dirty: op=133 checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.243Z debug(replica): 0n: append: appending to journal op=133
2026-01-05 17:49:39.243Z debug(journal): 0: write: view=2 slot=133 op=133 len=122240: d04003e88954be486c5d033509811f22 starting
2026-01-05 17:49:39.243Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=139460608 len=122880 locked
2026-01-05 17:49:39.243Z debug(replica): 0n: commit_start_journal: cached prepare op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea
2026-01-05 17:49:39.243Z debug(replica): 1n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=130
2026-01-05 17:49:39.243Z debug(replica): 1n: execute_op: replying to client: Reply{ .checksum=2f5f582fb47eff0dbb06a17a7c4c147d, .checksum_body=a8e13ab86bb166db0bb3ce0fa4218a5c, .cluster=0, .size=6272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .context=ab0fe69e5f22c8f36eaa786fc072cdf9, .client=89448119447425700321927633112927827628, .op=132, .commit=132, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.243Z debug(replica): 1n: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=2f5f582fb47eff0dbb06a17a7c4c147d, .checksum_body=a8e13ab86bb166db0bb3ce0fa4218a5c, .cluster=0, .size=6272, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=f72bbf1d1cfeb04bb27e47bf1624dc6b, .context=ab0fe69e5f22c8f36eaa786fc072cdf9, .client=89448119447425700321927633112927827628, .op=132, .commit=132, .timestamp=1767635377183927122, .request=130, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.243Z debug(forest): entering forest.compact() op=132 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:39.243Z debug(replica): 0n: repair_prepare: op=133 checksum=d04003e88954be486c5d033509811f22 (already writing)
2026-01-05 17:49:39.243Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=131)
2026-01-05 17:49:39.243Z debug(replica): 0n: execute_op: executing view=2 primary=false op=132 checksum=51e5d453cc4ea823baccfb72b57e94ea (lookup_accounts)
2026-01-05 17:49:39.243Z debug(replica): 0n: execute_op: commit_timestamp=1767635377107856452 prepare.header.timestamp=1767635377183927122
2026-01-05 17:49:39.243Z debug(replica): 0n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=130
2026-01-05 17:49:39.243Z debug(forest): entering forest.compact() op=132 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:39.243Z debug(client_replies): 1: write_reply: wrote (client=89448119447425700321927633112927827628 request=130)
2026-01-05 17:49:39.243Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=139460608 len=122880 unlocked
2026-01-05 17:49:39.243Z debug(journal): 1: write_header: op=133 sectors[32768..36864]
2026-01-05 17:49:39.243Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:39.243Z debug(client_replies): 0: write_reply: wrote (client=89448119447425700321927633112927827628 request=130)
2026-01-05 17:49:39.243Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:39.243Z debug(journal): 1: write: view=2 slot=133 op=133 len=122240: d04003e88954be486c5d033509811f22 complete, marking clean
2026-01-05 17:49:39.243Z debug(replica): 1n: send_prepare_ok: op=133 checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.243Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=139460608 len=122880 unlocked
2026-01-05 17:49:39.243Z debug(replica): 1n: sending prepare_ok to replica 2: PrepareOk{ .checksum=587c7c41e0f2cb5356641c74a1ab94fc, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=1, .parent=51e5d453cc4ea823baccfb72b57e94ea, .prepare_checksum=d04003e88954be486c5d033509811f22, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit_min=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.243Z debug(journal): 0: write_header: op=133 sectors[32768..36864]
2026-01-05 17:49:39.243Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:39.244Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:39.244Z debug(journal): 0: write: view=2 slot=133 op=133 len=122240: d04003e88954be486c5d033509811f22 complete, marking clean
2026-01-05 17:49:39.244Z debug(replica): 0n: send_prepare_ok: op=133 checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.244Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=587c7c41e0f2cb5356641c74a1ab94fc, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=1, .parent=51e5d453cc4ea823baccfb72b57e94ea, .prepare_checksum=d04003e88954be486c5d033509811f22, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit_min=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.244Z debug(vsr): 2: primary_abdicate_timeout reset
2026-01-05 17:49:39.244Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2026-01-05 17:49:39.244Z debug(replica): 2N: on_prepare_ok: quorum received, prepare_checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.244Z debug(vsr): 2: prepare_timeout stopped
2026-01-05 17:49:39.244Z debug(replica): 0n: sending prepare_ok to replica 2: PrepareOk{ .checksum=83266037c1ac18436a5d827d68e1a65e, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=51e5d453cc4ea823baccfb72b57e94ea, .prepare_checksum=d04003e88954be486c5d033509811f22, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit_min=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.244Z debug(vsr): 2: primary_abdicate_timeout stopped
2026-01-05 17:49:39.247Z debug(replica): 2N: execute_op: executing view=2 primary=true op=133 checksum=d04003e88954be486c5d033509811f22 (lookup_transfers)
2026-01-05 17:49:39.247Z debug(replica): 2N: execute_op: commit_timestamp=1767635377183927122 prepare.header.timestamp=1767635379241594025
2026-01-05 17:49:39.248Z debug(replica): 2N: execute_op: advancing commit_max=132..133
2026-01-05 17:49:39.252Z debug(replica): 2N: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=131
2026-01-05 17:49:39.252Z debug(replica): 2N: execute_op: replying to client: Reply{ .checksum=38e8b7708745331bcd7588d80ab9e240, .checksum_body=0f1af3842b756330839274e59e298389, .cluster=0, .size=976128, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=8e180ba0a24ccc77c0b90a214e63561d, .context=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .op=133, .commit=133, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.252Z debug(replica): 2N: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=38e8b7708745331bcd7588d80ab9e240, .checksum_body=0f1af3842b756330839274e59e298389, .cluster=0, .size=976128, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=8e180ba0a24ccc77c0b90a214e63561d, .context=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .op=133, .commit=133, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.253Z debug(forest): entering forest.compact() op=133 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:39.253Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=83266037c1ac18436a5d827d68e1a65e, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=51e5d453cc4ea823baccfb72b57e94ea, .prepare_checksum=d04003e88954be486c5d033509811f22, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=133, .commit_min=132, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:39.253Z debug(replica): 2N: on_prepare_ok: not preparing op=133 checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:39.253Z debug(client_replies): 2: write_reply: wrote (client=89448119447425700321927633112927827628 request=131)
2026-01-05 17:49:39.258Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:39.258Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:39.258Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:39.258Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:39.263Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:39.263Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:39.263Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:39.263Z debug(replica): 2N: primary_pipeline_prepare: request checksum=3ba6709163b4028495cf3d8ddb13784f client=89448119447425700321927633112927827628
2026-01-05 17:49:39.263Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:39.263Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:39.263Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=61311b9a1011380cdcde0f807e8dc2f1 op=134
2026-01-05 17:49:39.263Z debug(vsr): 2: prepare_timeout started
2026-01-05 17:49:39.263Z debug(vsr): 2: primary_abdicate_timeout started
2026-01-05 17:49:39.263Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:39.263Z debug(replica): 2N: replicate: replicating op=134 to replica 1
2026-01-05 17:49:39.263Z debug(replica): 2N: sending prepare to replica 1: Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.263Z debug(replica): 2N: replicate: replicating op=134 to replica 0
2026-01-05 17:49:39.263Z debug(replica): 2N: sending prepare to replica 0: Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.264Z debug(replica): 2N: on_prepare: advancing: op=133..134 checksum=d04003e88954be486c5d033509811f22..61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:39.264Z debug(journal): 2: set_header_as_dirty: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:39.264Z debug(replica): 2N: append: appending to journal op=134
2026-01-05 17:49:39.264Z debug(journal): 2: write: view=2 slot=134 op=134 len=1008: 61311b9a1011380cdcde0f807e8dc2f1 starting
2026-01-05 17:49:39.264Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=140509184 len=4096 locked
2026-01-05 17:49:39.264Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:39.264Z debug(replica): 0n: on_message: view=2 status=normal Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:39.264Z debug(replica): 1n: on_message: view=2 status=normal Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:40.314Z info(supervisor): injecting network delays: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 424, .jitter_ms = 50 }, .lose = null, .corrupt = null }
2026-01-05 17:49:41.227Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:41.227Z debug(replica): 1n: on_prepare: advancing commit_max=132..133
2026-01-05 17:49:41.227Z debug(replica): 0n: on_prepare: advancing commit_max=132..133
2026-01-05 17:49:41.227Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:41.227Z debug(replica): 1n: on_prepare: caching prepare.op=134 (commit_min=132 op=133 commit_max=133 prepare_max=1007)
2026-01-05 17:49:41.227Z debug(replica): 0n: on_prepare: caching prepare.op=134 (commit_min=132 op=133 commit_max=133 prepare_max=1007)
2026-01-05 17:49:41.227Z debug(replica): 1n: on_prepare: advancing: op=133..134 checksum=d04003e88954be486c5d033509811f22..61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:41.227Z debug(replica): 0n: on_prepare: advancing: op=133..134 checksum=d04003e88954be486c5d033509811f22..61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:41.227Z debug(journal): 0: set_header_as_dirty: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:41.227Z debug(journal): 1: set_header_as_dirty: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:41.227Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.227Z debug(replica): 0n: append: appending to journal op=134
2026-01-05 17:49:41.227Z debug(replica): 1n: append: appending to journal op=134
2026-01-05 17:49:41.227Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.227Z debug(journal): 0: write: view=2 slot=134 op=134 len=1008: 61311b9a1011380cdcde0f807e8dc2f1 starting
2026-01-05 17:49:41.227Z debug(journal): 1: write: view=2 slot=134 op=134 len=1008: 61311b9a1011380cdcde0f807e8dc2f1 starting
2026-01-05 17:49:41.227Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=140509184 len=4096 locked
2026-01-05 17:49:41.227Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=140509184 len=4096 locked
2026-01-05 17:49:41.227Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=140509184 len=4096 unlocked
2026-01-05 17:49:41.227Z debug(journal): 2: write_header: op=134 sectors[32768..36864]
2026-01-05 17:49:41.227Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:41.227Z debug(replica): 1n: commit_start_journal: cached prepare op=133 checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:41.227Z debug(replica): 0n: commit_start_journal: cached prepare op=133 checksum=d04003e88954be486c5d033509811f22
2026-01-05 17:49:41.227Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.227Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:41.227Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:41.227Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.227Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:41.227Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:41.228Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.228Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:41.228Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:41.228Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.228Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:41.228Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:41.228Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:41.228Z debug(journal): 2: write: view=2 slot=134 op=134 len=1008: 61311b9a1011380cdcde0f807e8dc2f1 complete, marking clean
2026-01-05 17:49:41.228Z debug(replica): 2N: send_prepare_ok: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:41.228Z debug(replica): 2N: sending prepare_ok to replica 2: PrepareOk{ .checksum=9ce3a4d229868944ba61a61e980148fc, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=d04003e88954be486c5d033509811f22, .prepare_checksum=61311b9a1011380cdcde0f807e8dc2f1, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit_min=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.228Z debug(replica): 2N: on_message: view=2 status=normal PrepareOk{ .checksum=9ce3a4d229868944ba61a61e980148fc, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=2, .parent=d04003e88954be486c5d033509811f22, .prepare_checksum=61311b9a1011380cdcde0f807e8dc2f1, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit_min=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.228Z debug(vsr): 2: primary_abdicate_timeout reset
2026-01-05 17:49:41.228Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2026-01-05 17:49:41.228Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2026-01-05 17:49:41.230Z debug(replica): 0n: repair_prepare: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1 (already writing)
2026-01-05 17:49:41.230Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=132)
2026-01-05 17:49:41.230Z debug(replica): 1n: repair_prepare: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1 (already writing)
2026-01-05 17:49:41.230Z debug(replica): 0n: execute_op: executing view=2 primary=false op=133 checksum=d04003e88954be486c5d033509811f22 (lookup_transfers)
2026-01-05 17:49:41.230Z debug(replica): 0n: execute_op: commit_timestamp=1767635377183927122 prepare.header.timestamp=1767635379241594025
2026-01-05 17:49:41.231Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=132)
2026-01-05 17:49:41.231Z debug(replica): 1n: execute_op: executing view=2 primary=false op=133 checksum=d04003e88954be486c5d033509811f22 (lookup_transfers)
2026-01-05 17:49:41.231Z debug(replica): 1n: execute_op: commit_timestamp=1767635377183927122 prepare.header.timestamp=1767635379241594025
2026-01-05 17:49:41.237Z debug(replica): 0n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=131
2026-01-05 17:49:41.237Z debug(replica): 0n: execute_op: replying to client: Reply{ .checksum=38e8b7708745331bcd7588d80ab9e240, .checksum_body=0f1af3842b756330839274e59e298389, .cluster=0, .size=976128, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=8e180ba0a24ccc77c0b90a214e63561d, .context=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .op=133, .commit=133, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:41.237Z debug(replica): 0n: sending reply to client 89448119447425700321927633112927827628: Reply{ .checksum=38e8b7708745331bcd7588d80ab9e240, .checksum_body=0f1af3842b756330839274e59e298389, .cluster=0, .size=976128, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.reply, .replica=2, .request_checksum=8e180ba0a24ccc77c0b90a214e63561d, .context=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .op=133, .commit=133, .timestamp=1767635379241594025, .request=131, .operation=vsr.Operation(141) }
2026-01-05 17:49:41.237Z debug(forest): entering forest.compact() op=133 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:41.237Z info(supervisor): injecting network corruption: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 424, .jitter_ms = 50 }, .lose = null, .corrupt = 7/100 }
2026-01-05 17:49:41.237Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.238Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:41.238Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.238Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=140509184 len=4096 unlocked
2026-01-05 17:49:41.238Z debug(journal): 0: write_header: op=134 sectors[32768..36864]
2026-01-05 17:49:41.238Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:41.238Z debug(replica): 1n: client_table_entry_update: client=89448119447425700321927633112927827628 session=2 request=131
2026-01-05 17:49:41.238Z debug(forest): entering forest.compact() op=133 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2026-01-05 17:49:41.238Z debug(client_replies): 0: write_reply: wrote (client=89448119447425700321927633112927827628 request=131)
2026-01-05 17:49:41.238Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:41.238Z debug(journal): 0: write: view=2 slot=134 op=134 len=1008: 61311b9a1011380cdcde0f807e8dc2f1 complete, marking clean
2026-01-05 17:49:41.238Z debug(replica): 0n: send_prepare_ok: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:41.238Z debug(replica): 0n: sending prepare_ok to replica 2: PrepareOk{ .checksum=9f8529ae5e8fee01c41f62b123eb6888, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=d04003e88954be486c5d033509811f22, .prepare_checksum=61311b9a1011380cdcde0f807e8dc2f1, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit_min=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.238Z debug(replica): 1n: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.238Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:41.238Z debug(replica): 1n: sending request to replica 2: Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.238Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=140509184 len=4096 unlocked
2026-01-05 17:49:41.238Z debug(journal): 1: write_header: op=134 sectors[32768..36864]
2026-01-05 17:49:41.238Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 locked
2026-01-05 17:49:41.238Z debug(replica): 1n: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.238Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:41.238Z debug(replica): 1n: sending request to replica 2: Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.238Z debug(replica): 1n: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.238Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:41.238Z debug(replica): 1n: sending request to replica 2: Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.238Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=32768 len=4096 unlocked
2026-01-05 17:49:41.238Z debug(journal): 1: write: view=2 slot=134 op=134 len=1008: 61311b9a1011380cdcde0f807e8dc2f1 complete, marking clean
2026-01-05 17:49:41.238Z debug(replica): 1n: send_prepare_ok: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:41.238Z debug(replica): 1n: sending prepare_ok to replica 2: PrepareOk{ .checksum=8f185515e45122940628c91b5bfa3a60, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=1, .parent=d04003e88954be486c5d033509811f22, .prepare_checksum=61311b9a1011380cdcde0f807e8dc2f1, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit_min=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.238Z debug(client_replies): 1: write_reply: wrote (client=89448119447425700321927633112927827628 request=131)
2026-01-05 17:49:41.247Z info(supervisor): healing network
2026-01-05 17:49:41.247Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.247Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.248Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.248Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.248Z debug(vsr): 1: journal_repair_budget_timeout fired
2026-01-05 17:49:41.248Z debug(vsr): 1: journal_repair_budget_timeout reset
2026-01-05 17:49:41.257Z info(supervisor): 1: terminating replica
2026-01-05 17:49:41.267Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.267Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.267Z debug(vsr): 2: journal_repair_timeout fired
2026-01-05 17:49:41.267Z debug(vsr): 2: journal_repair_timeout reset
2026-01-05 17:49:41.268Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.268Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.268Z debug(vsr): 0: journal_repair_timeout fired
2026-01-05 17:49:41.268Z debug(vsr): 0: journal_repair_timeout reset
2026-01-05 17:49:41.268Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
warning(message_bus): 89448119447425700321927633112927827628: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2026-01-05 17:49:41.278Z info(supervisor): injecting network corruption: testing.vortex.faulty_network.Faults{ .delay = null, .lose = null, .corrupt = 2/100 }
2026-01-05 17:49:41.278Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=68ms
2026-01-05 17:49:41.287Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.287Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.288Z info(supervisor): healing network
2026-01-05 17:49:41.288Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.288Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.298Z info(supervisor): 1: starting replica
2026-01-05 17:49:41.303Z info(io): opening "0_1.tigerbeetle"...
2026-01-05 17:49:41.307Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.307Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.308Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.308Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.309Z info(supervisor): sleeping for 7.127s
2026-01-05 17:49:41.317Z debug(vsr): 2: pulse_timeout fired
2026-01-05 17:49:41.317Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:41.327Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.327Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.328Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.328Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.344Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2026-01-05 17:49:41.346Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2026-01-05 17:49:41.346Z info(message_bus): 0: on_connect: connected to=1
2026-01-05 17:49:41.346Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2026-01-05 17:49:41.346Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2026-01-05 17:49:41.347Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.348Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.348Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=50ms
2026-01-05 17:49:41.348Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.348Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.358Z info(main): multiversioning: upgrades disabled for development (0.0.1) release.
2026-01-05 17:49:41.358Z info(main): release=0.0.1
2026-01-05 17:49:41.358Z info(main): release_client_min=0.0.1
2026-01-05 17:49:41.358Z info(main): releases_bundled={ 0.0.1 }
2026-01-05 17:49:41.358Z info(main): git_commit=5ae887984e025d036bc7b740ce213bc27136f74a
2026-01-05 17:49:41.359Z debug(superblock): null: open: started
2026-01-05 17:49:41.359Z debug(superblock): null: open: read_header: copy=0 size=8192 offset=0
2026-01-05 17:49:41.359Z debug(superblock): null: open: read_header: copy=1 size=8192 offset=24576
2026-01-05 17:49:41.359Z debug(superblock): null: open: read_header: copy=2 size=8192 offset=49152
2026-01-05 17:49:41.359Z debug(superblock): null: open: read_header: copy=3 size=8192 offset=73728
2026-01-05 17:49:41.359Z debug(superblock_quorums): copy: 0/4: checksum=7903e906cae2f01ed8ced46266d01f6c parent=cb97f2a4f21ab19424b32430b6bfbf99 sequence=5
2026-01-05 17:49:41.359Z debug(superblock_quorums): copy: 1/4: checksum=7903e906cae2f01ed8ced46266d01f6c parent=cb97f2a4f21ab19424b32430b6bfbf99 sequence=5
2026-01-05 17:49:41.359Z debug(superblock_quorums): copy: 2/4: checksum=7903e906cae2f01ed8ced46266d01f6c parent=cb97f2a4f21ab19424b32430b6bfbf99 sequence=5
2026-01-05 17:49:41.360Z debug(superblock_quorums): copy: 3/4: checksum=7903e906cae2f01ed8ced46266d01f6c parent=cb97f2a4f21ab19424b32430b6bfbf99 sequence=5
2026-01-05 17:49:41.360Z debug(superblock_quorums): quorum: checksum=7903e906cae2f01ed8ced46266d01f6c parent=cb97f2a4f21ab19424b32430b6bfbf99 sequence=5 count=4 valid=true
2026-01-05 17:49:41.360Z debug(superblock): null: open: installed working superblock: checksum=7903e906cae2f01ed8ced46266d01f6c sequence=5 release=0.0.1 cluster=00000000000000000000000000000000 replica_id=308943487097555535311203420603596972560 size=1141374976 free_set_blocks_acquired_size=0 free_set_blocks_released_size=0 client_sessions_size=0 checkpoint_id=f222e9ce156b309eaeb4af665242ac18 commit_min_checksum=5146b8d0e1f69ca2e6867c42bb8263b7 commit_min=0 commit_max=11 log_view=2 view=2 sync_op_min=0 sync_op_max=0 manifest_oldest_checksum=00000000000000000000000000000000 manifest_oldest_address=0 manifest_newest_checksum=00000000000000000000000000000000 manifest_newest_address=0 manifest_block_count=0 snapshots_block_checksum=00000000000000000000000000000000 snapshots_block_address=0
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=12 checksum=00e4d70af2a3eafc41aee27dec721105
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=11 checksum=389eb242b4c1ddaeef39a9d392a9acf8
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=10 checksum=89dd16f5c3feabe504177dd46e91c0d6
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=9 checksum=abf7c7850a6bd8da7fe96c43ecd9f618
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=8 checksum=70a5bb8cc1448446ca80fe96d9db37e1
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=7 checksum=27efab217bf0af7d9e54b3907f12ddc1
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=6 checksum=4682595513e20a60c8778765eb35f441
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=5 checksum=c99b08cc33946700ee17abd1ac0f1820
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=4 checksum=ea87ae125723c456eec8f7f5439aed86
2026-01-05 17:49:41.360Z debug(superblock): null: open: vsr_header: op=0 checksum=5146b8d0e1f69ca2e6867c42bb8263b7
2026-01-05 17:49:41.360Z debug(superblock): null: open: complete
2026-01-05 17:49:41.361Z debug(journal): 1: slot_count=1024 size=1.000244140625GiB headers_size=256KiB prepares_size=1GiB
2026-01-05 17:49:41.368Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.368Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.368Z debug(vsr): 2: journal_repair_timeout fired
2026-01-05 17:49:41.368Z debug(vsr): 2: journal_repair_timeout reset
2026-01-05 17:49:41.368Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.368Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.368Z debug(vsr): 0: journal_repair_timeout fired
2026-01-05 17:49:41.368Z debug(vsr): 0: journal_repair_timeout reset
2026-01-05 17:49:41.388Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.388Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.388Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.388Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.398Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2026-01-05 17:49:41.398Z info(message_bus): 0: on_connect: connected to=1
2026-01-05 17:49:41.398Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2026-01-05 17:49:41.398Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2026-01-05 17:49:41.408Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.408Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.408Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=75ms
2026-01-05 17:49:41.408Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.408Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.418Z debug(vsr): 2: pulse_timeout fired
2026-01-05 17:49:41.418Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:41.428Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.428Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.428Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.428Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.448Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.448Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.448Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2026-01-05 17:49:41.449Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.449Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.468Z debug(vsr): 2: prepare_timeout fired
2026-01-05 17:49:41.468Z debug(vsr): 2: prepare_timeout backing off
2026-01-05 17:49:41.468Z debug(vsr): 2: prepare_timeout after=25..3 (rtt=1 min=1 max=1000 attempts=1)
2026-01-05 17:49:41.468Z debug(replica): 2N: on_prepare_timeout: waiting for replica 0
2026-01-05 17:49:41.468Z debug(replica): 2N: on_prepare_timeout: waiting for replica 1
2026-01-05 17:49:41.468Z debug(replica): 2N: on_prepare_timeout: replicating to replica 1
2026-01-05 17:49:41.468Z debug(replica): 2N: sending prepare to replica 1: Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.468Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.468Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.468Z debug(vsr): 2: journal_repair_timeout fired
2026-01-05 17:49:41.468Z debug(vsr): 2: journal_repair_timeout reset
2026-01-05 17:49:41.469Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.469Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.469Z debug(vsr): 0: journal_repair_timeout fired
2026-01-05 17:49:41.469Z debug(vsr): 0: journal_repair_timeout reset
2026-01-05 17:49:41.483Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2026-01-05 17:49:41.484Z info(message_bus): 0: on_connect: connected to=1
2026-01-05 17:49:41.484Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2026-01-05 17:49:41.484Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2026-01-05 17:49:41.488Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.488Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.489Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=51ms
2026-01-05 17:49:41.489Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.489Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.492Z debug(manifest_log): 1: Manifest.Pace.half_bar_append_blocks_max = 1
2026-01-05 17:49:41.492Z debug(manifest_log): 1: Manifest.Pace.half_bar_compact_blocks_max = 2
2026-01-05 17:49:41.492Z debug(manifest_log): 1: Manifest.Pace.log_blocks_full_max = 586
2026-01-05 17:49:41.492Z debug(manifest_log): 1: Manifest.Pace.log_blocks_cycle_max = 1172
2026-01-05 17:49:41.492Z debug(manifest_log): 1: Manifest.Pace.log_blocks_max = 1466
2026-01-05 17:49:41.492Z debug(manifest_log): 1: Manifest.Pace.tables_max = 2396744
2026-01-05 17:49:41.498Z debug(vsr): 2: prepare_timeout fired
2026-01-05 17:49:41.498Z debug(vsr): 2: prepare_timeout backing off
2026-01-05 17:49:41.498Z debug(vsr): 2: prepare_timeout after=3..7 (rtt=1 min=1 max=1000 attempts=2)
2026-01-05 17:49:41.498Z debug(replica): 2N: on_prepare_timeout: waiting for replica 0
2026-01-05 17:49:41.498Z debug(replica): 2N: on_prepare_timeout: waiting for replica 1
2026-01-05 17:49:41.498Z debug(replica): 2N: on_prepare_timeout: replicating to replica 0
2026-01-05 17:49:41.498Z debug(replica): 2N: sending prepare to replica 0: Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.498Z debug(replica): 0n: on_message: view=2 status=normal Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.498Z warning(replica): 0n: on_prepare: not replicating op=134 commit_min=133 present=true
2026-01-05 17:49:41.498Z debug(replica): 0n: on_prepare: ignoring (repair)
2026-01-05 17:49:41.498Z debug(replica): 0n: on_repair: ignoring (duplicate)
2026-01-05 17:49:41.498Z debug(replica): 0n: send_prepare_ok: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:41.498Z debug(replica): 0n: sending prepare_ok to replica 2: PrepareOk{ .checksum=9f8529ae5e8fee01c41f62b123eb6888, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=d04003e88954be486c5d033509811f22, .prepare_checksum=61311b9a1011380cdcde0f807e8dc2f1, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit_min=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.508Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.508Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.509Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.509Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.518Z debug(vsr): 2: pulse_timeout fired
2026-01-05 17:49:41.518Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:41.525Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2026-01-05 17:49:41.528Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.528Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.529Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.529Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.540Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2026-01-05 17:49:41.540Z info(message_bus): 0: on_connect: connected to=1
2026-01-05 17:49:41.540Z warning(faulty_network): connect failed (1,7): error.ConnectionRefused
2026-01-05 17:49:41.540Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2026-01-05 17:49:41.548Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.549Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.549Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=93ms
2026-01-05 17:49:41.549Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.549Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.569Z debug(vsr): 2: prepare_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 2: prepare_timeout backing off
2026-01-05 17:49:41.569Z debug(vsr): 2: prepare_timeout after=7..3 (rtt=1 min=1 max=1000 attempts=3)
2026-01-05 17:49:41.569Z debug(replica): 2N: on_prepare_timeout: waiting for replica 0
2026-01-05 17:49:41.569Z debug(replica): 2N: on_prepare_timeout: waiting for replica 1
2026-01-05 17:49:41.569Z debug(replica): 2N: on_prepare_timeout: replicating to replica 1
2026-01-05 17:49:41.569Z debug(replica): 2N: sending prepare to replica 1: Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.569Z debug(vsr): 2: commit_message_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 2: commit_message_timeout reset
2026-01-05 17:49:41.569Z debug(replica): 2N: sending commit to replica 0: Commit{ .checksum=88e22568533da761b42dbe5c86411284, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.commit, .replica=2, .commit_checksum=d04003e88954be486c5d033509811f22, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .commit=133, .timestamp_monotonic=39755162394427533 }
2026-01-05 17:49:41.569Z warning(faulty_network): send error (2,9): error.BrokenPipe
2026-01-05 17:49:41.569Z debug(replica): 2N: sending commit to replica 1: Commit{ .checksum=88e22568533da761b42dbe5c86411284, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.commit, .replica=2, .commit_checksum=d04003e88954be486c5d033509811f22, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .commit=133, .timestamp_monotonic=39755162394427533 }
2026-01-05 17:49:41.569Z debug(vsr): 2: start_view_change_message_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 2: start_view_change_message_timeout reset
2026-01-05 17:49:41.569Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.569Z debug(vsr): 2: journal_repair_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 2: journal_repair_timeout reset
2026-01-05 17:49:41.569Z debug(replica): 0n: on_message: view=2 status=normal Commit{ .checksum=88e22568533da761b42dbe5c86411284, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.commit, .replica=2, .commit_checksum=d04003e88954be486c5d033509811f22, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .commit=133, .timestamp_monotonic=39755162394427533 }
2026-01-05 17:49:41.569Z debug(vsr): 0: normal_heartbeat_timeout reset
2026-01-05 17:49:41.569Z debug(replica): 0n: on_commit: checksum verified
2026-01-05 17:49:41.569Z debug(vsr): 2: grid_repair_budget_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 2: grid_repair_budget_timeout reset
2026-01-05 17:49:41.569Z warning(message_bus): 2: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2026-01-05 17:49:41.569Z debug(vsr): 0: ping_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 0: ping_timeout reset
2026-01-05 17:49:41.569Z debug(replica): 0n: sending ping to replica 1: Ping{ .checksum=ac4473bca386f5e6504337dcd08b7670, .checksum_body=74730330cb75ecd4a42c51968e55e4bd, .cluster=0, .size=512, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.ping, .replica=0, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .ping_timestamp_monotonic=39755162394828346, .release_count=1, .route=0 }
2026-01-05 17:49:41.569Z debug(replica): 0n: sending ping to replica 2: Ping{ .checksum=ac4473bca386f5e6504337dcd08b7670, .checksum_body=74730330cb75ecd4a42c51968e55e4bd, .cluster=0, .size=512, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.ping, .replica=0, .checkpoint_id=f222e9ce156b309eaeb4af665242ac18, .checkpoint_op=0, .ping_timestamp_monotonic=39755162394828346, .release_count=1, .route=0 }
2026-01-05 17:49:41.569Z debug(vsr): 0: start_view_change_message_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 0: start_view_change_message_timeout reset
2026-01-05 17:49:41.569Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.569Z debug(vsr): 0: journal_repair_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 0: journal_repair_timeout reset
2026-01-05 17:49:41.569Z debug(vsr): 0: repair_sync_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 0: repair_sync_timeout reset
2026-01-05 17:49:41.569Z debug(vsr): 0: grid_repair_budget_timeout fired
2026-01-05 17:49:41.569Z debug(vsr): 0: grid_repair_budget_timeout reset
2026-01-05 17:49:41.582Z debug(replica): 0n: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.583Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2026-01-05 17:49:41.583Z debug(replica): 0n: sending request to replica 2: Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.589Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.589Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.589Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.589Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.599Z debug(vsr): 2: prepare_timeout fired
2026-01-05 17:49:41.599Z debug(vsr): 2: prepare_timeout backing off
2026-01-05 17:49:41.599Z debug(vsr): 2: prepare_timeout after=3..6 (rtt=1 min=1 max=1000 attempts=4)
2026-01-05 17:49:41.599Z debug(replica): 2N: on_prepare_timeout: waiting for replica 0
2026-01-05 17:49:41.599Z debug(replica): 2N: on_prepare_timeout: waiting for replica 1
2026-01-05 17:49:41.599Z debug(replica): 2N: on_prepare_timeout: replicating to replica 0
2026-01-05 17:49:41.599Z debug(replica): 2N: sending prepare to replica 0: Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.599Z debug(replica): 0n: on_message: view=2 status=normal Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.599Z warning(replica): 0n: on_prepare: not replicating op=134 commit_min=133 present=true
2026-01-05 17:49:41.599Z debug(replica): 0n: on_prepare: ignoring (repair)
2026-01-05 17:49:41.599Z debug(replica): 0n: on_repair: ignoring (duplicate)
2026-01-05 17:49:41.599Z debug(replica): 0n: send_prepare_ok: op=134 checksum=61311b9a1011380cdcde0f807e8dc2f1
2026-01-05 17:49:41.599Z debug(replica): 0n: sending prepare_ok to replica 2: PrepareOk{ .checksum=9f8529ae5e8fee01c41f62b123eb6888, .checksum_body=49f174618255402de6e7e3c40d60cc83, .cluster=0, .size=256, .epoch=0, .view=2, .release=0.0.0, .protocol=0, .command=vsr.Command.prepare_ok, .replica=0, .parent=d04003e88954be486c5d033509811f22, .prepare_checksum=61311b9a1011380cdcde0f807e8dc2f1, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit_min=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.601Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.601Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:41.601Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:41.601Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.601Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:41.601Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:41.601Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.601Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:41.601Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:41.601Z debug(replica): 2N: on_message: view=2 status=normal Request{ .checksum=3ba6709163b4028495cf3d8ddb13784f, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.request, .replica=0, .parent=e09ad7abf9dad9c0427d01d38aa6db78, .client=89448119447425700321927633112927827628, .session=2, .timestamp=0, .request=132, .operation=vsr.Operation(140), .previous_request_latency=22255810 }
2026-01-05 17:49:41.601Z debug(replica): 2N: on_request: new request
2026-01-05 17:49:41.601Z debug(replica): 2N: on_request: ignoring (already preparing)
2026-01-05 17:49:41.609Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.609Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.610Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.610Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.616Z warning(faulty_network): connect failed (1,8): error.ConnectionRefused
2026-01-05 17:49:41.619Z debug(vsr): 2: pulse_timeout fired
2026-01-05 17:49:41.619Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:41.629Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.629Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.630Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.630Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.634Z debug(replica): 1r: init: replica_count=3 quorum_view_change=2 quorum_replication=2 release=0.0.1
2026-01-05 17:49:41.634Z info(replica): superblock release=0.0.1
2026-01-05 17:49:41.634Z debug(journal): 1: recover: recovering
2026-01-05 17:49:41.634Z debug(journal): 1: recover_headers: offset=0 size=262144 recovering
2026-01-05 17:49:41.634Z debug(journal): 1: recover_headers: offset=0 size=262144 recovered
2026-01-05 17:49:41.634Z debug(journal): 1: recover_headers: complete
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=0
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=1
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=2
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=3
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=4
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=5
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=6
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=7
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=8
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=9
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=10
2026-01-05 17:49:41.634Z debug(journal): 1: recover_prepare: recovering slot=11
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=12
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=13
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=14
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=15
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=16
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=17
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=18
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=19
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=20
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=21
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=22
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=23
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=24
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=25
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=26
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=27
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=28
2026-01-05 17:49:41.635Z debug(journal): 1: recover_prepare: recovering slot=29
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=30
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=31
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=32
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=33
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=34
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=35
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=36
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=37
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=38
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=39
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=40
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=41
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=42
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=43
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=44
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=45
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=46
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=47
2026-01-05 17:49:41.636Z debug(journal): 1: recover_prepare: recovering slot=48
2026-01-05 17:49:41.637Z debug(journal): 1: recover_prepare: recovering slot=49
2026-01-05 17:49:41.637Z debug(journal): 1: recover_prepare: recovering slot=50
2026-01-05 17:49:41.637Z debug(journal): 1: recover_prepare: recovering slot=51
2026-01-05 17:49:41.638Z debug(journal): 1: recover_prepare: recovering slot=52
2026-01-05 17:49:41.638Z debug(journal): 1: recover_prepare: recovering slot=53
2026-01-05 17:49:41.638Z debug(journal): 1: recover_prepare: recovering slot=54
2026-01-05 17:49:41.638Z debug(journal): 1: recover_prepare: recovering slot=55
2026-01-05 17:49:41.638Z debug(journal): 1: recover_prepare: recovering slot=56
2026-01-05 17:49:41.638Z debug(journal): 1: recover_prepare: recovering slot=57
2026-01-05 17:49:41.638Z debug(journal): 1: recover_prepare: recovering slot=58
2026-01-05 17:49:41.639Z debug(journal): 1: recover_prepare: recovering slot=59
2026-01-05 17:49:41.642Z debug(journal): 1: recover_prepare: recovering slot=60
2026-01-05 17:49:41.642Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2026-01-05 17:49:41.642Z debug(journal): 1: recover_prepare: recovering slot=61
2026-01-05 17:49:41.642Z info(message_bus): 0: on_connect: connected to=1
2026-01-05 17:49:41.642Z debug(journal): 1: recover_prepare: recovering slot=62
thread 1 panic: reached unreachable code
2026-01-05 17:49:41.642Z debug(journal): 1: recover_prepare: recovering slot=63
2026-01-05 17:49:41.643Z debug(journal): 1: recover_prepare: recovering slot=64
2026-01-05 17:49:41.643Z debug(journal): 1: recover_prepare: recovering slot=65
2026-01-05 17:49:41.643Z debug(journal): 1: recover_prepare: recovering slot=66
2026-01-05 17:49:41.643Z debug(journal): 1: recover_prepare: recovering slot=67
2026-01-05 17:49:41.644Z debug(journal): 1: recover_prepare: recovering slot=68
2026-01-05 17:49:41.646Z debug(journal): 1: recover_prepare: recovering slot=69
2026-01-05 17:49:41.646Z debug(journal): 1: recover_prepare: recovering slot=70
2026-01-05 17:49:41.646Z debug(journal): 1: recover_prepare: recovering slot=71
2026-01-05 17:49:41.649Z debug(journal): 1: recover_prepare: recovering slot=72
2026-01-05 17:49:41.649Z debug(journal): 1: recover_prepare: recovering slot=73
2026-01-05 17:49:41.649Z debug(journal): 1: recover_prepare: recovering slot=74
2026-01-05 17:49:41.649Z debug(journal): 1: recover_prepare: recovering slot=75
2026-01-05 17:49:41.650Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.650Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.650Z debug(journal): 1: recover_prepare: recovering slot=76
2026-01-05 17:49:41.650Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.650Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.653Z debug(journal): 1: recover_prepare: recovering slot=77
2026-01-05 17:49:41.653Z debug(journal): 1: recover_prepare: recovering slot=78
2026-01-05 17:49:41.653Z debug(journal): 1: recover_prepare: recovering slot=79
2026-01-05 17:49:41.654Z debug(journal): 1: recover_prepare: recovering slot=80
2026-01-05 17:49:41.654Z debug(journal): 1: recover_prepare: recovering slot=81
2026-01-05 17:49:41.654Z debug(journal): 1: recover_prepare: recovering slot=82
2026-01-05 17:49:41.654Z debug(journal): 1: recover_prepare: recovering slot=83
2026-01-05 17:49:41.654Z debug(journal): 1: recover_prepare: recovering slot=84
2026-01-05 17:49:41.655Z debug(journal): 1: recover_prepare: recovering slot=85
2026-01-05 17:49:41.655Z debug(journal): 1: recover_prepare: recovering slot=86
2026-01-05 17:49:41.655Z debug(journal): 1: recover_prepare: recovering slot=87
2026-01-05 17:49:41.655Z debug(journal): 1: recover_prepare: recovering slot=88
2026-01-05 17:49:41.655Z debug(journal): 1: recover_prepare: recovering slot=89
2026-01-05 17:49:41.655Z debug(journal): 1: recover_prepare: recovering slot=90
2026-01-05 17:49:41.657Z debug(journal): 1: recover_prepare: recovering slot=91
2026-01-05 17:49:41.659Z debug(journal): 1: recover_prepare: recovering slot=92
2026-01-05 17:49:41.660Z debug(vsr): 2: prepare_timeout fired
2026-01-05 17:49:41.660Z debug(vsr): 2: prepare_timeout backing off
2026-01-05 17:49:41.660Z debug(vsr): 2: prepare_timeout after=6..30 (rtt=1 min=1 max=1000 attempts=5)
2026-01-05 17:49:41.660Z debug(replica): 2N: on_prepare_timeout: waiting for replica 0
2026-01-05 17:49:41.660Z debug(replica): 2N: on_prepare_timeout: waiting for replica 1
2026-01-05 17:49:41.660Z debug(replica): 2N: on_prepare_timeout: replicating to replica 1
2026-01-05 17:49:41.660Z debug(replica): 2N: sending prepare to replica 1: Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.660Z debug(message_bus): 2: send_message_to_replica: no connection to=1 header=Prepare{ .checksum=61311b9a1011380cdcde0f807e8dc2f1, .checksum_body=d9180124dcb5fe47f0f7a83d667a1620, .cluster=0, .size=1008, .epoch=0, .view=2, .release=0.0.1, .protocol=0, .command=vsr.Command.prepare, .replica=2, .parent=d04003e88954be486c5d033509811f22, .request_checksum=3ba6709163b4028495cf3d8ddb13784f, .checkpoint_id=00000000000000000000000000000000, .client=89448119447425700321927633112927827628, .op=134, .commit=133, .timestamp=1767635379263850366, .request=132, .operation=vsr.Operation(140) }
2026-01-05 17:49:41.660Z debug(journal): 1: recover_prepare: recovering slot=93
2026-01-05 17:49:41.660Z debug(journal): 1: recover_prepare: recovering slot=94
2026-01-05 17:49:41.661Z debug(journal): 1: recover_prepare: recovering slot=95
2026-01-05 17:49:41.661Z debug(journal): 1: recover_prepare: recovering slot=96
2026-01-05 17:49:41.661Z debug(journal): 1: recover_prepare: recovering slot=97
2026-01-05 17:49:41.661Z debug(journal): 1: recover_prepare: recovering slot=98
2026-01-05 17:49:41.662Z debug(journal): 1: recover_prepare: recovering slot=99
2026-01-05 17:49:41.664Z debug(journal): 1: recover_prepare: recovering slot=100
2026-01-05 17:49:41.667Z debug(journal): 1: recover_prepare: recovering slot=101
2026-01-05 17:49:41.668Z debug(journal): 1: recover_prepare: recovering slot=102
2026-01-05 17:49:41.668Z debug(journal): 1: recover_prepare: recovering slot=103
2026-01-05 17:49:41.670Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.670Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:41.670Z debug(vsr): 2: journal_repair_timeout fired
2026-01-05 17:49:41.670Z debug(vsr): 2: journal_repair_timeout reset
2026-01-05 17:49:41.670Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:41.670Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:41.670Z debug(vsr): 0: journal_repair_timeout fired
2026-01-05 17:49:41.670Z debug(vsr): 0: journal_repair_timeout reset
2026-01-05 17:49:41.670Z debug(journal): 1: recover_prepare: recovering slot=104
2026-01-05 17:49:41.670Z debug(journal): 1: recover_prepare: recovering slot=105
2026-01-05 17:49:41.671Z debug(journal): 1: recover_prepare: recovering slot=106
2026-01-05 17:49:41.671Z debug(journal): 1: recover_prepare: recovering slot=107
2026-01-05 17:49:41.671Z debug(journal): 1: recover_prepare: recovering slot=108
2026-01-05 17:49:41.673Z debug(journal): 1: recover_prepare: recovering slot=109
/root/tigerbeetle/zig/lib/std/debug.zig:550:14: 0x1244e5d in 2026-01-05 17:49:41.673Z debug(journal): 1: recover_prepare: recovering slot=110
2026-01-05 17:49:41.690Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:41.690Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:43.793Z debug(vsr): 2: journal_repair_budget_timeout reset
assert2026-01-05 17:49:43.793Z debug(vsr): 0: journal_repair_budget_timeout reset
 (vortex)
2026-01-05 17:49:43.793Z debug(journal): 1: recover_prepare: recovering slot=111
    if (!ok) unreachable; // assertion failure
             ^
/root/tigerbeetle/working/main/src/list.zig:70:41: 0x1333087 in push (vortex)
            if (constants.verify) assert(!list.contains(node));
                                        ^
/root/tigerbeetle/working/main/src/io/linux.zig:282:27: 0x1305075 in enqueue (vortex)
        self.awaiting.push(completion);
                          ^
/root/tigerbeetle/working/main/src/io/linux.zig:1182:21: 0x13a616f in send__anon_51642 (vortex)
        self.enqueue(completion);
                    ^
2026-01-05 17:49:43.794Z debug(journal): 1: recover_prepare: recovering slot=112
/root/tigerbeetle/working/main/src/testing/vortex/faulty_network.zig:204:21: 0x13a5f48 in send (vortex)
        pipe.io.send(
                    ^
/root/tigerbeetle/working/main/src/testing/vortex/faulty_network.zig:187:22: 0x13a5ac5 in on_recv (vortex)
            pipe.send(pipe.buffer[pipe.send_count..pipe.recv_count]);
                     ^
/root/tigerbeetle/working/main/src/io/linux.zig:1856:25: 0x13a4d89 in erased (vortex)
                callback(ctx, completion, result.*);
                        ^
/root/tigerbeetle/working/main/src/io/linux.zig:700:40: 0x13066e3 in complete (vortex)
                    completion.callback(completion.context, completion, &result);
                                       ^
/root/tigerbeetle/working/main/src/io/linux.zig:194:49: 0x1304dd0 in flush (vortex)
                .inactive => completion.complete(),
                                                ^
/root/tigerbeetle/working/main/src/io/linux.zig:149:27: 0x13074e6 in run_for_ns (vortex)
            try self.flush(1, &timeouts, &etime);
                          ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:264:41: 0x1307ee0 in run (vortex)
            try supervisor.io.run_for_ns(constants.vsr.tick_ms * std.time.ns_per_ms);
                                        ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:208:23: 0x130c94b in main (vortex)
    try supervisor.run();
                      ^
/root/tigerbeetle/working/main/src/vortex.zig:61:61: 0x1321a54 in main (vortex)
        .supervisor => |supervisor_args| try Supervisor.main(allocator, supervisor_args),
                                                            ^
/root/tigerbeetle/zig/lib/std/start.zig:660:37: 0x1322527 in main (vortex)
            const result = root.main() catch |err| {
                                    ^
/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c:95:7: 0x152ffb8 in libc_start_main_stage2 (/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c)
 exit(main(argc, argv, envp));
      ^
Unwind error at address `exe:0x152ffb8` (error.AddressOutOfRange), trace may be incomplete

2026-01-05 17:49:43.802Z debug(journal): 1: recover_prepare: recovering slot=113
2026-01-05 17:49:43.810Z debug(journal): 1: recover_prepare: recovering slot=114
2026-01-05 17:49:43.810Z debug(journal): 1: recover_prepare: recovering slot=115
2026-01-05 17:49:43.811Z debug(journal): 1: recover_prepare: recovering slot=116
2026-01-05 17:49:43.813Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:43.813Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:43.813Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:43.813Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:43.814Z debug(journal): 1: recover_prepare: recovering slot=117
2026-01-05 17:49:43.816Z debug(journal): 1: recover_prepare: recovering slot=118
2026-01-05 17:49:43.816Z debug(journal): 1: recover_prepare: recovering slot=119
2026-01-05 17:49:43.816Z debug(journal): 1: recover_prepare: recovering slot=120
2026-01-05 17:49:43.821Z debug(journal): 1: recover_prepare: recovering slot=121
2026-01-05 17:49:43.821Z debug(journal): 1: recover_prepare: recovering slot=122
2026-01-05 17:49:43.821Z debug(journal): 1: recover_prepare: recovering slot=123
2026-01-05 17:49:43.821Z debug(journal): 1: recover_prepare: recovering slot=124
2026-01-05 17:49:43.822Z debug(journal): 1: recover_prepare: recovering slot=125
2026-01-05 17:49:43.822Z debug(journal): 1: recover_prepare: recovering slot=126
2026-01-05 17:49:43.823Z debug(journal): 1: recover_prepare: recovering slot=127
2026-01-05 17:49:43.823Z debug(vsr): 2: pulse_timeout fired
2026-01-05 17:49:43.823Z debug(vsr): 2: pulse_timeout reset
2026-01-05 17:49:43.825Z debug(journal): 1: recover_prepare: recovering slot=128
2026-01-05 17:49:43.826Z debug(journal): 1: recover_prepare: recovering slot=129
2026-01-05 17:49:43.826Z debug(journal): 1: recover_prepare: recovering slot=130
2026-01-05 17:49:43.826Z debug(journal): 1: recover_prepare: recovering slot=131
2026-01-05 17:49:43.827Z debug(journal): 1: recover_prepare: recovering slot=132
2026-01-05 17:49:43.830Z debug(journal): 1: recover_prepare: recovering slot=133
2026-01-05 17:49:43.830Z debug(journal): 1: recover_prepare: recovering slot=134
2026-01-05 17:49:43.831Z debug(journal): 1: recover_prepare: recovering slot=135
2026-01-05 17:49:43.831Z debug(journal): 1: recover_prepare: recovering slot=136
2026-01-05 17:49:43.831Z debug(journal): 1: recover_prepare: recovering slot=137
2026-01-05 17:49:43.831Z debug(journal): 1: recover_prepare: recovering slot=138
2026-01-05 17:49:43.833Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:43.833Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:43.833Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:43.833Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:43.835Z debug(journal): 1: recover_prepare: recovering slot=139
2026-01-05 17:49:43.839Z debug(journal): 1: recover_prepare: recovering slot=140
2026-01-05 17:49:43.839Z debug(journal): 1: recover_prepare: recovering slot=141
2026-01-05 17:49:43.839Z debug(journal): 1: recover_prepare: recovering slot=142
2026-01-05 17:49:43.839Z debug(journal): 1: recover_prepare: recovering slot=143
2026-01-05 17:49:43.840Z debug(journal): 1: recover_prepare: recovering slot=144
2026-01-05 17:49:43.840Z debug(journal): 1: recover_prepare: recovering slot=145
2026-01-05 17:49:43.840Z debug(journal): 1: recover_prepare: recovering slot=146
2026-01-05 17:49:43.840Z debug(journal): 1: recover_prepare: recovering slot=147
2026-01-05 17:49:43.840Z debug(journal): 1: recover_prepare: recovering slot=148
2026-01-05 17:49:43.840Z debug(journal): 1: recover_prepare: recovering slot=149
2026-01-05 17:49:43.840Z debug(journal): 1: recover_prepare: recovering slot=150
2026-01-05 17:49:43.840Z debug(journal): 1: recover_prepare: recovering slot=151
2026-01-05 17:49:43.840Z debug(journal): 1: recover_prepare: recovering slot=152
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=153
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=154
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=155
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=156
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=157
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=158
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=159
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=160
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=161
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=162
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=163
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=164
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=165
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=166
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=167
2026-01-05 17:49:43.841Z debug(journal): 1: recover_prepare: recovering slot=168
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=169
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=170
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=171
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=172
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=173
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=174
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=175
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=176
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=177
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=178
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=179
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=180
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=181
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=182
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=183
2026-01-05 17:49:43.842Z debug(journal): 1: recover_prepare: recovering slot=184
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=185
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=186
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=187
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=188
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=189
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=190
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=191
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=192
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=193
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=194
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=195
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=196
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=197
2026-01-05 17:49:43.843Z debug(journal): 1: recover_prepare: recovering slot=198
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=199
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=200
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=201
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=202
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=203
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=204
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=205
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=206
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=207
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=208
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=209
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=210
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=211
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=212
2026-01-05 17:49:43.844Z debug(journal): 1: recover_prepare: recovering slot=213
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=214
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=215
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=216
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=217
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=218
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=219
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=220
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=221
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=222
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=223
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=224
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=225
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=226
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=227
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=228
2026-01-05 17:49:43.845Z debug(journal): 1: recover_prepare: recovering slot=229
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=230
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=231
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=232
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=233
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=234
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=235
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=236
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=237
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=238
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=239
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=240
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=241
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=242
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=243
2026-01-05 17:49:43.846Z debug(journal): 1: recover_prepare: recovering slot=244
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=245
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=246
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=247
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=248
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=249
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=250
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=251
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=252
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=253
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=254
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=255
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=256
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=257
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=258
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=259
2026-01-05 17:49:43.847Z debug(journal): 1: recover_prepare: recovering slot=260
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=261
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=262
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=263
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=264
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=265
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=266
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=267
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=268
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=269
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=270
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=271
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=272
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=273
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=274
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=275
2026-01-05 17:49:43.848Z debug(journal): 1: recover_prepare: recovering slot=276
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=277
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=278
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=279
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=280
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=281
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=282
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=283
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=284
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=285
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=286
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=287
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=288
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=289
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=290
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=291
2026-01-05 17:49:43.849Z debug(journal): 1: recover_prepare: recovering slot=292
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=293
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=294
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=295
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=296
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=297
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=298
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=299
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=300
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=301
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=302
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=303
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=304
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=305
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=306
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=307
2026-01-05 17:49:43.850Z debug(journal): 1: recover_prepare: recovering slot=308
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=309
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=310
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=311
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=312
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=313
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=314
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=315
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=316
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=317
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=318
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=319
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=320
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=321
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=322
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=323
2026-01-05 17:49:43.851Z debug(journal): 1: recover_prepare: recovering slot=324
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=325
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=326
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=327
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=328
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=329
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=330
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=331
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=332
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=333
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=334
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=335
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=336
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=337
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=338
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=339
2026-01-05 17:49:43.852Z debug(journal): 1: recover_prepare: recovering slot=340
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=341
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=342
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=343
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=344
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=345
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=346
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=347
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=348
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=349
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=350
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=351
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=352
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=353
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=354
2026-01-05 17:49:43.853Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:43.853Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:43.853Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:43.853Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=355
2026-01-05 17:49:43.853Z debug(journal): 1: recover_prepare: recovering slot=356
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=357
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=358
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=359
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=360
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=361
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=362
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=363
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=364
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=365
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=366
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=367
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=368
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=369
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=370
2026-01-05 17:49:43.854Z debug(journal): 1: recover_prepare: recovering slot=371
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=372
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=373
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=374
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=375
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=376
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=377
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=378
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=379
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=380
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=381
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=382
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=383
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=384
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=385
2026-01-05 17:49:43.855Z debug(journal): 1: recover_prepare: recovering slot=386
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=387
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=388
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=389
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=390
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=391
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=392
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=393
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=394
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=395
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=396
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=397
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=398
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=399
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=400
2026-01-05 17:49:43.856Z debug(journal): 1: recover_prepare: recovering slot=401
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=402
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=403
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=404
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=405
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=406
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=407
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=408
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=409
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=410
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=411
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=412
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=413
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=414
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=415
2026-01-05 17:49:43.857Z debug(journal): 1: recover_prepare: recovering slot=416
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=417
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=418
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=419
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=420
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=421
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=422
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=423
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=424
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=425
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=426
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=427
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=428
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=429
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=430
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=431
2026-01-05 17:49:43.858Z debug(journal): 1: recover_prepare: recovering slot=432
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=433
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=434
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=435
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=436
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=437
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=438
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=439
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=440
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=441
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=442
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=443
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=444
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=445
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=446
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=447
2026-01-05 17:49:43.859Z debug(journal): 1: recover_prepare: recovering slot=448
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=449
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=450
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=451
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=452
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=453
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=454
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=455
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=456
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=457
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=458
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=459
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=460
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=461
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=462
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=463
2026-01-05 17:49:43.860Z debug(journal): 1: recover_prepare: recovering slot=464
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=465
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=466
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=467
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=468
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=469
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=470
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=471
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=472
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=473
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=474
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=475
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=476
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=477
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=478
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=479
2026-01-05 17:49:43.861Z debug(journal): 1: recover_prepare: recovering slot=480
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=481
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=482
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=483
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=484
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=485
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=486
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=487
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=488
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=489
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=490
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=491
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=492
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=493
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=494
2026-01-05 17:49:43.862Z debug(journal): 1: recover_prepare: recovering slot=495
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=496
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=497
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=498
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=499
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=500
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=501
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=502
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=503
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=504
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=505
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=506
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=507
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=508
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=509
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=510
2026-01-05 17:49:43.863Z debug(journal): 1: recover_prepare: recovering slot=511
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=512
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=513
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=514
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=515
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=516
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=517
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=518
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=519
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=520
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=521
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=522
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=523
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=524
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=525
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=526
2026-01-05 17:49:43.864Z debug(journal): 1: recover_prepare: recovering slot=527
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=528
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=529
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=530
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=531
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=532
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=533
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=534
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=535
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=536
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=537
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=538
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=539
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=540
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=541
2026-01-05 17:49:43.865Z debug(journal): 1: recover_prepare: recovering slot=542
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=543
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=544
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=545
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=546
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=547
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=548
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=549
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=550
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=551
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=552
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=553
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=554
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=555
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=556
2026-01-05 17:49:43.866Z debug(journal): 1: recover_prepare: recovering slot=557
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=558
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=559
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=560
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=561
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=562
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=563
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=564
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=565
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=566
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=567
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=568
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=569
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=570
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=571
2026-01-05 17:49:43.867Z debug(journal): 1: recover_prepare: recovering slot=572
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=573
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=574
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=575
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=576
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=577
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=578
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=579
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=580
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=581
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=582
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=583
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=584
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=585
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=586
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=587
2026-01-05 17:49:43.868Z debug(journal): 1: recover_prepare: recovering slot=588
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=589
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=590
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=591
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=592
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=593
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=594
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=595
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=596
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=597
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=598
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=599
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=600
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=601
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=602
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=603
2026-01-05 17:49:43.869Z debug(journal): 1: recover_prepare: recovering slot=604
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=605
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=606
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=607
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=608
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=609
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=610
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=611
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=612
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=613
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=614
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=615
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=616
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=617
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=618
2026-01-05 17:49:43.870Z debug(journal): 1: recover_prepare: recovering slot=619
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=620
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=621
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=622
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=623
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=624
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=625
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=626
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=627
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=628
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=629
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=630
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=631
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=632
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=633
2026-01-05 17:49:43.871Z debug(journal): 1: recover_prepare: recovering slot=634
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=635
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=636
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=637
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=638
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=639
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=640
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=641
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=642
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=643
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=644
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=645
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=646
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=647
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=648
2026-01-05 17:49:43.872Z debug(journal): 1: recover_prepare: recovering slot=649
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=650
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=651
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=652
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=653
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=654
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=655
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=656
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=657
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=658
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=659
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=660
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=661
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=662
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=663
2026-01-05 17:49:43.873Z debug(journal): 1: recover_prepare: recovering slot=664
2026-01-05 17:49:43.873Z debug(vsr): 2: journal_repair_budget_timeout fired
2026-01-05 17:49:43.873Z debug(vsr): 2: journal_repair_budget_timeout reset
2026-01-05 17:49:43.873Z debug(vsr): 0: journal_repair_budget_timeout fired
2026-01-05 17:49:43.873Z debug(vsr): 2: journal_repair_timeout fired
2026-01-05 17:49:43.873Z debug(vsr): 0: journal_repair_budget_timeout reset
2026-01-05 17:49:43.873Z debug(vsr): 2: journal_repair_timeout reset
2026-01-05 17:49:43.873Z debug(vsr): 0: journal_repair_timeout fired
2026-01-05 17:49:43.873Z debug(vsr): 0: journal_repair_timeout reset
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=665
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=666
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=667
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=668
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=669
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=670
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=671
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=672
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=673
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=674
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=675
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=676
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=677
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=678
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=679
2026-01-05 17:49:43.874Z debug(journal): 1: recover_prepare: recovering slot=680
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=681
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=682
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=683
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=684
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=685
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=686
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=687
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=688
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=689
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=690
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=691
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=692
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=693
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=694
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=695
2026-01-05 17:49:43.875Z debug(journal): 1: recover_prepare: recovering slot=696
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=697
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=698
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=699
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=700
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=701
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=702
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=703
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=704
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=705
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=706
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=707
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=708
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=709
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=710
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=711
2026-01-05 17:49:43.876Z debug(journal): 1: recover_prepare: recovering slot=712
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=713
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=714
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=715
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=716
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=717
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=718
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=719
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=720
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=721
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=722
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=723
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=724
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=725
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=726
2026-01-05 17:49:43.877Z debug(journal): 1: recover_prepare: recovering slot=727
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=728
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=729
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=730
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=731
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=732
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=733
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=734
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=735
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=736
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=737
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=738
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=739
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=740
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=741
2026-01-05 17:49:43.878Z debug(journal): 1: recover_prepare: recovering slot=742
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=743
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=744
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=745
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=746
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=747
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=748
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=749
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=750
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=751
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=752
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=753
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=754
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=755
2026-01-05 17:49:43.879Z debug(journal): 1: recover_prepare: recovering slot=756
2026-01-05 17:49:44.221Z info(unshare): sandboxed subprocesses exited with signal 11
