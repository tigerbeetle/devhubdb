se last_beat=false
2025-11-17 16:09:56.069Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.069Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.069Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 16:09:56.069Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.069Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.070Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.070Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.070Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 16:09:56.071Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.071Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.071Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 16:09:56.072Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.073Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.073Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 16:09:56.075Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.075Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-17 16:09:56.075Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.075Z debug(clock): 2: synchronized: truechimers=3/3 clock_offset=0ns..0ns accuracy=0ns
2025-11-17 16:09:56.075Z debug(clock): 2: system time is 20ns behind
2025-11-17 16:09:56.075Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.075Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.087Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=40960 len=4096 unlocked
2025-11-17 16:09:56.087Z debug(journal): 1: write: view=1 slot=165 op=165 len=315136: 224313515727840002064536701741190180850 complete, marking clean
2025-11-17 16:09:56.087Z debug(replica): 1N: send_prepare_ok: op=165 checksum=224313515727840002064536701741190180850
2025-11-17 16:09:56.087Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 186169451849258317992214566022422075411, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175241274850214268582761334226775935040, .parent_padding = 0, .prepare_checksum = 224313515727840002064536701741190180850, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 165, .commit_min = 164, .timestamp = 1763395795347549073, .request = 163, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.087Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 186169451849258317992214566022422075411, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175241274850214268582761334226775935040, .parent_padding = 0, .prepare_checksum = 224313515727840002064536701741190180850, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 165, .commit_min = 164, .timestamp = 1763395795347549073, .request = 163, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.087Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-17 16:09:56.087Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-17 16:09:56.087Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-17 16:09:56.089Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.089Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.094Z debug(client_replies): 2: write_reply: wrote (client=38222820279219011668827931571755163046 request=162)
2025-11-17 16:09:56.095Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.095Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.104Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.104Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.104Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 16:09:56.109Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.109Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.115Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.115Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.129Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.129Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.135Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.135Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.135Z debug(vsr): 2: journal_repair_timeout fired
2025-11-17 16:09:56.135Z debug(vsr): 2: journal_repair_timeout reset
2025-11-17 16:09:56.135Z debug(replica): 2n: repair_prepare: op=165 checksum=224313515727840002064536701741190180850 (already writing)
2025-11-17 16:09:56.136Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.136Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.136Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 16:09:56.136Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.136Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-17 16:09:56.136Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.137Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 313798409214692101476363464422496944867, .checksum_padding = 0, .checksum_body = 158449312630284780581167180964251756953, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 315136, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 296986985820952150755649102271678988844, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 163, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 350967204, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.137Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.137Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 16:09:56.143Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=173015040 len=315392 unlocked
2025-11-17 16:09:56.143Z debug(journal): 2: write_header: op=165 sectors[40960..45056]
2025-11-17 16:09:56.143Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=40960 len=4096 locked
2025-11-17 16:09:56.149Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.149Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.149Z debug(vsr): 1: journal_repair_timeout fired
2025-11-17 16:09:56.149Z debug(vsr): 1: journal_repair_timeout reset
2025-11-17 16:09:56.149Z debug(vsr): 1: pulse_timeout fired
2025-11-17 16:09:56.149Z debug(vsr): 1: pulse_timeout reset
2025-11-17 16:09:56.155Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.155Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.162Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=40960 len=4096 unlocked
2025-11-17 16:09:56.162Z debug(journal): 2: write: view=1 slot=165 op=165 len=315136: 224313515727840002064536701741190180850 complete, marking clean
2025-11-17 16:09:56.162Z debug(replica): 2n: send_prepare_ok: op=165 checksum=224313515727840002064536701741190180850
2025-11-17 16:09:56.162Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 108669710079317707092275755628052977416, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175241274850214268582761334226775935040, .parent_padding = 0, .prepare_checksum = 224313515727840002064536701741190180850, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 165, .commit_min = 164, .timestamp = 1763395795347549073, .request = 163, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.162Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 108669710079317707092275755628052977416, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175241274850214268582761334226775935040, .parent_padding = 0, .prepare_checksum = 224313515727840002064536701741190180850, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 165, .commit_min = 164, .timestamp = 1763395795347549073, .request = 163, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.162Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-17 16:09:56.162Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-17 16:09:56.162Z debug(replica): 1N: on_prepare_ok: quorum received, context=224313515727840002064536701741190180850
2025-11-17 16:09:56.162Z debug(vsr): 1: prepare_timeout stopped
2025-11-17 16:09:56.162Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-17 16:09:56.163Z debug(replica): 1N: execute_op: executing view=1 primary=true op=165 checksum=224313515727840002064536701741190180850 (create_transfers)
2025-11-17 16:09:56.163Z debug(replica): 1N: execute_op: commit_timestamp=1763395794741073910 prepare.header.timestamp=1763395795347549073
2025-11-17 16:09:56.169Z debug(replica): 1N: execute_op: advancing commit_max=164..165
2025-11-17 16:09:56.169Z debug(replica): 1N: client_table_entry_update: client=38222820279219011668827931571755163046 session=2 request=163
2025-11-17 16:09:56.169Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 230406101233757464397673346270817887593, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 313798409214692101476363464422496944867, .request_checksum_padding = 0, .context = 164875189400932205549770253155878366174, .context_padding = 0, .client = 38222820279219011668827931571755163046, .op = 165, .commit = 165, .timestamp = 1763395795347549073, .request = 163, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.169Z debug(replica): 1N: sending reply to client 38222820279219011668827931571755163046: vsr.message_header.Header.Reply{ .checksum = 230406101233757464397673346270817887593, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 313798409214692101476363464422496944867, .request_checksum_padding = 0, .context = 164875189400932205549770253155878366174, .context_padding = 0, .client = 38222820279219011668827931571755163046, .op = 165, .commit = 165, .timestamp = 1763395795347549073, .request = 163, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.169Z debug(forest): entering forest.compact() op=165 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 38222820279219011668827931571755163046: on_reply: slow request, request=163 op=165 size=315136 create_transfers time=1027ms
2025-11-17 16:09:56.175Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.175Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.182Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.182Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.189Z debug(client_replies): 1: write_reply: wrote (client=38222820279219011668827931571755163046 request=163)
2025-11-17 16:09:56.193Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 84936816845624647584083449719924927048, .checksum_padding = 0, .checksum_body = 141660017457507227746050632152457265715, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2192, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 164875189400932205549770253155878366174, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 164, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1027160558, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.193Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.193Z debug(replica): 1N: primary_pipeline_prepare: request checksum=84936816845624647584083449719924927048 client=38222820279219011668827931571755163046
2025-11-17 16:09:56.193Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=107158187545713947938426087186861067044 op=166
2025-11-17 16:09:56.193Z debug(vsr): 1: prepare_timeout started
2025-11-17 16:09:56.193Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-17 16:09:56.193Z debug(vsr): 1: pulse_timeout reset
2025-11-17 16:09:56.193Z debug(replica): 1N: replicate: replicating op=166 to replica 0
2025-11-17 16:09:56.193Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 107158187545713947938426087186861067044, .checksum_padding = 0, .checksum_body = 141660017457507227746050632152457265715, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2192, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 224313515727840002064536701741190180850, .parent_padding = 0, .request_checksum = 84936816845624647584083449719924927048, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit = 165, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.194Z debug(replica): 1N: replicate: replicating op=166 to replica 2
2025-11-17 16:09:56.194Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 107158187545713947938426087186861067044, .checksum_padding = 0, .checksum_body = 141660017457507227746050632152457265715, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2192, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 224313515727840002064536701741190180850, .parent_padding = 0, .request_checksum = 84936816845624647584083449719924927048, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit = 165, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.194Z debug(replica): 1N: on_prepare: advancing: op=165..166 checksum=224313515727840002064536701741190180850..107158187545713947938426087186861067044
2025-11-17 16:09:56.194Z debug(journal): 1: set_header_as_dirty: op=166 checksum=107158187545713947938426087186861067044
2025-11-17 16:09:56.194Z debug(replica): 1N: append: appending to journal op=166
2025-11-17 16:09:56.194Z debug(journal): 1: write: view=1 slot=166 op=166 len=2192: 107158187545713947938426087186861067044 starting
2025-11-17 16:09:56.194Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=174063616 len=4096 locked
2025-11-17 16:09:56.194Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 107158187545713947938426087186861067044, .checksum_padding = 0, .checksum_body = 141660017457507227746050632152457265715, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2192, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 224313515727840002064536701741190180850, .parent_padding = 0, .request_checksum = 84936816845624647584083449719924927048, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit = 165, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.194Z debug(replica): 2n: on_prepare: advancing commit_max=164..165
2025-11-17 16:09:56.194Z debug(replica): 2n: on_prepare: caching prepare.op=166 (commit_min=164 op=165 commit_max=165 prepare_max=1007)
2025-11-17 16:09:56.194Z debug(replica): 2n: on_prepare: advancing: op=165..166 checksum=224313515727840002064536701741190180850..107158187545713947938426087186861067044
2025-11-17 16:09:56.194Z debug(journal): 2: set_header_as_dirty: op=166 checksum=107158187545713947938426087186861067044
2025-11-17 16:09:56.194Z debug(replica): 2n: append: appending to journal op=166
2025-11-17 16:09:56.194Z debug(journal): 2: write: view=1 slot=166 op=166 len=2192: 107158187545713947938426087186861067044 starting
2025-11-17 16:09:56.194Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=174063616 len=4096 locked
2025-11-17 16:09:56.194Z debug(replica): 2n: commit_start_journal: cached prepare op=165 checksum=224313515727840002064536701741190180850
2025-11-17 16:09:56.194Z debug(replica): 2n: repair_prepare: op=166 checksum=107158187545713947938426087186861067044 (already writing)
2025-11-17 16:09:56.194Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=164)
2025-11-17 16:09:56.195Z debug(replica): 2n: execute_op: executing view=1 primary=false op=165 checksum=224313515727840002064536701741190180850 (create_transfers)
2025-11-17 16:09:56.195Z debug(replica): 2n: execute_op: commit_timestamp=1763395794741073910 prepare.header.timestamp=1763395795347549073
2025-11-17 16:09:56.201Z debug(replica): 2n: client_table_entry_update: client=38222820279219011668827931571755163046 session=2 request=163
2025-11-17 16:09:56.201Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 230406101233757464397673346270817887593, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 313798409214692101476363464422496944867, .request_checksum_padding = 0, .context = 164875189400932205549770253155878366174, .context_padding = 0, .client = 38222820279219011668827931571755163046, .op = 165, .commit = 165, .timestamp = 1763395795347549073, .request = 163, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.201Z debug(replica): 2n: sending reply to client 38222820279219011668827931571755163046: vsr.message_header.Header.Reply{ .checksum = 230406101233757464397673346270817887593, .checksum_padding = 0, .checksum_body = 311752944233308762869332694583075543730, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 264, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 313798409214692101476363464422496944867, .request_checksum_padding = 0, .context = 164875189400932205549770253155878366174, .context_padding = 0, .client = 38222820279219011668827931571755163046, .op = 165, .commit = 165, .timestamp = 1763395795347549073, .request = 163, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.201Z debug(forest): entering forest.compact() op=165 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-17 16:09:56.202Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.202Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.204Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.204Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.204Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=174063616 len=4096 unlocked
2025-11-17 16:09:56.204Z debug(journal): 1: write_header: op=166 sectors[40960..45056]
2025-11-17 16:09:56.204Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=40960 len=4096 locked
2025-11-17 16:09:56.215Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=174063616 len=4096 unlocked
2025-11-17 16:09:56.215Z debug(journal): 2: write_header: op=166 sectors[40960..45056]
2025-11-17 16:09:56.215Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=40960 len=4096 unlocked
2025-11-17 16:09:56.215Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=40960 len=4096 locked
2025-11-17 16:09:56.215Z debug(journal): 1: write: view=1 slot=166 op=166 len=2192: 107158187545713947938426087186861067044 complete, marking clean
2025-11-17 16:09:56.215Z debug(client_replies): 2: write_reply: wrote (client=38222820279219011668827931571755163046 request=163)
2025-11-17 16:09:56.215Z debug(replica): 1N: send_prepare_ok: op=166 checksum=107158187545713947938426087186861067044
2025-11-17 16:09:56.215Z debug(replica): 1N: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 155435326423474257504853603522858335115, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 224313515727840002064536701741190180850, .parent_padding = 0, .prepare_checksum = 107158187545713947938426087186861067044, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit_min = 165, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.215Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 155435326423474257504853603522858335115, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 224313515727840002064536701741190180850, .parent_padding = 0, .prepare_checksum = 107158187545713947938426087186861067044, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit_min = 165, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.215Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-17 16:09:56.215Z debug(replica): 1N: on_prepare_ok: 1 message(s)
2025-11-17 16:09:56.215Z debug(replica): 1N: on_prepare_ok: waiting for quorum
2025-11-17 16:09:56.222Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=40960 len=4096 unlocked
2025-11-17 16:09:56.222Z debug(journal): 2: write: view=1 slot=166 op=166 len=2192: 107158187545713947938426087186861067044 complete, marking clean
2025-11-17 16:09:56.222Z debug(replica): 2n: send_prepare_ok: op=166 checksum=107158187545713947938426087186861067044
2025-11-17 16:09:56.222Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 875938074347736619976569087790397310, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 224313515727840002064536701741190180850, .parent_padding = 0, .prepare_checksum = 107158187545713947938426087186861067044, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit_min = 165, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.222Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 875938074347736619976569087790397310, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 224313515727840002064536701741190180850, .parent_padding = 0, .prepare_checksum = 107158187545713947938426087186861067044, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit_min = 165, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.222Z debug(vsr): 1: primary_abdicate_timeout reset
2025-11-17 16:09:56.222Z debug(replica): 1N: on_prepare_ok: 2 message(s)
2025-11-17 16:09:56.222Z debug(replica): 1N: on_prepare_ok: quorum received, context=107158187545713947938426087186861067044
2025-11-17 16:09:56.222Z debug(vsr): 1: prepare_timeout stopped
2025-11-17 16:09:56.222Z debug(vsr): 1: primary_abdicate_timeout stopped
2025-11-17 16:09:56.223Z debug(replica): 1N: execute_op: executing view=1 primary=true op=166 checksum=107158187545713947938426087186861067044 (lookup_accounts)
2025-11-17 16:09:56.223Z debug(replica): 1N: execute_op: commit_timestamp=1763395795347549073 prepare.header.timestamp=1763395796193944756
2025-11-17 16:09:56.223Z debug(replica): 1N: execute_op: advancing commit_max=165..166
2025-11-17 16:09:56.223Z debug(replica): 1N: client_table_entry_update: client=38222820279219011668827931571755163046 session=2 request=164
2025-11-17 16:09:56.223Z debug(replica): 1N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 283311112265319374736621513790622764984, .checksum_padding = 0, .checksum_body = 174983667594469427189415514260945700989, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 15744, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 84936816845624647584083449719924927048, .request_checksum_padding = 0, .context = 35295834494444405622473191054186154992, .context_padding = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit = 166, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.223Z debug(replica): 1N: sending reply to client 38222820279219011668827931571755163046: vsr.message_header.Header.Reply{ .checksum = 283311112265319374736621513790622764984, .checksum_padding = 0, .checksum_body = 174983667594469427189415514260945700989, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 15744, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 84936816845624647584083449719924927048, .request_checksum_padding = 0, .context = 35295834494444405622473191054186154992, .context_padding = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit = 166, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.223Z debug(forest): entering forest.compact() op=166 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-17 16:09:56.223Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.223Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.224Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.224Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.224Z info(workload): accounts created = 120, transfers = 216629, pending transfers = 0, commands run = 82
2025-11-17 16:09:56.230Z debug(client_replies): 1: write_reply: wrote (client=38222820279219011668827931571755163046 request=164)
2025-11-17 16:09:56.237Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 257096930606452021112693195051389027227, .checksum_padding = 0, .checksum_body = 299271990817752029552945781221136067273, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 733824, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 35295834494444405622473191054186154992, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 165, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 30062018, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.237Z debug(replica): 2n: on_request: forwarding new request to primary (view=1)
2025-11-17 16:09:56.237Z debug(replica): 2n: sending request to replica 1: vsr.message_header.Header.Request{ .checksum = 257096930606452021112693195051389027227, .checksum_padding = 0, .checksum_body = 299271990817752029552945781221136067273, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 733824, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 35295834494444405622473191054186154992, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 165, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 30062018, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.237Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 257096930606452021112693195051389027227, .checksum_padding = 0, .checksum_body = 299271990817752029552945781221136067273, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 733824, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 35295834494444405622473191054186154992, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 165, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 30062018, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.237Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.237Z debug(replica): 1N: primary_pipeline_prepare: request checksum=257096930606452021112693195051389027227 client=38222820279219011668827931571755163046
2025-11-17 16:09:56.240Z debug(replica): 1N: primary_pipeline_prepare: prepare checksum=203056072289723568638096574421585023150 op=167
2025-11-17 16:09:56.240Z debug(vsr): 1: prepare_timeout started
2025-11-17 16:09:56.240Z debug(vsr): 1: primary_abdicate_timeout started
2025-11-17 16:09:56.240Z debug(vsr): 1: pulse_timeout reset
2025-11-17 16:09:56.240Z debug(replica): 1N: replicate: replicating op=167 to replica 0
2025-11-17 16:09:56.240Z debug(replica): 1N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 203056072289723568638096574421585023150, .checksum_padding = 0, .checksum_body = 299271990817752029552945781221136067273, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 733824, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107158187545713947938426087186861067044, .parent_padding = 0, .request_checksum = 257096930606452021112693195051389027227, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 167, .commit = 166, .timestamp = 1763395796237964818, .request = 165, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.240Z debug(replica): 1N: replicate: replicating op=167 to replica 2
2025-11-17 16:09:56.240Z debug(replica): 1N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 203056072289723568638096574421585023150, .checksum_padding = 0, .checksum_body = 299271990817752029552945781221136067273, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 733824, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107158187545713947938426087186861067044, .parent_padding = 0, .request_checksum = 257096930606452021112693195051389027227, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 167, .commit = 166, .timestamp = 1763395796237964818, .request = 165, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.240Z debug(replica): 1N: on_prepare: advancing: op=166..167 checksum=107158187545713947938426087186861067044..203056072289723568638096574421585023150
2025-11-17 16:09:56.240Z debug(journal): 1: set_header_as_dirty: op=167 checksum=203056072289723568638096574421585023150
2025-11-17 16:09:56.240Z debug(replica): 1N: append: appending to journal op=167
2025-11-17 16:09:56.240Z debug(journal): 1: write: view=1 slot=167 op=167 len=733824: 203056072289723568638096574421585023150 starting
2025-11-17 16:09:56.240Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=175112192 len=737280 locked
2025-11-17 16:09:56.243Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.243Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.244Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.244Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.244Z debug(vsr): 2: journal_repair_timeout fired
2025-11-17 16:09:56.244Z debug(vsr): 2: journal_repair_timeout reset
2025-11-17 16:09:56.263Z debug(vsr): 1: ping_timeout fired
2025-11-17 16:09:56.263Z debug(vsr): 1: ping_timeout reset
2025-11-17 16:09:56.263Z debug(replica): 1N: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 156837506616118668351310353213667839419, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 20654598350035890, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.263Z debug(replica): 1N: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 156837506616118668351310353213667839419, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 20654598350035890, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.263Z debug(vsr): 1: commit_message_timeout fired
2025-11-17 16:09:56.263Z debug(vsr): 1: commit_message_timeout reset
2025-11-17 16:09:56.263Z debug(replica): 1N: sending commit to replica 0: vsr.message_header.Header.Commit{ .checksum = 142125904489077295634109583042820280488, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 107158187545713947938426087186861067044, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 166, .timestamp_monotonic = 20654598350083549, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.263Z debug(replica): 1N: sending commit to replica 2: vsr.message_header.Header.Commit{ .checksum = 142125904489077295634109583042820280488, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 107158187545713947938426087186861067044, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 166, .timestamp_monotonic = 20654598350083549, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.264Z debug(vsr): 1: start_view_change_message_timeout fired
2025-11-17 16:09:56.264Z debug(vsr): 1: start_view_change_message_timeout reset
2025-11-17 16:09:56.264Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.264Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.264Z debug(vsr): 1: journal_repair_timeout fired
2025-11-17 16:09:56.264Z debug(vsr): 1: journal_repair_timeout reset
2025-11-17 16:09:56.264Z debug(replica): 1N: repair_prepare: op=167 checksum=203056072289723568638096574421585023150 (already writing)
2025-11-17 16:09:56.264Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-11-17 16:09:56.264Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-11-17 16:09:56.264Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.264Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.284Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.284Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.284Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.284Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.291Z warning(faulty_network): recv error (1,4): error.ConnectionResetByPeer
2025-11-17 16:09:56.292Z info(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 0 } orderly shutdown
2025-11-17 16:09:56.292Z warning(faulty_network): recv error (0,0): error.ConnectionResetByPeer
2025-11-17 16:09:56.292Z info(message_bus): 2: on_recv: from=vsr.Peer{ .replica = 0 } orderly shutdown
2025-11-17 16:09:56.292Z error(supervisor): 0: replica terminated unexpectedly with process.Child.Term{ .Signal = 9 }
2025-11-17 16:09:56.294Z debug(replica): 1N: on_message: view=1 status=normal vsr.message_header.Header.Request{ .checksum = 257096930606452021112693195051389027227, .checksum_padding = 0, .checksum_body = 299271990817752029552945781221136067273, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 733824, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 35295834494444405622473191054186154992, .parent_padding = 0, .client = 38222820279219011668827931571755163046, .session = 2, .timestamp = 0, .request = 165, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 30062018, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.294Z debug(replica): 1N: on_request: new request
2025-11-17 16:09:56.294Z debug(replica): 1N: on_request: ignoring (already preparing)
2025-11-17 16:09:56.294Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Prepare{ .checksum = 203056072289723568638096574421585023150, .checksum_padding = 0, .checksum_body = 299271990817752029552945781221136067273, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 733824, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107158187545713947938426087186861067044, .parent_padding = 0, .request_checksum = 257096930606452021112693195051389027227, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 167, .commit = 166, .timestamp = 1763395796237964818, .request = 165, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.294Z debug(replica): 2n: on_prepare: advancing commit_max=165..166
2025-11-17 16:09:56.294Z debug(replica): 2n: on_prepare: caching prepare.op=167 (commit_min=165 op=166 commit_max=166 prepare_max=1007)
2025-11-17 16:09:56.294Z debug(replica): 2n: on_prepare: advancing: op=166..167 checksum=107158187545713947938426087186861067044..203056072289723568638096574421585023150
2025-11-17 16:09:56.294Z debug(journal): 2: set_header_as_dirty: op=167 checksum=203056072289723568638096574421585023150
2025-11-17 16:09:56.294Z debug(replica): 2n: append: appending to journal op=167
2025-11-17 16:09:56.294Z debug(journal): 2: write: view=1 slot=167 op=167 len=733824: 203056072289723568638096574421585023150 starting
2025-11-17 16:09:56.294Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=175112192 len=737280 locked
2025-11-17 16:09:56.294Z debug(replica): 2n: commit_start_journal: cached prepare op=166 checksum=107158187545713947938426087186861067044
2025-11-17 16:09:56.294Z debug(replica): 2n: repair_prepare: op=167 checksum=203056072289723568638096574421585023150 (already writing)
2025-11-17 16:09:56.294Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=165)
2025-11-17 16:09:56.294Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Ping{ .checksum = 156837506616118668351310353213667839419, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 20654598350035890, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905728, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.304Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-17 16:09:56.317Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-17 16:09:56.317Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 89835933527799814932534145891339120682, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 20654598350035890, .pong_timestamp_wall = 1763395796317648546, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.317Z debug(replica): 2n: on_message: view=1 status=normal vsr.message_header.Header.Commit{ .checksum = 142125904489077295634109583042820280488, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 107158187545713947938426087186861067044, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 166, .timestamp_monotonic = 20654598350083549, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.317Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-11-17 16:09:56.317Z debug(replica): 2n: on_commit: checksum verified
2025-11-17 16:09:56.317Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=165)
2025-11-17 16:09:56.317Z debug(replica): 2n: execute_op: executing view=1 primary=false op=166 checksum=107158187545713947938426087186861067044 (lookup_accounts)
2025-11-17 16:09:56.317Z debug(replica): 2n: execute_op: commit_timestamp=1763395795347549073 prepare.header.timestamp=1763395796193944756
2025-11-17 16:09:56.317Z debug(replica): 2n: client_table_entry_update: client=38222820279219011668827931571755163046 session=2 request=164
2025-11-17 16:09:56.317Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 283311112265319374736621513790622764984, .checksum_padding = 0, .checksum_body = 174983667594469427189415514260945700989, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 15744, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 84936816845624647584083449719924927048, .request_checksum_padding = 0, .context = 35295834494444405622473191054186154992, .context_padding = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit = 166, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.317Z debug(replica): 2n: sending reply to client 38222820279219011668827931571755163046: vsr.message_header.Header.Reply{ .checksum = 283311112265319374736621513790622764984, .checksum_padding = 0, .checksum_body = 174983667594469427189415514260945700989, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 15744, .epoch = 0, .view = 1, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 84936816845624647584083449719924927048, .request_checksum_padding = 0, .context = 35295834494444405622473191054186154992, .context_padding = 0, .client = 38222820279219011668827931571755163046, .op = 166, .commit = 166, .timestamp = 1763395796193944756, .request = 164, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-17 16:09:56.319Z debug(forest): entering forest.compact() op=166 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-17 16:09:56.322Z info(supervisor): 1: terminating replica
2025-11-17 16:09:56.330Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.330Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.342Z debug(client_replies): 2: write_reply: wrote (client=38222820279219011668827931571755163046 request=164)
2025-11-17 16:09:56.350Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.350Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.370Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-17 16:09:56.370Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-17 16:09:56.370Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.370Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.370Z debug(vsr): 2: journal_repair_timeout fired
2025-11-17 16:09:56.370Z debug(vsr): 2: journal_repair_timeout reset
2025-11-17 16:09:56.370Z debug(replica): 2n: repair_prepare: op=167 checksum=203056072289723568638096574421585023150 (already writing)
2025-11-17 16:09:56.370Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-17 16:09:56.370Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-17 16:09:56.388Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=175112192 len=737280 unlocked
2025-11-17 16:09:56.388Z debug(journal): 2: write_header: op=167 sectors[40960..45056]
2025-11-17 16:09:56.388Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=40960 len=4096 locked
2025-11-17 16:09:56.390Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.390Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.393Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=40960 len=4096 unlocked
2025-11-17 16:09:56.393Z debug(journal): 2: write: view=1 slot=167 op=167 len=733824: 203056072289723568638096574421585023150 complete, marking clean
2025-11-17 16:09:56.393Z debug(replica): 2n: send_prepare_ok: op=167 checksum=203056072289723568638096574421585023150
2025-11-17 16:09:56.393Z debug(replica): 2n: sending prepare_ok to replica 1: vsr.message_header.Header.PrepareOk{ .checksum = 33400808384382760894640234697119279175, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 1, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 107158187545713947938426087186861067044, .parent_padding = 0, .prepare_checksum = 203056072289723568638096574421585023150, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 38222820279219011668827931571755163046, .op = 167, .commit_min = 166, .timestamp = 1763395796237964818, .request = 165, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-17 16:09:56.410Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.410Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.430Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.430Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.450Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.450Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.470Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.470Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.470Z debug(vsr): 2: journal_repair_timeout fired
2025-11-17 16:09:56.470Z debug(vsr): 2: journal_repair_timeout reset
2025-11-17 16:09:56.490Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.490Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.510Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-17 16:09:56.510Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-17 16:09:56.517Z info(supervisor): 2: terminating replica
2025-11-17 16:09:57.006Z error: TestFailed
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:425:25: 0x1308fdf in run (vortex)
                        return error.TestFailed;
                        ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:207:5: 0x130c881 in main (vortex)
    try supervisor.run();
    ^
/root/tigerbeetle/working/main/src/vortex.zig:61:42: 0x13214e4 in main (vortex)
        .supervisor => |supervisor_args| try Supervisor.main(allocator, supervisor_args),
                                         ^
warning(message_bus): 38222820279219011668827931571755163046: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
warning(message_bus): 38222820279219011668827931571755163046: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
