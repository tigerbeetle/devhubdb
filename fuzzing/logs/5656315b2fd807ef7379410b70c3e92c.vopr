694972, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:41.445Z debug(replica): 0N: primary_pipeline_prepare: request checksum=250642225106363936751539469967989535048 client=323321802946825702132615013286692535219
2025-11-24 19:42:41.445Z debug(replica): 0N: primary_pipeline_prepare: prepare checksum=192257984292774967620877459702865073863 op=180
2025-11-24 19:42:41.445Z debug(vsr): 0: prepare_timeout started
2025-11-24 19:42:41.445Z debug(vsr): 0: primary_abdicate_timeout started
2025-11-24 19:42:41.445Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:41.445Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 250642225106363936751539469967989535048, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 78229259269725858827546093461364404336, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 178, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2728694972, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(replica): 0N: replicate: replicating op=180 to replica 1
2025-11-24 19:42:41.445Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 19:42:41.445Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 192257984292774967620877459702865073863, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .request_checksum = 250642225106363936751539469967989535048, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 250642225106363936751539469967989535048, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 78229259269725858827546093461364404336, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 178, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2728694972, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(replica): 0N: replicate: replicating op=180 to replica 2
2025-11-24 19:42:41.445Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 192257984292774967620877459702865073863, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .request_checksum = 250642225106363936751539469967989535048, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(replica): 0N: on_prepare: advancing: op=179..180 checksum=175335667413114181383867159288548171492..192257984292774967620877459702865073863
2025-11-24 19:42:41.445Z debug(journal): 0: set_header_as_dirty: op=180 checksum=192257984292774967620877459702865073863
2025-11-24 19:42:41.445Z debug(replica): 0N: append: appending to journal op=180
2025-11-24 19:42:41.445Z debug(journal): 0: write: view=3 slot=180 op=180 len=2320: 192257984292774967620877459702865073863 starting
2025-11-24 19:42:41.445Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=188743680 len=4096 locked
2025-11-24 19:42:41.445Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 192257984292774967620877459702865073863, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .request_checksum = 250642225106363936751539469967989535048, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(replica): 1n: on_prepare: advancing commit_max=178..179
2025-11-24 19:42:41.445Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 192257984292774967620877459702865073863, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .request_checksum = 250642225106363936751539469967989535048, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(replica): 1n: on_prepare: caching prepare.op=180 (commit_min=178 op=179 commit_max=179 prepare_max=1007)
2025-11-24 19:42:41.445Z debug(replica): 2n: on_prepare: advancing commit_max=178..179
2025-11-24 19:42:41.445Z debug(replica): 2n: on_prepare: caching prepare.op=180 (commit_min=178 op=179 commit_max=179 prepare_max=1007)
2025-11-24 19:42:41.445Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 250642225106363936751539469967989535048, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 78229259269725858827546093461364404336, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 178, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2728694972, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:41.445Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 19:42:41.445Z debug(replica): 2n: on_prepare: advancing: op=179..180 checksum=175335667413114181383867159288548171492..192257984292774967620877459702865073863
2025-11-24 19:42:41.445Z debug(replica): 1n: on_prepare: advancing: op=179..180 checksum=175335667413114181383867159288548171492..192257984292774967620877459702865073863
2025-11-24 19:42:41.445Z debug(journal): 2: set_header_as_dirty: op=180 checksum=192257984292774967620877459702865073863
2025-11-24 19:42:41.445Z debug(replica): 2n: append: appending to journal op=180
2025-11-24 19:42:41.445Z debug(journal): 1: set_header_as_dirty: op=180 checksum=192257984292774967620877459702865073863
2025-11-24 19:42:41.445Z debug(replica): 1n: append: appending to journal op=180
2025-11-24 19:42:41.445Z debug(journal): 2: write: view=3 slot=180 op=180 len=2320: 192257984292774967620877459702865073863 starting
2025-11-24 19:42:41.445Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=188743680 len=4096 unlocked
2025-11-24 19:42:41.445Z debug(journal): 0: write_header: op=180 sectors[45056..49152]
2025-11-24 19:42:41.445Z debug(journal): 1: write: view=3 slot=180 op=180 len=2320: 192257984292774967620877459702865073863 starting
2025-11-24 19:42:41.445Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=188743680 len=4096 locked
2025-11-24 19:42:41.445Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:41.445Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=188743680 len=4096 locked
2025-11-24 19:42:41.445Z debug(replica): 2n: commit_start_journal: cached prepare op=179 checksum=175335667413114181383867159288548171492
2025-11-24 19:42:41.445Z debug(replica): 1n: commit_start_journal: cached prepare op=179 checksum=175335667413114181383867159288548171492
2025-11-24 19:42:41.445Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:41.445Z debug(journal): 0: write: view=3 slot=180 op=180 len=2320: 192257984292774967620877459702865073863 complete, marking clean
2025-11-24 19:42:41.445Z debug(replica): 0N: send_prepare_ok: op=180 checksum=192257984292774967620877459702865073863
2025-11-24 19:42:41.445Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 47732077581652033821134555890600758785, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .prepare_checksum = 192257984292774967620877459702865073863, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit_min = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 47732077581652033821134555890600758785, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .prepare_checksum = 192257984292774967620877459702865073863, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit_min = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.445Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:41.445Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 19:42:41.445Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 19:42:41.446Z debug(replica): 2n: repair_prepare: op=180 checksum=192257984292774967620877459702865073863 (already writing)
2025-11-24 19:42:41.446Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=178)
2025-11-24 19:42:41.446Z debug(replica): 1n: repair_prepare: op=180 checksum=192257984292774967620877459702865073863 (already writing)
2025-11-24 19:42:41.446Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=178)
2025-11-24 19:42:41.448Z debug(replica): 2n: execute_op: executing view=3 primary=false op=179 checksum=175335667413114181383867159288548171492 (create_transfers)
2025-11-24 19:42:41.448Z debug(replica): 2n: execute_op: commit_timestamp=1764013358665712943 prepare.header.timestamp=1764013358702488113
2025-11-24 19:42:41.448Z debug(replica): 1n: execute_op: executing view=3 primary=false op=179 checksum=175335667413114181383867159288548171492 (create_transfers)
2025-11-24 19:42:41.448Z debug(replica): 1n: execute_op: commit_timestamp=1764013358665712943 prepare.header.timestamp=1764013358702488113
2025-11-24 19:42:41.460Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:41.460Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:41.469Z debug(replica): 2n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=177
2025-11-24 19:42:41.469Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 287701050567195852176004730441229480825, .checksum_padding = 0, .checksum_body = 317130472969479645741804541105897845387, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 330619327999152601426123853382444032220, .request_checksum_padding = 0, .context = 78229259269725858827546093461364404336, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 179, .commit = 179, .timestamp = 1764013358702488113, .request = 177, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.469Z debug(replica): 2n: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 287701050567195852176004730441229480825, .checksum_padding = 0, .checksum_body = 317130472969479645741804541105897845387, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 330619327999152601426123853382444032220, .request_checksum_padding = 0, .context = 78229259269725858827546093461364404336, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 179, .commit = 179, .timestamp = 1764013358702488113, .request = 177, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.469Z debug(forest): entering forest.compact() op=179 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:41.471Z debug(replica): 1n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=177
2025-11-24 19:42:41.471Z debug(forest): entering forest.compact() op=179 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:41.480Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:41.480Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:41.480Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=188743680 len=4096 unlocked
2025-11-24 19:42:41.480Z debug(journal): 2: write_header: op=180 sectors[45056..49152]
2025-11-24 19:42:41.480Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:41.480Z debug(client_replies): 2: write_reply: wrote (client=323321802946825702132615013286692535219 request=177)
2025-11-24 19:42:41.480Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:41.480Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:41.480Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:41.480Z debug(journal): 2: write: view=3 slot=180 op=180 len=2320: 192257984292774967620877459702865073863 complete, marking clean
2025-11-24 19:42:41.480Z debug(replica): 2n: send_prepare_ok: op=180 checksum=192257984292774967620877459702865073863
2025-11-24 19:42:41.480Z debug(replica): 2n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 267355180932483735109707170789150473248, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .prepare_checksum = 192257984292774967620877459702865073863, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit_min = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.481Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 267355180932483735109707170789150473248, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .prepare_checksum = 192257984292774967620877459702865073863, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit_min = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.481Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:41.481Z debug(replica): 0N: on_prepare_ok: 2 message(s)
2025-11-24 19:42:41.481Z debug(replica): 0N: on_prepare_ok: quorum received, context=192257984292774967620877459702865073863
2025-11-24 19:42:41.481Z debug(vsr): 0: prepare_timeout stopped
2025-11-24 19:42:41.481Z debug(vsr): 0: primary_abdicate_timeout stopped
2025-11-24 19:42:41.481Z debug(replica): 0N: execute_op: executing view=3 primary=true op=180 checksum=192257984292774967620877459702865073863 (lookup_accounts)
2025-11-24 19:42:41.481Z debug(replica): 0N: execute_op: commit_timestamp=1764013358702488113 prepare.header.timestamp=1764013361445076939
2025-11-24 19:42:41.481Z debug(replica): 0N: execute_op: advancing commit_max=179..180
2025-11-24 19:42:41.481Z debug(replica): 0N: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=178
2025-11-24 19:42:41.481Z debug(replica): 0N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 151511879866405971640553154681859762865, .checksum_padding = 0, .checksum_body = 20490345662655130689381858030366581631, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250642225106363936751539469967989535048, .request_checksum_padding = 0, .context = 96366611582437213654694969077663387408, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit = 180, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.481Z debug(replica): 0N: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 151511879866405971640553154681859762865, .checksum_padding = 0, .checksum_body = 20490345662655130689381858030366581631, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250642225106363936751539469967989535048, .request_checksum_padding = 0, .context = 96366611582437213654694969077663387408, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit = 180, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.481Z debug(forest): entering forest.compact() op=180 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:41.482Z debug(client_replies): 0: write_reply: wrote (client=323321802946825702132615013286692535219 request=178)
2025-11-24 19:42:41.483Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=188743680 len=4096 unlocked
2025-11-24 19:42:41.483Z debug(journal): 1: write_header: op=180 sectors[45056..49152]
2025-11-24 19:42:41.483Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:41.483Z debug(client_replies): 1: write_reply: wrote (client=323321802946825702132615013286692535219 request=177)
2025-11-24 19:42:41.483Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:41.483Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:41.483Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:41.483Z debug(journal): 1: write: view=3 slot=180 op=180 len=2320: 192257984292774967620877459702865073863 complete, marking clean
2025-11-24 19:42:41.483Z debug(replica): 1n: send_prepare_ok: op=180 checksum=192257984292774967620877459702865073863
2025-11-24 19:42:41.483Z debug(replica): 1n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 318672062237797691117455232947890123585, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .prepare_checksum = 192257984292774967620877459702865073863, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit_min = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.483Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 318672062237797691117455232947890123585, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 175335667413114181383867159288548171492, .parent_padding = 0, .prepare_checksum = 192257984292774967620877459702865073863, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit_min = 179, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.483Z debug(replica): 0N: on_prepare_ok: not preparing op=180 checksum=192257984292774967620877459702865073863
2025-11-24 19:42:41.490Z info(workload): accounts created = 128, transfers = 248452, pending transfers = 0, commands run = 89
2025-11-24 19:42:41.503Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 58987814582336734098121803665506098189, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 96366611582437213654694969077663387408, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 179, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 45791949, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.503Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:41.503Z debug(replica): 0N: primary_pipeline_prepare: request checksum=58987814582336734098121803665506098189 client=323321802946825702132615013286692535219
2025-11-24 19:42:41.503Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 58987814582336734098121803665506098189, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 96366611582437213654694969077663387408, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 179, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 45791949, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.503Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 19:42:41.503Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 58987814582336734098121803665506098189, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 96366611582437213654694969077663387408, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 179, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 45791949, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.503Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:41.503Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:41.504Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:41.504Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:41.506Z debug(replica): 0N: primary_pipeline_prepare: prepare checksum=312311258755631104165807379498885171188 op=181
2025-11-24 19:42:41.506Z debug(vsr): 0: prepare_timeout started
2025-11-24 19:42:41.506Z debug(vsr): 0: primary_abdicate_timeout started
2025-11-24 19:42:41.506Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:41.506Z debug(replica): 0N: replicate: replicating op=181 to replica 1
2025-11-24 19:42:41.506Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 312311258755631104165807379498885171188, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .request_checksum = 58987814582336734098121803665506098189, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.506Z debug(replica): 0N: replicate: replicating op=181 to replica 2
2025-11-24 19:42:41.506Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 312311258755631104165807379498885171188, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .request_checksum = 58987814582336734098121803665506098189, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.506Z debug(replica): 0N: on_prepare: advancing: op=180..181 checksum=192257984292774967620877459702865073863..312311258755631104165807379498885171188
2025-11-24 19:42:41.506Z debug(journal): 0: set_header_as_dirty: op=181 checksum=312311258755631104165807379498885171188
2025-11-24 19:42:41.506Z debug(replica): 0N: append: appending to journal op=181
2025-11-24 19:42:41.506Z debug(journal): 0: write: view=3 slot=181 op=181 len=792832: 312311258755631104165807379498885171188 starting
2025-11-24 19:42:41.506Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=189792256 len=794624 locked
2025-11-24 19:42:41.510Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 312311258755631104165807379498885171188, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .request_checksum = 58987814582336734098121803665506098189, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:41.510Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 58987814582336734098121803665506098189, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 96366611582437213654694969077663387408, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 179, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 45791949, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:41.510Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 312311258755631104165807379498885171188, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .request_checksum = 58987814582336734098121803665506098189, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.905Z debug(replica): 2n: on_prepare: advancing commit_max=179..180
2025-11-24 19:42:43.905Z debug(replica): 2n: on_prepare: caching prepare.op=181 (commit_min=179 op=180 commit_max=180 prepare_max=1007)
2025-11-24 19:42:43.905Z debug(replica): 1n: on_prepare: advancing commit_max=179..180
2025-11-24 19:42:43.905Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:43.905Z debug(replica): 1n: on_prepare: caching prepare.op=181 (commit_min=179 op=180 commit_max=180 prepare_max=1007)
2025-11-24 19:42:43.905Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 19:42:43.905Z debug(replica): 2n: on_prepare: advancing: op=180..181 checksum=192257984292774967620877459702865073863..312311258755631104165807379498885171188
2025-11-24 19:42:43.905Z debug(journal): 2: set_header_as_dirty: op=181 checksum=312311258755631104165807379498885171188
2025-11-24 19:42:43.905Z debug(replica): 2n: append: appending to journal op=181
2025-11-24 19:42:43.905Z debug(replica): 1n: on_prepare: advancing: op=180..181 checksum=192257984292774967620877459702865073863..312311258755631104165807379498885171188
2025-11-24 19:42:43.905Z debug(journal): 2: write: view=3 slot=181 op=181 len=792832: 312311258755631104165807379498885171188 starting
2025-11-24 19:42:43.905Z debug(journal): 1: set_header_as_dirty: op=181 checksum=312311258755631104165807379498885171188
2025-11-24 19:42:43.905Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=189792256 len=794624 locked
2025-11-24 19:42:43.905Z debug(replica): 1n: append: appending to journal op=181
2025-11-24 19:42:43.905Z debug(journal): 1: write: view=3 slot=181 op=181 len=792832: 312311258755631104165807379498885171188 starting
2025-11-24 19:42:43.905Z debug(replica): 2n: commit_start_journal: cached prepare op=180 checksum=192257984292774967620877459702865073863
2025-11-24 19:42:43.905Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=189792256 len=794624 locked
2025-11-24 19:42:43.905Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=189792256 len=794624 unlocked
2025-11-24 19:42:43.905Z debug(journal): 0: write_header: op=181 sectors[45056..49152]
2025-11-24 19:42:43.905Z debug(replica): 1n: commit_start_journal: cached prepare op=180 checksum=192257984292774967620877459702865073863
2025-11-24 19:42:43.905Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:43.905Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:43.905Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:43.905Z debug(replica): 2n: repair_prepare: op=181 checksum=312311258755631104165807379498885171188 (already writing)
2025-11-24 19:42:43.905Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=179)
2025-11-24 19:42:43.905Z debug(replica): 2n: execute_op: executing view=3 primary=false op=180 checksum=192257984292774967620877459702865073863 (lookup_accounts)
2025-11-24 19:42:43.905Z debug(replica): 2n: execute_op: commit_timestamp=1764013358702488113 prepare.header.timestamp=1764013361445076939
2025-11-24 19:42:43.905Z debug(replica): 1n: repair_prepare: op=181 checksum=312311258755631104165807379498885171188 (already writing)
2025-11-24 19:42:43.905Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=179)
2025-11-24 19:42:43.905Z debug(replica): 1n: execute_op: executing view=3 primary=false op=180 checksum=192257984292774967620877459702865073863 (lookup_accounts)
2025-11-24 19:42:43.905Z debug(replica): 1n: execute_op: commit_timestamp=1764013358702488113 prepare.header.timestamp=1764013361445076939
2025-11-24 19:42:43.905Z debug(replica): 2n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=178
2025-11-24 19:42:43.905Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 151511879866405971640553154681859762865, .checksum_padding = 0, .checksum_body = 20490345662655130689381858030366581631, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250642225106363936751539469967989535048, .request_checksum_padding = 0, .context = 96366611582437213654694969077663387408, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit = 180, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.905Z debug(replica): 2n: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 151511879866405971640553154681859762865, .checksum_padding = 0, .checksum_body = 20490345662655130689381858030366581631, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 250642225106363936751539469967989535048, .request_checksum_padding = 0, .context = 96366611582437213654694969077663387408, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 180, .commit = 180, .timestamp = 1764013361445076939, .request = 178, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.905Z debug(forest): entering forest.compact() op=180 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:43.905Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:43.905Z debug(replica): 1n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=178
2025-11-24 19:42:43.905Z debug(journal): 0: write: view=3 slot=181 op=181 len=792832: 312311258755631104165807379498885171188 complete, marking clean
2025-11-24 19:42:43.905Z debug(replica): 0N: send_prepare_ok: op=181 checksum=312311258755631104165807379498885171188
2025-11-24 19:42:43.905Z debug(forest): entering forest.compact() op=180 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:43.905Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 299894599723715517230980446925663958791, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .prepare_checksum = 312311258755631104165807379498885171188, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit_min = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.905Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 299894599723715517230980446925663958791, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .prepare_checksum = 312311258755631104165807379498885171188, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit_min = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.905Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:43.905Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 19:42:43.905Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 19:42:43.905Z debug(client_replies): 2: write_reply: wrote (client=323321802946825702132615013286692535219 request=178)
2025-11-24 19:42:43.906Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=189792256 len=794624 unlocked
2025-11-24 19:42:43.906Z debug(journal): 2: write_header: op=181 sectors[45056..49152]
2025-11-24 19:42:43.906Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:43.906Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:43.906Z debug(journal): 2: write: view=3 slot=181 op=181 len=792832: 312311258755631104165807379498885171188 complete, marking clean
2025-11-24 19:42:43.906Z debug(replica): 2n: send_prepare_ok: op=181 checksum=312311258755631104165807379498885171188
2025-11-24 19:42:43.906Z debug(replica): 2n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 173324554822941201524145302154440947608, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .prepare_checksum = 312311258755631104165807379498885171188, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit_min = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.909Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 58987814582336734098121803665506098189, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 96366611582437213654694969077663387408, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 179, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 45791949, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.909Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:43.909Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 19:42:43.909Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 173324554822941201524145302154440947608, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .prepare_checksum = 312311258755631104165807379498885171188, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit_min = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.909Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:43.909Z debug(replica): 0N: on_prepare_ok: 2 message(s)
2025-11-24 19:42:43.909Z debug(replica): 0N: on_prepare_ok: quorum received, context=312311258755631104165807379498885171188
2025-11-24 19:42:43.909Z debug(vsr): 0: prepare_timeout stopped
2025-11-24 19:42:43.909Z debug(vsr): 0: primary_abdicate_timeout stopped
2025-11-24 19:42:43.909Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 58987814582336734098121803665506098189, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 96366611582437213654694969077663387408, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 179, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 45791949, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.909Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 19:42:43.909Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 58987814582336734098121803665506098189, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 96366611582437213654694969077663387408, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 179, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 45791949, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.909Z debug(client_replies): 1: write_reply: wrote (client=323321802946825702132615013286692535219 request=178)
2025-11-24 19:42:43.909Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=189792256 len=794624 unlocked
2025-11-24 19:42:43.909Z debug(journal): 1: write_header: op=181 sectors[45056..49152]
2025-11-24 19:42:43.909Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:43.909Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:43.909Z debug(journal): 1: write: view=3 slot=181 op=181 len=792832: 312311258755631104165807379498885171188 complete, marking clean
2025-11-24 19:42:43.910Z debug(replica): 1n: send_prepare_ok: op=181 checksum=312311258755631104165807379498885171188
2025-11-24 19:42:43.910Z debug(replica): 1n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 10861636487441503739089196736339721395, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .prepare_checksum = 312311258755631104165807379498885171188, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit_min = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.912Z debug(replica): 0N: execute_op: executing view=3 primary=true op=181 checksum=312311258755631104165807379498885171188 (create_transfers)
2025-11-24 19:42:43.912Z debug(replica): 0N: execute_op: commit_timestamp=1764013361445076939 prepare.header.timestamp=1764013361503211221
2025-11-24 19:42:43.915Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:43.915Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:43.915Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:43.915Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:43.935Z debug(replica): 0N: execute_op: advancing commit_max=180..181
2025-11-24 19:42:43.935Z debug(replica): 0N: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=179
2025-11-24 19:42:43.935Z debug(replica): 0N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 143308304229406344260293826376601896054, .checksum_padding = 0, .checksum_body = 333924054266898176662430888659491408315, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 58987814582336734098121803665506098189, .request_checksum_padding = 0, .context = 92373739691515903534477063191961572136, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit = 181, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.935Z debug(replica): 0N: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 143308304229406344260293826376601896054, .checksum_padding = 0, .checksum_body = 333924054266898176662430888659491408315, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 58987814582336734098121803665506098189, .request_checksum_padding = 0, .context = 92373739691515903534477063191961572136, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit = 181, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.935Z debug(forest): entering forest.compact() op=181 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 323321802946825702132615013286692535219: on_reply: slow request, request=179 op=181 size=792832 create_transfers time=2439ms
2025-11-24 19:42:43.935Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:43.935Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:43.935Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 19:42:43.935Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 19:42:43.935Z debug(vsr): 1: ping_timeout fired
2025-11-24 19:42:43.935Z debug(vsr): 1: ping_timeout reset
2025-11-24 19:42:43.935Z debug(replica): 1n: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 266230680743074223015757185173907409738, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133446897533977, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.936Z debug(replica): 1n: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 266230680743074223015757185173907409738, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133446897533977, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.936Z debug(vsr): 1: start_view_change_message_timeout fired
2025-11-24 19:42:43.936Z debug(vsr): 1: start_view_change_message_timeout reset
2025-11-24 19:42:43.936Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:43.936Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:43.936Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Ping{ .checksum = 266230680743074223015757185173907409738, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133446897533977, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.936Z debug(vsr): 1: journal_repair_timeout fired
2025-11-24 19:42:43.936Z debug(vsr): 1: journal_repair_timeout reset
2025-11-24 19:42:43.936Z debug(replica): 2n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 294083537793011684068523780268391968286, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36133446897533977, .pong_timestamp_wall = 1764013363936114986, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.936Z debug(vsr): 1: repair_sync_timeout fired
2025-11-24 19:42:43.936Z debug(vsr): 1: repair_sync_timeout reset
2025-11-24 19:42:43.936Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-11-24 19:42:43.936Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-11-24 19:42:43.936Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Pong{ .checksum = 294083537793011684068523780268391968286, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36133446897533977, .pong_timestamp_wall = 1764013363936114986, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.936Z debug(clock): 1: learn: replica=2 m0=36133446897533977 t1=1764013363936114986 m2=36133446897853876 t2=1764013363936283405 one_way_delay=159949 asymmetric_delay=0 clock_offset=-8470
2025-11-24 19:42:43.946Z debug(clock): 1: synchronized: truechimers=2/3 clock_offset=0ns..0ns accuracy=0ns
2025-11-24 19:42:43.946Z debug(clock): 1: system time is 141ns behind
2025-11-24 19:42:43.947Z debug(client_replies): 0: write_reply: wrote (client=323321802946825702132615013286692535219 request=179)
2025-11-24 19:42:43.951Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 58987814582336734098121803665506098189, .checksum_padding = 0, .checksum_body = 52412615823645345622673164379335353931, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 792832, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 96366611582437213654694969077663387408, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 179, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 45791949, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.951Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 19:42:43.951Z debug(client_replies): 0: read_reply: start (client=323321802946825702132615013286692535219 reply=143308304229406344260293826376601896054)
2025-11-24 19:42:43.951Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 10861636487441503739089196736339721395, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 192257984292774967620877459702865073863, .parent_padding = 0, .prepare_checksum = 312311258755631104165807379498885171188, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit_min = 180, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.951Z debug(replica): 0N: on_prepare_ok: not preparing op=181 checksum=312311258755631104165807379498885171188
2025-11-24 19:42:43.951Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Ping{ .checksum = 266230680743074223015757185173907409738, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133446897533977, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.951Z debug(replica): 0N: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 5920977043669994108429160288833958863, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36133446897533977, .pong_timestamp_wall = 1764013363951197079, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.951Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Pong{ .checksum = 5920977043669994108429160288833958863, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36133446897533977, .pong_timestamp_wall = 1764013363951197079, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.951Z debug(clock): 1: learn: m0=36133446897533977 < window.monotonic=36133446907841435
2025-11-24 19:42:43.951Z debug(client_replies): 0: read_reply: done (client=323321802946825702132615013286692535219 reply=143308304229406344260293826376601896054)
2025-11-24 19:42:43.951Z debug(replica): 0N: on_request: repeat reply (client=323321802946825702132615013286692535219 request=179)
2025-11-24 19:42:43.951Z debug(replica): 0N: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 143308304229406344260293826376601896054, .checksum_padding = 0, .checksum_body = 333924054266898176662430888659491408315, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 58987814582336734098121803665506098189, .request_checksum_padding = 0, .context = 92373739691515903534477063191961572136, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit = 181, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.955Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:43.955Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:43.956Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:43.956Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:43.957Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:43.957Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:43.957Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 19:42:43.957Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 19:42:43.958Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 295660426386321331954023771760435775696, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 92373739691515903534477063191961572136, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 180, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2439927934, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.958Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:43.958Z debug(replica): 0N: primary_pipeline_prepare: request checksum=295660426386321331954023771760435775696 client=323321802946825702132615013286692535219
2025-11-24 19:42:43.958Z debug(replica): 0N: primary_pipeline_prepare: prepare checksum=25052576896655597876054065737995279801 op=182
2025-11-24 19:42:43.958Z debug(vsr): 0: prepare_timeout started
2025-11-24 19:42:43.958Z debug(vsr): 0: primary_abdicate_timeout started
2025-11-24 19:42:43.958Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:43.958Z debug(replica): 0N: replicate: replicating op=182 to replica 1
2025-11-24 19:42:43.958Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 25052576896655597876054065737995279801, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312311258755631104165807379498885171188, .parent_padding = 0, .request_checksum = 295660426386321331954023771760435775696, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit = 181, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.958Z debug(replica): 0N: replicate: replicating op=182 to replica 2
2025-11-24 19:42:43.958Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 25052576896655597876054065737995279801, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312311258755631104165807379498885171188, .parent_padding = 0, .request_checksum = 295660426386321331954023771760435775696, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit = 181, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.958Z debug(replica): 0N: on_prepare: advancing: op=181..182 checksum=312311258755631104165807379498885171188..25052576896655597876054065737995279801
2025-11-24 19:42:43.958Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 295660426386321331954023771760435775696, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 92373739691515903534477063191961572136, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 180, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2439927934, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.958Z debug(journal): 0: set_header_as_dirty: op=182 checksum=25052576896655597876054065737995279801
2025-11-24 19:42:43.958Z debug(replica): 0N: append: appending to journal op=182
2025-11-24 19:42:43.958Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 19:42:43.958Z debug(journal): 0: write: view=3 slot=182 op=182 len=2320: 25052576896655597876054065737995279801 starting
2025-11-24 19:42:43.958Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=190840832 len=4096 locked
2025-11-24 19:42:43.958Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 295660426386321331954023771760435775696, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 92373739691515903534477063191961572136, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 180, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2439927934, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.958Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 25052576896655597876054065737995279801, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312311258755631104165807379498885171188, .parent_padding = 0, .request_checksum = 295660426386321331954023771760435775696, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit = 181, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.958Z debug(replica): 2n: on_prepare: advancing commit_max=180..181
2025-11-24 19:42:43.958Z debug(replica): 2n: on_prepare: caching prepare.op=182 (commit_min=180 op=181 commit_max=181 prepare_max=1007)
2025-11-24 19:42:43.958Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 295660426386321331954023771760435775696, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 92373739691515903534477063191961572136, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 180, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2439927934, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.958Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:43.958Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 19:42:43.958Z debug(replica): 2n: on_prepare: advancing: op=181..182 checksum=312311258755631104165807379498885171188..25052576896655597876054065737995279801
2025-11-24 19:42:43.958Z debug(journal): 2: set_header_as_dirty: op=182 checksum=25052576896655597876054065737995279801
2025-11-24 19:42:43.958Z debug(replica): 2n: append: appending to journal op=182
2025-11-24 19:42:43.958Z debug(journal): 2: write: view=3 slot=182 op=182 len=2320: 25052576896655597876054065737995279801 starting
2025-11-24 19:42:43.958Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=190840832 len=4096 locked
2025-11-24 19:42:43.958Z debug(replica): 2n: commit_start_journal: cached prepare op=181 checksum=312311258755631104165807379498885171188
2025-11-24 19:42:43.958Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=190840832 len=4096 unlocked
2025-11-24 19:42:43.958Z debug(journal): 0: write_header: op=182 sectors[45056..49152]
2025-11-24 19:42:43.958Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:43.958Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:43.958Z debug(journal): 0: write: view=3 slot=182 op=182 len=2320: 25052576896655597876054065737995279801 complete, marking clean
2025-11-24 19:42:43.958Z debug(replica): 0N: send_prepare_ok: op=182 checksum=25052576896655597876054065737995279801
2025-11-24 19:42:43.958Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 318206213077910868852912006044781506694, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312311258755631104165807379498885171188, .parent_padding = 0, .prepare_checksum = 25052576896655597876054065737995279801, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit_min = 181, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.958Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 318206213077910868852912006044781506694, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312311258755631104165807379498885171188, .parent_padding = 0, .prepare_checksum = 25052576896655597876054065737995279801, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit_min = 181, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.958Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:43.958Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 19:42:43.958Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 19:42:43.959Z debug(replica): 2n: repair_prepare: op=182 checksum=25052576896655597876054065737995279801 (already writing)
2025-11-24 19:42:43.959Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=180)
2025-11-24 19:42:43.960Z debug(replica): 2n: execute_op: executing view=3 primary=false op=181 checksum=312311258755631104165807379498885171188 (create_transfers)
2025-11-24 19:42:43.960Z debug(replica): 2n: execute_op: commit_timestamp=1764013361445076939 prepare.header.timestamp=1764013361503211221
2025-11-24 19:42:43.975Z debug(replica): 2n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=179
2025-11-24 19:42:43.975Z debug(forest): entering forest.compact() op=181 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:43.976Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:43.976Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:43.977Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:43.977Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:43.983Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=190840832 len=4096 unlocked
2025-11-24 19:42:43.983Z debug(journal): 2: write_header: op=182 sectors[45056..49152]
2025-11-24 19:42:43.983Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:43.983Z debug(client_replies): 2: write_reply: wrote (client=323321802946825702132615013286692535219 request=179)
2025-11-24 19:42:43.983Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:43.983Z debug(journal): 2: write: view=3 slot=182 op=182 len=2320: 25052576896655597876054065737995279801 complete, marking clean
2025-11-24 19:42:43.983Z debug(replica): 2n: send_prepare_ok: op=182 checksum=25052576896655597876054065737995279801
2025-11-24 19:42:43.983Z debug(replica): 2n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 137706177483835919080195662576530771266, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312311258755631104165807379498885171188, .parent_padding = 0, .prepare_checksum = 25052576896655597876054065737995279801, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit_min = 181, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.983Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 137706177483835919080195662576530771266, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312311258755631104165807379498885171188, .parent_padding = 0, .prepare_checksum = 25052576896655597876054065737995279801, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit_min = 181, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.983Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:43.983Z debug(replica): 0N: on_prepare_ok: 2 message(s)
2025-11-24 19:42:43.983Z debug(replica): 0N: on_prepare_ok: quorum received, context=25052576896655597876054065737995279801
2025-11-24 19:42:43.983Z debug(vsr): 0: prepare_timeout stopped
2025-11-24 19:42:43.983Z debug(vsr): 0: primary_abdicate_timeout stopped
2025-11-24 19:42:43.983Z debug(replica): 0N: execute_op: executing view=3 primary=true op=182 checksum=25052576896655597876054065737995279801 (lookup_accounts)
2025-11-24 19:42:43.983Z debug(replica): 0N: execute_op: commit_timestamp=1764013361503211221 prepare.header.timestamp=1764013363958138868
2025-11-24 19:42:43.983Z debug(replica): 0N: execute_op: advancing commit_max=181..182
2025-11-24 19:42:43.983Z debug(replica): 0N: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=180
2025-11-24 19:42:43.983Z debug(replica): 0N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 15415897454059433590672767639601539167, .checksum_padding = 0, .checksum_body = 196240752802249718086488176524228527987, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 295660426386321331954023771760435775696, .request_checksum_padding = 0, .context = 13419321619857820417170139502338057035, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit = 182, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.983Z debug(replica): 0N: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 15415897454059433590672767639601539167, .checksum_padding = 0, .checksum_body = 196240752802249718086488176524228527987, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 295660426386321331954023771760435775696, .request_checksum_padding = 0, .context = 13419321619857820417170139502338057035, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit = 182, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:43.984Z debug(forest): entering forest.compact() op=182 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:43.984Z debug(client_replies): 0: write_reply: wrote (client=323321802946825702132615013286692535219 request=180)
2025-11-24 19:42:43.993Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:43.993Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:43.996Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:43.996Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:43.997Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:43.997Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:43.998Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 25052576896655597876054065737995279801, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312311258755631104165807379498885171188, .parent_padding = 0, .request_checksum = 295660426386321331954023771760435775696, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit = 181, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:43.998Z debug(replica): 1n: on_prepare: advancing commit_max=180..181
2025-11-24 19:42:43.998Z debug(replica): 1n: on_prepare: caching prepare.op=182 (commit_min=180 op=181 commit_max=181 prepare_max=1007)
2025-11-24 19:42:43.998Z debug(replica): 1n: on_prepare: advancing: op=181..182 checksum=312311258755631104165807379498885171188..25052576896655597876054065737995279801
2025-11-24 19:42:43.998Z debug(journal): 1: set_header_as_dirty: op=182 checksum=25052576896655597876054065737995279801
2025-11-24 19:42:43.998Z debug(replica): 1n: append: appending to journal op=182
2025-11-24 19:42:43.998Z debug(journal): 1: write: view=3 slot=182 op=182 len=2320: 25052576896655597876054065737995279801 starting
2025-11-24 19:42:43.998Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=190840832 len=4096 locked
2025-11-24 19:42:43.998Z debug(replica): 1n: commit_start_journal: cached prepare op=181 checksum=312311258755631104165807379498885171188
2025-11-24 19:42:43.998Z info(workload): accounts created = 128, transfers = 254643, pending transfers = 0, commands run = 90
2025-11-24 19:42:43.999Z debug(replica): 1n: repair_prepare: op=182 checksum=25052576896655597876054065737995279801 (already writing)
2025-11-24 19:42:43.999Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=180)
2025-11-24 19:42:44.001Z debug(replica): 1n: execute_op: executing view=3 primary=false op=181 checksum=312311258755631104165807379498885171188 (create_transfers)
2025-11-24 19:42:44.001Z debug(replica): 1n: execute_op: commit_timestamp=1764013361445076939 prepare.header.timestamp=1764013361503211221
2025-11-24 19:42:44.003Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 190606545017241956841360206291921099089, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 13419321619857820417170139502338057035, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 181, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 40732384, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:44.003Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:44.003Z debug(replica): 0N: primary_pipeline_prepare: request checksum=190606545017241956841360206291921099089 client=323321802946825702132615013286692535219
2025-11-24 19:42:44.013Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:44.004Z debug(replica): 0N: primary_pipeline_prepare: prepare checksum=53488760202783532770282344540085536021 op=183
2025-11-24 19:42:44.026Z debug(replica): 1n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=179
2025-11-24 19:42:44.013Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:46.888Z debug(vsr): 0: prepare_timeout started
2025-11-24 19:42:46.888Z debug(vsr): 0: primary_abdicate_timeout started
2025-11-24 19:42:46.011Z info(supervisor): injecting network loss: testing.vortex.faulty_network.Faults{ .delay = null, .lose = 4/100, .corrupt = null }
2025-11-24 19:42:46.888Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:46.888Z debug(replica): 0N: replicate: replicating op=183 to replica 1
2025-11-24 19:42:46.888Z debug(replica): 1n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 143308304229406344260293826376601896054, .checksum_padding = 0, .checksum_body = 333924054266898176662430888659491408315, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 58987814582336734098121803665506098189, .request_checksum_padding = 0, .context = 92373739691515903534477063191961572136, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit = 181, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.888Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 53488760202783532770282344540085536021, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .request_checksum = 190606545017241956841360206291921099089, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.888Z debug(replica): 1n: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 143308304229406344260293826376601896054, .checksum_padding = 0, .checksum_body = 333924054266898176662430888659491408315, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 58987814582336734098121803665506098189, .request_checksum_padding = 0, .context = 92373739691515903534477063191961572136, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 181, .commit = 181, .timestamp = 1764013361503211221, .request = 179, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.888Z debug(forest): entering forest.compact() op=181 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:46.888Z debug(replica): 0N: replicate: replicating op=183 to replica 2
2025-11-24 19:42:46.888Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 53488760202783532770282344540085536021, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .request_checksum = 190606545017241956841360206291921099089, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.888Z debug(replica): 0N: on_prepare: advancing: op=182..183 checksum=25052576896655597876054065737995279801..53488760202783532770282344540085536021
2025-11-24 19:42:46.888Z debug(journal): 0: set_header_as_dirty: op=183 checksum=53488760202783532770282344540085536021
2025-11-24 19:42:46.888Z debug(replica): 0N: append: appending to journal op=183
2025-11-24 19:42:46.888Z debug(journal): 0: write: view=3 slot=183 op=183 len=277120: 53488760202783532770282344540085536021 starting
2025-11-24 19:42:46.888Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=191889408 len=278528 locked
2025-11-24 19:42:46.889Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 190606545017241956841360206291921099089, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 13419321619857820417170139502338057035, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 181, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 40732384, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.889Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 19:42:46.889Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 190606545017241956841360206291921099089, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 13419321619857820417170139502338057035, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 181, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 40732384, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.889Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 190606545017241956841360206291921099089, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 13419321619857820417170139502338057035, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 181, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 40732384, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.889Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:46.890Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 19:42:46.890Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=191889408 len=278528 unlocked
2025-11-24 19:42:46.890Z debug(journal): 0: write_header: op=183 sectors[45056..49152]
2025-11-24 19:42:46.890Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:46.890Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:46.890Z debug(journal): 0: write: view=3 slot=183 op=183 len=277120: 53488760202783532770282344540085536021 complete, marking clean
2025-11-24 19:42:46.890Z debug(replica): 0N: send_prepare_ok: op=183 checksum=53488760202783532770282344540085536021
2025-11-24 19:42:46.890Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 293724856221160701931447175861693017749, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .prepare_checksum = 53488760202783532770282344540085536021, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit_min = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.890Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 293724856221160701931447175861693017749, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .prepare_checksum = 53488760202783532770282344540085536021, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit_min = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.890Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:46.890Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 19:42:46.890Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 19:42:46.890Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 53488760202783532770282344540085536021, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .request_checksum = 190606545017241956841360206291921099089, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.891Z debug(replica): 2n: on_prepare: advancing commit_max=181..182
2025-11-24 19:42:46.891Z debug(replica): 2n: on_prepare: caching prepare.op=183 (commit_min=181 op=182 commit_max=182 prepare_max=1007)
2025-11-24 19:42:46.891Z debug(replica): 2n: on_prepare: advancing: op=182..183 checksum=25052576896655597876054065737995279801..53488760202783532770282344540085536021
2025-11-24 19:42:46.891Z debug(journal): 2: set_header_as_dirty: op=183 checksum=53488760202783532770282344540085536021
2025-11-24 19:42:46.891Z debug(replica): 2n: append: appending to journal op=183
2025-11-24 19:42:46.891Z debug(journal): 2: write: view=3 slot=183 op=183 len=277120: 53488760202783532770282344540085536021 starting
2025-11-24 19:42:46.891Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=191889408 len=278528 locked
2025-11-24 19:42:46.891Z debug(replica): 2n: commit_start_journal: cached prepare op=182 checksum=25052576896655597876054065737995279801
2025-11-24 19:42:46.891Z debug(replica): 2n: repair_prepare: op=183 checksum=53488760202783532770282344540085536021 (already writing)
2025-11-24 19:42:46.891Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=181)
2025-11-24 19:42:46.891Z debug(replica): 2n: execute_op: executing view=3 primary=false op=182 checksum=25052576896655597876054065737995279801 (lookup_accounts)
2025-11-24 19:42:46.891Z debug(replica): 2n: execute_op: commit_timestamp=1764013361503211221 prepare.header.timestamp=1764013363958138868
2025-11-24 19:42:46.891Z debug(replica): 2n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=180
2025-11-24 19:42:46.891Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 15415897454059433590672767639601539167, .checksum_padding = 0, .checksum_body = 196240752802249718086488176524228527987, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 295660426386321331954023771760435775696, .request_checksum_padding = 0, .context = 13419321619857820417170139502338057035, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit = 182, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.891Z debug(replica): 2n: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 15415897454059433590672767639601539167, .checksum_padding = 0, .checksum_body = 196240752802249718086488176524228527987, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 295660426386321331954023771760435775696, .request_checksum_padding = 0, .context = 13419321619857820417170139502338057035, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit = 182, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.891Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 190606545017241956841360206291921099089, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 13419321619857820417170139502338057035, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 181, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 40732384, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.891Z debug(forest): entering forest.compact() op=182 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:46.891Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:46.891Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 19:42:46.891Z debug(client_replies): 2: write_reply: wrote (client=323321802946825702132615013286692535219 request=180)
2025-11-24 19:42:46.892Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=191889408 len=278528 unlocked
2025-11-24 19:42:46.892Z debug(journal): 2: write_header: op=183 sectors[45056..49152]
2025-11-24 19:42:46.892Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:46.892Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:46.892Z debug(journal): 2: write: view=3 slot=183 op=183 len=277120: 53488760202783532770282344540085536021 complete, marking clean
2025-11-24 19:42:46.892Z debug(replica): 2n: send_prepare_ok: op=183 checksum=53488760202783532770282344540085536021
2025-11-24 19:42:46.892Z debug(replica): 2n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 84053430819696192031746861705998327749, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .prepare_checksum = 53488760202783532770282344540085536021, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit_min = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.892Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 84053430819696192031746861705998327749, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .prepare_checksum = 53488760202783532770282344540085536021, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit_min = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.892Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:46.892Z debug(replica): 0N: on_prepare_ok: 2 message(s)
2025-11-24 19:42:46.892Z debug(replica): 0N: on_prepare_ok: quorum received, context=53488760202783532770282344540085536021
2025-11-24 19:42:46.892Z debug(vsr): 0: prepare_timeout stopped
2025-11-24 19:42:46.892Z debug(vsr): 0: primary_abdicate_timeout stopped
2025-11-24 19:42:46.893Z debug(replica): 0N: execute_op: executing view=3 primary=true op=183 checksum=53488760202783532770282344540085536021 (create_transfers)
2025-11-24 19:42:46.893Z debug(replica): 0N: execute_op: commit_timestamp=1764013363958138868 prepare.header.timestamp=1764013364003121301
2025-11-24 19:42:46.898Z info(supervisor): sleeping for 490.898ms
2025-11-24 19:42:46.900Z warning(replica): 1n: commit_dispatch: slow request, request=179 size=792832 create_transfers time=2901ms
2025-11-24 19:42:46.900Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=190840832 len=4096 unlocked
2025-11-24 19:42:46.900Z debug(journal): 1: write_header: op=182 sectors[45056..49152]
2025-11-24 19:42:46.900Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:46.900Z debug(client_replies): 1: write_reply: wrote (client=323321802946825702132615013286692535219 request=179)
2025-11-24 19:42:46.901Z debug(replica): 0N: execute_op: advancing commit_max=182..183
2025-11-24 19:42:46.901Z debug(replica): 0N: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=181
2025-11-24 19:42:46.901Z debug(replica): 0N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 208929120119286881451163540399546182245, .checksum_padding = 0, .checksum_body = 252271904436463863891991372994883614728, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 190606545017241956841360206291921099089, .request_checksum_padding = 0, .context = 142006707948578513506132462770086189799, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit = 183, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.901Z debug(replica): 0N: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 208929120119286881451163540399546182245, .checksum_padding = 0, .checksum_body = 252271904436463863891991372994883614728, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 190606545017241956841360206291921099089, .request_checksum_padding = 0, .context = 142006707948578513506132462770086189799, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit = 183, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.901Z debug(forest): entering forest.compact() op=183 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 323321802946825702132615013286692535219: on_reply: slow request, request=181 op=183 size=277120 create_transfers time=2901ms
2025-11-24 19:42:46.902Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 190606545017241956841360206291921099089, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 13419321619857820417170139502338057035, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 181, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 40732384, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.902Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 19:42:46.902Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 190606545017241956841360206291921099089, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 13419321619857820417170139502338057035, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 181, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 40732384, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.903Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 53488760202783532770282344540085536021, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .request_checksum = 190606545017241956841360206291921099089, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.903Z debug(replica): 1n: on_prepare: advancing commit_max=181..182
2025-11-24 19:42:46.903Z debug(replica): 1n: on_prepare: caching prepare.op=183 (commit_min=181 op=182 commit_max=182 prepare_max=1007)
2025-11-24 19:42:46.903Z debug(replica): 1n: on_prepare: advancing: op=182..183 checksum=25052576896655597876054065737995279801..53488760202783532770282344540085536021
2025-11-24 19:42:46.903Z debug(journal): 1: set_header_as_dirty: op=183 checksum=53488760202783532770282344540085536021
2025-11-24 19:42:46.903Z debug(replica): 1n: append: appending to journal op=183
2025-11-24 19:42:46.903Z debug(journal): 1: write: view=3 slot=183 op=183 len=277120: 53488760202783532770282344540085536021 starting
2025-11-24 19:42:46.903Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=191889408 len=278528 locked
2025-11-24 19:42:46.903Z debug(replica): 1n: commit_start_journal: cached prepare op=182 checksum=25052576896655597876054065737995279801
2025-11-24 19:42:46.903Z debug(replica): 1n: repair_prepare: op=182 checksum=25052576896655597876054065737995279801 (already writing)
2025-11-24 19:42:46.903Z debug(replica): 1n: repair_prepare: op=183 checksum=53488760202783532770282344540085536021 (already writing)
2025-11-24 19:42:46.903Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=181)
2025-11-24 19:42:46.903Z debug(replica): 1n: execute_op: executing view=3 primary=false op=182 checksum=25052576896655597876054065737995279801 (lookup_accounts)
2025-11-24 19:42:46.903Z debug(replica): 1n: execute_op: commit_timestamp=1764013361503211221 prepare.header.timestamp=1764013363958138868
2025-11-24 19:42:46.903Z debug(replica): 1n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=180
2025-11-24 19:42:46.904Z debug(forest): entering forest.compact() op=182 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:46.904Z debug(client_replies): 1: write_reply: wrote (client=323321802946825702132615013286692535219 request=180)
2025-11-24 19:42:46.904Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:46.904Z debug(journal): 1: write: view=3 slot=182 op=182 len=2320: 25052576896655597876054065737995279801 complete, marking clean
2025-11-24 19:42:46.904Z debug(replica): 1n: send_prepare_ok: op=182 checksum=25052576896655597876054065737995279801
2025-11-24 19:42:46.904Z debug(replica): 1n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 201481939602201131180774849794628183804, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 312311258755631104165807379498885171188, .parent_padding = 0, .prepare_checksum = 25052576896655597876054065737995279801, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 182, .commit_min = 182, .timestamp = 1764013363958138868, .request = 180, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.904Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=191889408 len=278528 unlocked
2025-11-24 19:42:46.904Z debug(journal): 1: write_header: op=183 sectors[45056..49152]
2025-11-24 19:42:46.904Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:46.904Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:46.904Z debug(journal): 1: write: view=3 slot=183 op=183 len=277120: 53488760202783532770282344540085536021 complete, marking clean
2025-11-24 19:42:46.904Z debug(replica): 1n: send_prepare_ok: op=183 checksum=53488760202783532770282344540085536021
2025-11-24 19:42:46.904Z debug(replica): 1n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 39783289556138004745500729170896336005, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .prepare_checksum = 53488760202783532770282344540085536021, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit_min = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.906Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:46.906Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:46.906Z debug(client_replies): 0: write_reply: wrote (client=323321802946825702132615013286692535219 request=181)
2025-11-24 19:42:46.907Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 190606545017241956841360206291921099089, .checksum_padding = 0, .checksum_body = 216384004706124251848491798582181076044, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 277120, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 13419321619857820417170139502338057035, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 181, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 40732384, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.907Z debug(replica): 0N: on_request: replying to duplicate request
2025-11-24 19:42:46.907Z debug(client_replies): 0: read_reply: start (client=323321802946825702132615013286692535219 reply=208929120119286881451163540399546182245)
2025-11-24 19:42:46.907Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 39783289556138004745500729170896336005, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 25052576896655597876054065737995279801, .parent_padding = 0, .prepare_checksum = 53488760202783532770282344540085536021, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit_min = 182, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.907Z debug(replica): 0N: on_prepare_ok: not preparing op=183 checksum=53488760202783532770282344540085536021
2025-11-24 19:42:46.907Z debug(client_replies): 0: read_reply: done (client=323321802946825702132615013286692535219 reply=208929120119286881451163540399546182245)
2025-11-24 19:42:46.907Z debug(replica): 0N: on_request: repeat reply (client=323321802946825702132615013286692535219 request=181)
2025-11-24 19:42:46.907Z debug(replica): 0N: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 208929120119286881451163540399546182245, .checksum_padding = 0, .checksum_body = 252271904436463863891991372994883614728, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 190606545017241956841360206291921099089, .request_checksum_padding = 0, .context = 142006707948578513506132462770086189799, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit = 183, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.908Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:46.908Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:46.909Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 303774161206467708828674460996194695368, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 142006707948578513506132462770086189799, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 182, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2901485744, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.909Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:46.909Z debug(replica): 0N: primary_pipeline_prepare: request checksum=303774161206467708828674460996194695368 client=323321802946825702132615013286692535219
2025-11-24 19:42:46.909Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 303774161206467708828674460996194695368, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 142006707948578513506132462770086189799, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 182, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2901485744, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.909Z debug(replica): 1n: on_request: forwarding new request to primary (view=3)
2025-11-24 19:42:46.909Z debug(replica): 0N: primary_pipeline_prepare: prepare checksum=138515667987549449509472186458062982598 op=184
2025-11-24 19:42:46.909Z debug(vsr): 0: prepare_timeout started
2025-11-24 19:42:46.909Z debug(vsr): 0: primary_abdicate_timeout started
2025-11-24 19:42:46.909Z debug(replica): 1n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 303774161206467708828674460996194695368, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 142006707948578513506132462770086189799, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 182, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2901485744, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.909Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:46.909Z debug(replica): 0N: replicate: replicating op=184 to replica 1
2025-11-24 19:42:46.909Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 138515667987549449509472186458062982598, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .request_checksum = 303774161206467708828674460996194695368, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.909Z debug(replica): 0N: replicate: replicating op=184 to replica 2
2025-11-24 19:42:46.909Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 138515667987549449509472186458062982598, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .request_checksum = 303774161206467708828674460996194695368, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.909Z debug(replica): 0N: on_prepare: advancing: op=183..184 checksum=53488760202783532770282344540085536021..138515667987549449509472186458062982598
2025-11-24 19:42:46.909Z debug(replica): 1n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 138515667987549449509472186458062982598, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .request_checksum = 303774161206467708828674460996194695368, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.909Z debug(journal): 0: set_header_as_dirty: op=184 checksum=138515667987549449509472186458062982598
2025-11-24 19:42:46.909Z debug(replica): 0N: append: appending to journal op=184
2025-11-24 19:42:46.909Z debug(replica): 1n: on_prepare: advancing commit_max=182..183
2025-11-24 19:42:46.909Z debug(replica): 1n: on_prepare: caching prepare.op=184 (commit_min=182 op=183 commit_max=183 prepare_max=1007)
2025-11-24 19:42:46.909Z debug(journal): 0: write: view=3 slot=184 op=184 len=2320: 138515667987549449509472186458062982598 starting
2025-11-24 19:42:46.909Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=192937984 len=4096 locked
2025-11-24 19:42:46.910Z debug(replica): 1n: on_prepare: advancing: op=183..184 checksum=53488760202783532770282344540085536021..138515667987549449509472186458062982598
2025-11-24 19:42:46.910Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Prepare{ .checksum = 138515667987549449509472186458062982598, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .request_checksum = 303774161206467708828674460996194695368, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.910Z debug(journal): 1: set_header_as_dirty: op=184 checksum=138515667987549449509472186458062982598
2025-11-24 19:42:46.910Z debug(replica): 1n: append: appending to journal op=184
2025-11-24 19:42:46.910Z debug(replica): 2n: on_prepare: advancing commit_max=182..183
2025-11-24 19:42:46.910Z debug(journal): 1: write: view=3 slot=184 op=184 len=2320: 138515667987549449509472186458062982598 starting
2025-11-24 19:42:46.910Z debug(replica): 2n: on_prepare: caching prepare.op=184 (commit_min=182 op=183 commit_max=183 prepare_max=1007)
2025-11-24 19:42:46.910Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=192937984 len=4096 locked
2025-11-24 19:42:46.910Z debug(replica): 1n: commit_start_journal: cached prepare op=183 checksum=53488760202783532770282344540085536021
2025-11-24 19:42:46.910Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 303774161206467708828674460996194695368, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 142006707948578513506132462770086189799, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 182, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2901485744, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.910Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:46.910Z debug(replica): 0N: on_request: ignoring (already preparing)
2025-11-24 19:42:46.910Z debug(replica): 2n: on_prepare: advancing: op=183..184 checksum=53488760202783532770282344540085536021..138515667987549449509472186458062982598
2025-11-24 19:42:46.910Z debug(journal): 2: set_header_as_dirty: op=184 checksum=138515667987549449509472186458062982598
2025-11-24 19:42:46.910Z debug(replica): 2n: append: appending to journal op=184
2025-11-24 19:42:46.910Z debug(journal): 2: write: view=3 slot=184 op=184 len=2320: 138515667987549449509472186458062982598 starting
2025-11-24 19:42:46.910Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=192937984 len=4096 locked
2025-11-24 19:42:46.910Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=192937984 len=4096 unlocked
2025-11-24 19:42:46.910Z debug(replica): 2n: commit_start_journal: cached prepare op=183 checksum=53488760202783532770282344540085536021
2025-11-24 19:42:46.910Z debug(journal): 0: write_header: op=184 sectors[45056..49152]
2025-11-24 19:42:46.910Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:46.910Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:46.910Z debug(journal): 0: write: view=3 slot=184 op=184 len=2320: 138515667987549449509472186458062982598 complete, marking clean
2025-11-24 19:42:46.910Z debug(replica): 0N: send_prepare_ok: op=184 checksum=138515667987549449509472186458062982598
2025-11-24 19:42:46.910Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 312873033446748126344314569900215057800, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .prepare_checksum = 138515667987549449509472186458062982598, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit_min = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.910Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 312873033446748126344314569900215057800, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .prepare_checksum = 138515667987549449509472186458062982598, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit_min = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.910Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:46.910Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 19:42:46.910Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 19:42:46.910Z debug(replica): 1n: repair_prepare: op=184 checksum=138515667987549449509472186458062982598 (already writing)
2025-11-24 19:42:46.910Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=182)
2025-11-24 19:42:46.910Z debug(replica): 2n: repair_prepare: op=184 checksum=138515667987549449509472186458062982598 (already writing)
2025-11-24 19:42:46.910Z debug(replica): 2n: commit_journal: already committing (prefetch; commit_min=182)
2025-11-24 19:42:46.911Z debug(replica): 1n: execute_op: executing view=3 primary=false op=183 checksum=53488760202783532770282344540085536021 (create_transfers)
2025-11-24 19:42:46.911Z debug(replica): 1n: execute_op: commit_timestamp=1764013363958138868 prepare.header.timestamp=1764013364003121301
2025-11-24 19:42:46.911Z debug(replica): 2n: execute_op: executing view=3 primary=false op=183 checksum=53488760202783532770282344540085536021 (create_transfers)
2025-11-24 19:42:46.911Z debug(replica): 2n: execute_op: commit_timestamp=1764013363958138868 prepare.header.timestamp=1764013364003121301
2025-11-24 19:42:46.919Z debug(replica): 2n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=181
2025-11-24 19:42:46.919Z debug(forest): entering forest.compact() op=183 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:46.919Z debug(replica): 1n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=181
2025-11-24 19:42:46.919Z debug(replica): 1n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 208929120119286881451163540399546182245, .checksum_padding = 0, .checksum_body = 252271904436463863891991372994883614728, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 190606545017241956841360206291921099089, .request_checksum_padding = 0, .context = 142006707948578513506132462770086189799, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit = 183, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.919Z debug(replica): 1n: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 208929120119286881451163540399546182245, .checksum_padding = 0, .checksum_body = 252271904436463863891991372994883614728, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 344, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 190606545017241956841360206291921099089, .request_checksum_padding = 0, .context = 142006707948578513506132462770086189799, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 183, .commit = 183, .timestamp = 1764013364003121301, .request = 181, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.919Z debug(forest): entering forest.compact() op=183 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:46.923Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=192937984 len=4096 unlocked
2025-11-24 19:42:46.923Z debug(journal): 2: write_header: op=184 sectors[45056..49152]
2025-11-24 19:42:46.923Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:46.923Z debug(client_replies): 2: write_reply: wrote (client=323321802946825702132615013286692535219 request=181)
2025-11-24 19:42:46.924Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:46.924Z debug(journal): 2: write: view=3 slot=184 op=184 len=2320: 138515667987549449509472186458062982598 complete, marking clean
2025-11-24 19:42:46.924Z debug(replica): 2n: send_prepare_ok: op=184 checksum=138515667987549449509472186458062982598
2025-11-24 19:42:46.924Z debug(replica): 2n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 133886779829452063309572058383140848627, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .prepare_checksum = 138515667987549449509472186458062982598, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit_min = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.924Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:46.924Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:46.924Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=192937984 len=4096 unlocked
2025-11-24 19:42:46.924Z debug(journal): 1: write_header: op=184 sectors[45056..49152]
2025-11-24 19:42:46.924Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 133886779829452063309572058383140848627, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .prepare_checksum = 138515667987549449509472186458062982598, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit_min = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.924Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:46.924Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:46.924Z debug(client_replies): 1: write_reply: wrote (client=323321802946825702132615013286692535219 request=181)
2025-11-24 19:42:46.924Z debug(replica): 0N: on_prepare_ok: 2 message(s)
2025-11-24 19:42:46.924Z debug(replica): 0N: on_prepare_ok: quorum received, context=138515667987549449509472186458062982598
2025-11-24 19:42:46.924Z debug(vsr): 0: prepare_timeout stopped
2025-11-24 19:42:46.924Z debug(vsr): 0: primary_abdicate_timeout stopped
2025-11-24 19:42:46.924Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:46.924Z debug(journal): 1: write: view=3 slot=184 op=184 len=2320: 138515667987549449509472186458062982598 complete, marking clean
2025-11-24 19:42:46.924Z debug(replica): 1n: send_prepare_ok: op=184 checksum=138515667987549449509472186458062982598
2025-11-24 19:42:46.924Z debug(replica): 1n: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 72457163588242066621364091393933159507, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .prepare_checksum = 138515667987549449509472186458062982598, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit_min = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:46.924Z debug(replica): 0N: execute_op: executing view=3 primary=true op=184 checksum=138515667987549449509472186458062982598 (lookup_accounts)
2025-11-24 19:42:46.924Z debug(replica): 0N: execute_op: commit_timestamp=1764013364003121301 prepare.header.timestamp=1764013366909837307
2025-11-24 19:42:46.924Z debug(replica): 0N: execute_op: advancing commit_max=183..184
2025-11-24 19:42:46.924Z debug(replica): 0N: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=182
2025-11-24 19:42:46.924Z debug(replica): 0N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 152059556477938876518485625633977886279, .checksum_padding = 0, .checksum_body = 219310283719235677857233250670414289984, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 303774161206467708828674460996194695368, .request_checksum_padding = 0, .context = 117575418631240764473290073356656454255, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit = 184, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.933Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:46.933Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:46.933Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 19:42:46.933Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 19:42:46.944Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:46.944Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:46.944Z debug(vsr): 1: journal_repair_timeout fired
2025-11-24 19:42:46.944Z debug(vsr): 1: journal_repair_timeout reset
2025-11-24 19:42:46.954Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:46.954Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:46.924Z debug(replica): 0N: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 152059556477938876518485625633977886279, .checksum_padding = 0, .checksum_body = 219310283719235677857233250670414289984, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 303774161206467708828674460996194695368, .request_checksum_padding = 0, .context = 117575418631240764473290073356656454255, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit = 184, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:46.964Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:46.974Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:49.868Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:47.389Z info(supervisor): injecting network delays: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 477, .jitter_ms = 50 }, .lose = 4/100, .corrupt = null }
2025-11-24 19:42:49.868Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:49.868Z debug(forest): entering forest.compact() op=184 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-24 19:42:49.869Z warning(replica): 0N: commit_dispatch: slow request, request=182 size=2320 lookup_accounts time=2944ms
2025-11-24 19:42:49.869Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 72457163588242066621364091393933159507, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 53488760202783532770282344540085536021, .parent_padding = 0, .prepare_checksum = 138515667987549449509472186458062982598, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit_min = 183, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:49.869Z debug(replica): 0N: on_prepare_ok: not preparing op=184 checksum=138515667987549449509472186458062982598
2025-11-24 19:42:49.869Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:49.869Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:49.869Z debug(client_replies): 0: write_reply: wrote (client=323321802946825702132615013286692535219 request=182)
2025-11-24 19:42:49.878Z info(supervisor): injecting network corruption: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 477, .jitter_ms = 50 }, .lose = 4/100, .corrupt = 10/100 }
2025-11-24 19:42:49.888Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:49.888Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-24 19:42:49.888Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-24 19:42:49.888Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:49.888Z info(supervisor): 1: terminating replica
2025-11-24 19:42:49.889Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:49.889Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:49.889Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 19:42:49.889Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 19:42:49.899Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 19:42:49.899Z info(message_bus): 2: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 19:42:49.899Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=54ms
2025-11-24 19:42:49.908Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:49.908Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:49.909Z info(supervisor): healing network
2025-11-24 19:42:49.909Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:49.909Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:49.919Z info(supervisor): 1: starting replica
2025-11-24 19:42:49.922Z info(io): opening "0_1.tigerbeetle"...
2025-11-24 19:42:49.928Z debug(vsr): 2: ping_timeout fired
2025-11-24 19:42:49.928Z debug(vsr): 2: ping_timeout reset
2025-11-24 19:42:49.928Z debug(replica): 2n: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 313552576943094162299486002754379884354, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133452890518222, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.929Z debug(replica): 2n: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 313552576943094162299486002754379884354, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133452890518222, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.929Z debug(message_bus): 2: send_message_to_replica: no connection to=1 header=vsr.message_header.Header.Ping{ .checksum = 313552576943094162299486002754379884354, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133452890518222, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.929Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-24 19:42:49.929Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Ping{ .checksum = 313552576943094162299486002754379884354, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133452890518222, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.929Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-24 19:42:49.929Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:49.929Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:49.929Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 19:42:49.929Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 19:42:49.929Z debug(replica): 0N: sending pong to replica 2: vsr.message_header.Header.Pong{ .checksum = 310633747274363886135262514780816860681, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36133452890518222, .pong_timestamp_wall = 1764013369929137330, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.929Z debug(vsr): 2: repair_sync_timeout fired
2025-11-24 19:42:49.929Z debug(vsr): 2: repair_sync_timeout reset
2025-11-24 19:42:49.929Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-24 19:42:49.929Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-24 19:42:49.929Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Pong{ .checksum = 310633747274363886135262514780816860681, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36133452890518222, .pong_timestamp_wall = 1764013369929137330, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.929Z debug(clock): 2: learn: replica=0 m0=36133452890518222 t1=1764013369929137330 m2=36133452890898091 t2=1764013369929327640 one_way_delay=189934 asymmetric_delay=0 clock_offset=-376
2025-11-24 19:42:49.929Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:49.929Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:49.930Z info(supervisor): 2: pausing replica
2025-11-24 19:42:49.940Z info(supervisor): 2: unpausing replica
2025-11-24 19:42:49.940Z debug(clock): 2: synchronized: truechimers=2/3 clock_offset=0ns..0ns accuracy=0ns
2025-11-24 19:42:49.940Z debug(clock): 2: system time is 190ns behind
2025-11-24 19:42:49.943Z info(main): multiversioning: upgrades disabled for development (0.0.1) release.
2025-11-24 19:42:49.943Z info(main): release=0.0.1
2025-11-24 19:42:49.943Z info(main): release_client_min=0.0.1
2025-11-24 19:42:49.943Z info(main): releases_bundled={ 0.0.1 }
2025-11-24 19:42:49.943Z info(main): git_commit=dc83a51708473953bf01abc04fbd1571c6d4c634
2025-11-24 19:42:49.943Z debug(superblock): null: open: started
2025-11-24 19:42:49.943Z debug(superblock): null: open: read_header: copy=0 size=8192 offset=0
2025-11-24 19:42:49.944Z debug(superblock): null: open: read_header: copy=1 size=8192 offset=24576
2025-11-24 19:42:49.944Z debug(superblock): null: open: read_header: copy=2 size=8192 offset=49152
2025-11-24 19:42:49.944Z debug(superblock): null: open: read_header: copy=3 size=8192 offset=73728
2025-11-24 19:42:49.944Z debug(superblock_quorums): copy: 0/4: checksum=ae13b8aa0dec472fac6e979378a12b5d parent=3a93d13ea2ee555894de799783951059 sequence=7
2025-11-24 19:42:49.944Z debug(superblock_quorums): copy: 1/4: checksum=ae13b8aa0dec472fac6e979378a12b5d parent=3a93d13ea2ee555894de799783951059 sequence=7
2025-11-24 19:42:49.944Z debug(superblock_quorums): copy: 2/4: checksum=ae13b8aa0dec472fac6e979378a12b5d parent=3a93d13ea2ee555894de799783951059 sequence=7
2025-11-24 19:42:49.944Z debug(superblock_quorums): copy: 3/4: checksum=ae13b8aa0dec472fac6e979378a12b5d parent=3a93d13ea2ee555894de799783951059 sequence=7
2025-11-24 19:42:49.944Z debug(superblock_quorums): quorum: checksum=ae13b8aa0dec472fac6e979378a12b5d parent=3a93d13ea2ee555894de799783951059 sequence=7 count=4 valid=true
2025-11-24 19:42:49.944Z debug(superblock): null: open: installed working superblock: checksum=ae13b8aa0dec472fac6e979378a12b5d sequence=7 release=0.0.1 cluster=00000000000000000000000000000000 replica_id=308943487097555535311203420603596972560 size=1141374976 free_set_blocks_acquired_size=0 free_set_blocks_released_size=0 client_sessions_size=0 checkpoint_id=f222e9ce156b309eaeb4af665242ac18 commit_min_checksum=108034676951432761169128540124443993015 commit_min=0 commit_max=109 log_view=3 view=3 sync_op_min=0 sync_op_max=0 manifest_oldest_checksum=0 manifest_oldest_address=0 manifest_newest_checksum=0 manifest_newest_address=0 manifest_block_count=0 snapshots_block_checksum=0 snapshots_block_address=0
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=108 checksum=269424224259698079665785883913259314127
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=107 checksum=292466678534980152324279158346296292713
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=106 checksum=105341368212795634903019432753214198874
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=105 checksum=265498337545802085734253534209599478660
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=104 checksum=1691139231469353254346335670301789046
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=103 checksum=213922544731599136696162110210278520482
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=102 checksum=140960597464479022902439535880836264567
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=101 checksum=65270821381375612733175910048221712989
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=100 checksum=225041494554652605925572847001541297405
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=47 checksum=10245711739337340841208990259649090076
2025-11-24 19:42:49.944Z debug(superblock): null: open: vsr_header: op=0 checksum=108034676951432761169128540124443993015
2025-11-24 19:42:49.944Z debug(superblock): null: open: complete
2025-11-24 19:42:49.944Z debug(journal): 1: slot_count=1024 size=1.000244140625GiB headers_size=256KiB prepares_size=1GiB
2025-11-24 19:42:49.949Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:49.949Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:49.949Z debug(vsr): 0: pulse_timeout fired
2025-11-24 19:42:49.949Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:49.950Z info(supervisor): injecting network loss: testing.vortex.faulty_network.Faults{ .delay = null, .lose = 8/100, .corrupt = null }
2025-11-24 19:42:49.950Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:49.950Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:49.953Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 19:42:49.953Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 19:42:49.953Z warning(faulty_network): connect failed (1,9): error.ConnectionRefused
2025-11-24 19:42:49.953Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 19:42:49.959Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=53ms
2025-11-24 19:42:49.960Z info(supervisor): sleeping for 1.042s
2025-11-24 19:42:49.969Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:49.969Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:49.970Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:49.970Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:49.977Z warning(faulty_network): connect failed (1,1): error.ConnectionRefused
2025-11-24 19:42:49.989Z debug(vsr): 0: ping_timeout fired
2025-11-24 19:42:49.989Z debug(vsr): 0: ping_timeout reset
2025-11-24 19:42:49.989Z debug(replica): 0N: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 66245860449723699898035283462549135651, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133452951527914, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905473, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(replica): 0N: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 66245860449723699898035283462549135651, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133452951527914, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905473, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(vsr): 0: commit_message_timeout fired
2025-11-24 19:42:49.990Z debug(vsr): 0: commit_message_timeout reset
2025-11-24 19:42:49.990Z debug(replica): 0N: sending commit to replica 1: vsr.message_header.Header.Commit{ .checksum = 114905152287082622028574156100707376569, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 138515667987549449509472186458062982598, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 184, .timestamp_monotonic = 36133452951640733, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(replica): 0N: sending commit to replica 2: vsr.message_header.Header.Commit{ .checksum = 114905152287082622028574156100707376569, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 138515667987549449509472186458062982598, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 184, .timestamp_monotonic = 36133452951640733, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(vsr): 0: start_view_change_message_timeout fired
2025-11-24 19:42:49.990Z debug(vsr): 0: start_view_change_message_timeout reset
2025-11-24 19:42:49.990Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:49.990Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Ping{ .checksum = 66245860449723699898035283462549135651, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 36133452951527914, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692905473, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:49.990Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 19:42:49.990Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 19:42:49.990Z debug(replica): 2n: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 289180231697166935390940013731521193275, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36133452951527914, .pong_timestamp_wall = 1764013369990158782, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-11-24 19:42:49.990Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-11-24 19:42:49.990Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Commit{ .checksum = 114905152287082622028574156100707376569, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 138515667987549449509472186458062982598, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 184, .timestamp_monotonic = 36133452951640733, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(vsr): 2: normal_heartbeat_timeout reset
2025-11-24 19:42:49.990Z debug(replica): 2n: on_commit: checksum verified
2025-11-24 19:42:49.990Z debug(replica): 2n: on_commit: advancing commit_max=183..184
2025-11-24 19:42:49.990Z debug(replica): 2n: commit_start_journal: cached prepare op=184 checksum=138515667987549449509472186458062982598
2025-11-24 19:42:49.990Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Pong{ .checksum = 289180231697166935390940013731521193275, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 36133452951527914, .pong_timestamp_wall = 1764013369990158782, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(clock): 0: learn: replica=2 m0=36133452951527914 t1=1764013369990158782 m2=36133452951925553 t2=1764013369990354982 one_way_delay=198819 asymmetric_delay=0 clock_offset=2619
2025-11-24 19:42:49.990Z debug(replica): 2n: execute_op: executing view=3 primary=false op=184 checksum=138515667987549449509472186458062982598 (lookup_accounts)
2025-11-24 19:42:49.990Z debug(replica): 2n: execute_op: commit_timestamp=1764013364003121301 prepare.header.timestamp=1764013366909837307
2025-11-24 19:42:49.990Z debug(replica): 2n: client_table_entry_update: client=323321802946825702132615013286692535219 session=2 request=182
2025-11-24 19:42:49.990Z debug(replica): 2n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 152059556477938876518485625633977886279, .checksum_padding = 0, .checksum_body = 219310283719235677857233250670414289984, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 303774161206467708828674460996194695368, .request_checksum_padding = 0, .context = 117575418631240764473290073356656454255, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit = 184, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(replica): 2n: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 152059556477938876518485625633977886279, .checksum_padding = 0, .checksum_body = 219310283719235677857233250670414289984, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 303774161206467708828674460996194695368, .request_checksum_padding = 0, .context = 117575418631240764473290073356656454255, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit = 184, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:49.990Z debug(forest): entering forest.compact() op=184 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 323321802946825702132615013286692535219: on_reply: slow request, request=182 op=184 size=2320 lookup_accounts time=3081ms
2025-11-24 19:42:49.991Z info(workload): accounts created = 128, transfers = 256805, pending transfers = 0, commands run = 91
2025-11-24 19:42:49.991Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:49.991Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:49.991Z debug(client_replies): 2: write_reply: wrote (client=323321802946825702132615013286692535219 request=182)
2025-11-24 19:42:50.000Z debug(clock): 0: synchronized: truechimers=2/3 clock_offset=0ns..0ns accuracy=0ns
2025-11-24 19:42:50.000Z debug(clock): 0: system time is 40ns behind
2025-11-24 19:42:50.006Z debug(replica): 2n: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 310918912720831606903191227949330477117, .checksum_padding = 0, .checksum_body = 300223529602328420631829499032162388468, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1018368, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 117575418631240764473290073356656454255, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 183, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3081376852, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:50.006Z debug(replica): 2n: on_request: forwarding new request to primary (view=3)
2025-11-24 19:42:50.006Z debug(replica): 2n: sending request to replica 0: vsr.message_header.Header.Request{ .checksum = 310918912720831606903191227949330477117, .checksum_padding = 0, .checksum_body = 300223529602328420631829499032162388468, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1018368, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 117575418631240764473290073356656454255, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 183, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3081376852, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:50.011Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.011Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.011Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 310918912720831606903191227949330477117, .checksum_padding = 0, .checksum_body = 300223529602328420631829499032162388468, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1018368, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 117575418631240764473290073356656454255, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 183, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 3081376852, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-24 19:42:50.011Z debug(replica): 0N: on_request: new request
2025-11-24 19:42:50.011Z debug(replica): 0N: primary_pipeline_prepare: request checksum=310918912720831606903191227949330477117 client=323321802946825702132615013286692535219
2025-11-24 19:42:50.015Z debug(replica): 0N: primary_pipeline_prepare: prepare checksum=78803742816023192353387521627953279089 op=185
2025-11-24 19:42:50.015Z debug(vsr): 0: prepare_timeout started
2025-11-24 19:42:50.015Z debug(vsr): 0: primary_abdicate_timeout started
2025-11-24 19:42:50.015Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:50.015Z debug(replica): 0N: replicate: replicating op=185 to replica 1
2025-11-24 19:42:50.015Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 78803742816023192353387521627953279089, .checksum_padding = 0, .checksum_body = 300223529602328420631829499032162388468, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1018368, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138515667987549449509472186458062982598, .parent_padding = 0, .request_checksum = 310918912720831606903191227949330477117, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 185, .commit = 184, .timestamp = 1764013370011280260, .request = 183, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:50.015Z debug(replica): 0N: replicate: replicating op=185 to replica 2
2025-11-24 19:42:50.015Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 78803742816023192353387521627953279089, .checksum_padding = 0, .checksum_body = 300223529602328420631829499032162388468, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1018368, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138515667987549449509472186458062982598, .parent_padding = 0, .request_checksum = 310918912720831606903191227949330477117, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 185, .commit = 184, .timestamp = 1764013370011280260, .request = 183, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:50.015Z debug(replica): 0N: on_prepare: advancing: op=184..185 checksum=138515667987549449509472186458062982598..78803742816023192353387521627953279089
2025-11-24 19:42:50.015Z debug(journal): 0: set_header_as_dirty: op=185 checksum=78803742816023192353387521627953279089
2025-11-24 19:42:50.015Z debug(replica): 0N: append: appending to journal op=185
2025-11-24 19:42:50.015Z debug(journal): 0: write: view=3 slot=185 op=185 len=1018368: 78803742816023192353387521627953279089 starting
2025-11-24 19:42:50.015Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=193986560 len=1019904 locked
2025-11-24 19:42:50.015Z warning(message_bus): 2: on_recv: from=vsr.Peer{ .replica = 0 } terminating connection: invalid header_checksum
2025-11-24 19:42:50.015Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 19:42:50.016Z warning(faulty_network): send error (2,2): error.ConnectionResetByPeer
2025-11-24 19:42:50.016Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-11-24 19:42:50.016Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 19:42:50.016Z warning(faulty_network): connect failed (1,2): error.ConnectionRefused
2025-11-24 19:42:50.016Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 19:42:50.016Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=70ms
2025-11-24 19:42:50.016Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=81ms
2025-11-24 19:42:50.016Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.016Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.016Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=193986560 len=1019904 unlocked
2025-11-24 19:42:50.016Z debug(journal): 0: write_header: op=185 sectors[45056..49152]
2025-11-24 19:42:50.016Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 locked
2025-11-24 19:42:50.016Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=45056 len=4096 unlocked
2025-11-24 19:42:50.016Z debug(journal): 0: write: view=3 slot=185 op=185 len=1018368: 78803742816023192353387521627953279089 complete, marking clean
2025-11-24 19:42:50.016Z debug(replica): 0N: send_prepare_ok: op=185 checksum=78803742816023192353387521627953279089
2025-11-24 19:42:50.016Z debug(replica): 0N: sending prepare_ok to replica 0: vsr.message_header.Header.PrepareOk{ .checksum = 234146575283848282533812542684515618119, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138515667987549449509472186458062982598, .parent_padding = 0, .prepare_checksum = 78803742816023192353387521627953279089, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 185, .commit_min = 184, .timestamp = 1764013370011280260, .request = 183, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:50.016Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 234146575283848282533812542684515618119, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 3, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138515667987549449509472186458062982598, .parent_padding = 0, .prepare_checksum = 78803742816023192353387521627953279089, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 185, .commit_min = 184, .timestamp = 1764013370011280260, .request = 183, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:50.016Z debug(vsr): 0: primary_abdicate_timeout reset
2025-11-24 19:42:50.016Z debug(replica): 0N: on_prepare_ok: 1 message(s)
2025-11-24 19:42:50.016Z debug(replica): 0N: on_prepare_ok: waiting for quorum
2025-11-24 19:42:50.031Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.031Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.031Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 19:42:50.031Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 19:42:50.036Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.036Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.051Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.051Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.055Z warning(faulty_network): connect failed (1,4): error.ConnectionRefused
2025-11-24 19:42:50.056Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.056Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.060Z debug(manifest_log): 1: Manifest.Pace.half_bar_append_blocks_max = 1
2025-11-24 19:42:50.060Z debug(manifest_log): 1: Manifest.Pace.half_bar_compact_blocks_max = 2
2025-11-24 19:42:50.060Z debug(manifest_log): 1: Manifest.Pace.log_blocks_full_max = 586
2025-11-24 19:42:50.060Z debug(manifest_log): 1: Manifest.Pace.log_blocks_cycle_max = 1172
2025-11-24 19:42:50.060Z debug(manifest_log): 1: Manifest.Pace.log_blocks_max = 1466
2025-11-24 19:42:50.060Z debug(manifest_log): 1: Manifest.Pace.tables_max = 2396744
2025-11-24 19:42:50.071Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.071Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.076Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.076Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.086Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 19:42:50.086Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 19:42:50.086Z warning(faulty_network): connect failed (1,3): error.ConnectionRefused
2025-11-24 19:42:50.087Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 19:42:50.091Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.091Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.096Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=55ms
2025-11-24 19:42:50.096Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.096Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.096Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 19:42:50.096Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 19:42:50.097Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-11-24 19:42:50.097Z info(message_bus): 0: on_connect: connected to=2
2025-11-24 19:42:50.107Z debug(vsr): 0: pulse_timeout fired
2025-11-24 19:42:50.107Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:50.111Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.111Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.117Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.117Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.131Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.131Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.131Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 19:42:50.131Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 19:42:50.137Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.137Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.147Z warning(faulty_network): connect failed (1,5): error.ConnectionRefused
2025-11-24 19:42:50.152Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.152Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.152Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 19:42:50.152Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 19:42:50.152Z warning(faulty_network): connect failed (1,6): error.ConnectionRefused
2025-11-24 19:42:50.152Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 1 } orderly shutdown
2025-11-24 19:42:50.157Z debug(message_bus): 0: connect_to_replica: connecting to=1 after=66ms
2025-11-24 19:42:50.157Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.157Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.172Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.172Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.177Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.177Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.192Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.192Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.197Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.197Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.197Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 19:42:50.197Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 19:42:50.207Z debug(vsr): 0: pulse_timeout fired
2025-11-24 19:42:50.207Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:50.210Z debug(replica): 1r: init: replica_count=3 quorum_view_change=2 quorum_replication=2 release=0.0.1
2025-11-24 19:42:50.211Z info(replica): superblock release=0.0.1
2025-11-24 19:42:50.211Z debug(journal): 1: recover: recovering
2025-11-24 19:42:50.211Z debug(journal): 1: recover_headers: offset=0 size=262144 recovering
2025-11-24 19:42:50.211Z debug(journal): 1: recover_headers: offset=0 size=262144 recovered
2025-11-24 19:42:50.211Z debug(journal): 1: recover_headers: complete
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=0
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=1
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=2
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=3
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=4
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=5
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=6
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=7
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=8
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=9
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=10
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=11
2025-11-24 19:42:50.211Z debug(journal): 1: recover_prepare: recovering slot=12
2025-11-24 19:42:50.212Z debug(journal): 1: recover_prepare: recovering slot=13
2025-11-24 19:42:50.212Z debug(journal): 1: recover_prepare: recovering slot=14
2025-11-24 19:42:50.212Z debug(journal): 1: recover_prepare: recovering slot=15
2025-11-24 19:42:50.212Z debug(journal): 1: recover_prepare: recovering slot=16
2025-11-24 19:42:50.212Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.212Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.212Z debug(journal): 1: recover_prepare: recovering slot=17
2025-11-24 19:42:50.216Z debug(journal): 1: recover_prepare: recovering slot=18
2025-11-24 19:42:50.216Z debug(journal): 1: recover_prepare: recovering slot=19
2025-11-24 19:42:50.217Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.217Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.220Z debug(journal): 1: recover_prepare: recovering slot=20
2025-11-24 19:42:50.220Z debug(journal): 1: recover_prepare: recovering slot=21
2025-11-24 19:42:50.221Z debug(journal): 1: recover_prepare: recovering slot=22
2025-11-24 19:42:50.221Z debug(journal): 1: recover_prepare: recovering slot=23
2025-11-24 19:42:50.223Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=1
2025-11-24 19:42:50.223Z info(message_bus): 0: on_connect: connected to=1
2025-11-24 19:42:50.225Z debug(journal): 1: recover_prepare: recovering slot=24
2025-11-24 19:42:50.226Z debug(journal): 1: recover_prepare: recovering slot=25
2025-11-24 19:42:50.226Z debug(journal): 1: recover_prepare: recovering slot=26
2025-11-24 19:42:50.226Z debug(journal): 1: recover_prepare: recovering slot=27
2025-11-24 19:42:50.227Z debug(journal): 1: recover_prepare: recovering slot=28
2025-11-24 19:42:50.227Z debug(journal): 1: recover_prepare: recovering slot=29
2025-11-24 19:42:50.227Z debug(journal): 1: recover_prepare: recovering slot=30
2025-11-24 19:42:50.227Z debug(journal): 1: recover_prepare: recovering slot=31
2025-11-24 19:42:50.228Z debug(journal): 1: recover_prepare: recovering slot=32
2025-11-24 19:42:50.228Z debug(journal): 1: recover_prepare: recovering slot=33
2025-11-24 19:42:50.228Z debug(journal): 1: recover_prepare: recovering slot=34
2025-11-24 19:42:50.232Z debug(journal): 1: recover_prepare: recovering slot=35
2025-11-24 19:42:50.232Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.232Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.232Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 19:42:50.232Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 19:42:50.232Z debug(journal): 1: recover_prepare: recovering slot=36
2025-11-24 19:42:50.232Z debug(journal): 1: recover_prepare: recovering slot=37
2025-11-24 19:42:50.232Z debug(journal): 1: recover_prepare: recovering slot=38
2025-11-24 19:42:50.232Z debug(journal): 1: recover_prepare: recovering slot=39
2025-11-24 19:42:50.234Z debug(journal): 1: recover_prepare: recovering slot=40
2025-11-24 19:42:50.234Z debug(journal): 1: recover_prepare: recovering slot=41
2025-11-24 19:42:50.234Z debug(journal): 1: recover_prepare: recovering slot=42
2025-11-24 19:42:50.236Z debug(journal): 1: recover_prepare: recovering slot=43
2025-11-24 19:42:50.237Z debug(journal): 1: recover_prepare: recovering slot=44
2025-11-24 19:42:50.237Z debug(journal): 1: recover_prepare: recovering slot=45
2025-11-24 19:42:50.237Z debug(journal): 1: recover_prepare: recovering slot=46
2025-11-24 19:42:50.237Z debug(journal): 1: recover_prepare: recovering slot=47
2025-11-24 19:42:50.237Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.237Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.240Z debug(journal): 1: recover_prepare: recovering slot=48
2025-11-24 19:42:50.241Z debug(journal): 1: recover_prepare: recovering slot=49
2025-11-24 19:42:50.241Z debug(journal): 1: recover_prepare: recovering slot=50
2025-11-24 19:42:50.241Z debug(journal): 1: recover_prepare: recovering slot=51
2025-11-24 19:42:50.242Z debug(journal): 1: recover_prepare: recovering slot=52
2025-11-24 19:42:50.245Z debug(journal): 1: recover_prepare: recovering slot=53
2025-11-24 19:42:50.245Z debug(journal): 1: recover_prepare: recovering slot=54
2025-11-24 19:42:50.248Z debug(journal): 1: recover_prepare: recovering slot=55
2025-11-24 19:42:50.248Z debug(journal): 1: recover_prepare: recovering slot=56
2025-11-24 19:42:50.248Z debug(journal): 1: recover_prepare: recovering slot=57
2025-11-24 19:42:50.248Z debug(journal): 1: recover_prepare: recovering slot=58
2025-11-24 19:42:50.250Z debug(journal): 1: recover_prepare: recovering slot=59
2025-11-24 19:42:50.251Z debug(journal): 1: recover_prepare: recovering slot=60
2025-11-24 19:42:50.252Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.252Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.253Z debug(journal): 1: recover_prepare: recovering slot=61
2025-11-24 19:42:50.253Z debug(journal): 1: recover_prepare: recovering slot=62
2025-11-24 19:42:50.253Z debug(journal): 1: recover_prepare: recovering slot=63
2025-11-24 19:42:50.253Z debug(journal): 1: recover_prepare: recovering slot=64
2025-11-24 19:42:50.253Z debug(journal): 1: recover_prepare: recovering slot=65
2025-11-24 19:42:50.253Z debug(journal): 1: recover_prepare: recovering slot=66
2025-11-24 19:42:50.253Z debug(journal): 1: recover_prepare: recovering slot=67
2025-11-24 19:42:50.257Z debug(journal): 1: recover_prepare: recovering slot=68
2025-11-24 19:42:50.257Z debug(journal): 1: recover_prepare: recovering slot=69
2025-11-24 19:42:50.257Z debug(journal): 1: recover_prepare: recovering slot=70
2025-11-24 19:42:50.257Z debug(journal): 1: recover_prepare: recovering slot=71
2025-11-24 19:42:50.257Z debug(vsr): 0: prepare_timeout fired
2025-11-24 19:42:50.257Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 19:42:50.257Z debug(vsr): 0: prepare_timeout after=25..3 (rtt=1 min=1 max=1000 attempts=1)
2025-11-24 19:42:50.257Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 19:42:50.257Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 19:42:50.257Z debug(replica): 0N: on_prepare_timeout: replicating to replica 2
2025-11-24 19:42:50.257Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 78803742816023192353387521627953279089, .checksum_padding = 0, .checksum_body = 300223529602328420631829499032162388468, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1018368, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138515667987549449509472186458062982598, .parent_padding = 0, .request_checksum = 310918912720831606903191227949330477117, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 185, .commit = 184, .timestamp = 1764013370011280260, .request = 183, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:50.258Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.258Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.258Z warning(message_bus): 2: on_recv: from=vsr.Peer{ .unknown = void } terminating connection: invalid header_checksum
2025-11-24 19:42:50.258Z warning(faulty_network): recv error (2,3): error.ConnectionResetByPeer
2025-11-24 19:42:50.258Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-11-24 19:42:50.260Z debug(journal): 1: recover_prepare: recovering slot=72
2025-11-24 19:42:50.262Z debug(journal): 1: recover_prepare: recovering slot=73
2025-11-24 19:42:50.262Z debug(journal): 1: recover_prepare: recovering slot=74
2025-11-24 19:42:50.266Z debug(journal): 1: recover_prepare: recovering slot=75
2025-11-24 19:42:50.268Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=70ms
2025-11-24 19:42:50.272Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.272Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:50.277Z debug(journal): 1: recover_prepare: recovering slot=76
2025-11-24 19:42:50.277Z debug(journal): 1: recover_prepare: recovering slot=77
2025-11-24 19:42:50.277Z debug(journal): 1: recover_prepare: recovering slot=78
2025-11-24 19:42:50.277Z debug(journal): 1: recover_prepare: recovering slot=79
2025-11-24 19:42:50.277Z debug(journal): 1: recover_prepare: recovering slot=80
2025-11-24 19:42:50.278Z debug(journal): 1: recover_prepare: recovering slot=81
2025-11-24 19:42:50.278Z debug(journal): 1: recover_prepare: recovering slot=82
2025-11-24 19:42:50.278Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:50.278Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:50.278Z debug(journal): 1: recover_prepare: recovering slot=83
2025-11-24 19:42:50.278Z debug(journal): 1: recover_prepare: recovering slot=84
2025-11-24 19:42:50.279Z debug(journal): 1: recover_prepare: recovering slot=85
2025-11-24 19:42:50.281Z debug(journal): 1: recover_prepare: recovering slot=86
2025-11-24 19:42:50.281Z debug(journal): 1: recover_prepare: recovering slot=87
2025-11-24 19:42:50.282Z debug(journal): 1: recover_prepare: recovering slot=88
2025-11-24 19:42:50.282Z debug(journal): 1: recover_prepare: recovering slot=89
2025-11-24 19:42:50.282Z debug(journal): 1: recover_prepare: recovering slot=90
2025-11-24 19:42:50.284Z debug(journal): 1: recover_prepare: recovering slot=91
2025-11-24 19:42:50.284Z debug(journal): 1: recover_prepare: recovering slot=92
2025-11-24 19:42:50.284Z debug(journal): 1: recover_prepare: recovering slot=93
2025-11-24 19:42:50.284Z debug(journal): 1: recover_prepare: recovering slot=94
2025-11-24 19:42:50.284Z debug(journal): 1: recover_prepare: recovering slot=95
2025-11-24 19:42:50.285Z debug(journal): 1: recover_prepare: recovering slot=96
2025-11-24 19:42:50.285Z debug(journal): 1: recover_prepare: recovering slot=97
2025-11-24 19:42:50.285Z debug(journal): 1: recover_prepare: recovering slot=98
2025-11-24 19:42:50.287Z debug(journal): 1: recover_prepare: recovering slot=99
2025-11-24 19:42:50.288Z debug(vsr): 0: prepare_timeout fired
2025-11-24 19:42:50.288Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 19:42:50.288Z debug(vsr): 0: prepare_timeout after=3..3 (rtt=1 min=1 max=1000 attempts=2)
2025-11-24 19:42:50.288Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 19:42:50.288Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 19:42:50.288Z debug(replica): 0N: on_prepare_timeout: replicating to replica 1
2025-11-24 19:42:50.291Z debug(journal): 1: recover_prepare: recovering slot=100
2025-11-24 19:42:50.291Z debug(journal): 1: recover_prepare: recovering slot=101
2025-11-24 19:42:50.291Z debug(journal): 1: recover_prepare: recovering slot=102
2025-11-24 19:42:50.291Z debug(journal): 1: recover_prepare: recovering slot=103
2025-11-24 19:42:50.292Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.292Z debug(vsr): 2: journal_repair_budget_timeout reset
thread 1 panic: reached unreachable code
2025-11-24 19:42:50.294Z debug(journal): 1: recover_prepare: recovering slot=104
2025-11-24 19:42:50.312Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:50.288Z debug(replica): 0N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 78803742816023192353387521627953279089, .checksum_padding = 0, .checksum_body = 300223529602328420631829499032162388468, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1018368, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138515667987549449509472186458062982598, .parent_padding = 0, .request_checksum = 310918912720831606903191227949330477117, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 185, .commit = 184, .timestamp = 1764013370011280260, .request = 183, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:52.672Z debug(vsr): 2: journal_repair_budget_timeout reset
/root/tigerbeetle/zig/lib/std/debug.zig:550:14: 0x124496d in assert (vortex)
    if (!ok) unreachable; // assertion failure
             ^
2025-11-24 19:42:52.672Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
/root/tigerbeetle/working/main/src/testing/vortex/faulty_network.zig:2025-11-24 19:42:52.672Z debug(replica): 0N: on_message: view=3 status=normal vsr.message_header.Header.Request{ .checksum = 303774161206467708828674460996194695368, .checksum_padding = 0, .checksum_body = 179822363220538573193979400527385066390, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 142006707948578513506132462770086189799, .parent_padding = 0, .client = 323321802946825702132615013286692535219, .session = 2, .timestamp = 0, .request = 182, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 2901485744, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
88:15: 0x13a3784 in recv (vortex)
2025-11-24 19:42:52.672Z debug(replica): 0N: on_request: replying to duplicate request
        assert(!pipe.recv_inflight);
              2025-11-24 19:42:52.672Z debug(client_replies): 0: read_reply: start (client=323321802946825702132615013286692535219 reply=152059556477938876518485625633977886279)
^
2025-11-24 19:42:52.672Z debug(journal): 1: recover_prepare: recovering slot=105
/root/tigerbeetle/working/main/src/testing/vortex/faulty_network.zig:233:22: 0x13a66b2 in on_send (vortex)
            pipe.recv();
                     ^
2025-11-24 19:42:52.673Z info(message_bus): 0: on_connect: connected to=2
/root/tigerbeetle/working/main/src/io/linux.zig:1856:25: 0x13a6209 in erased (vortex)
2025-11-24 19:42:52.673Z debug(client_replies): 0: read_reply: done (client=323321802946825702132615013286692535219 reply=152059556477938876518485625633977886279)
2025-11-24 19:42:52.673Z debug(replica): 0N: on_request: repeat reply (client=323321802946825702132615013286692535219 request=182)
2025-11-24 19:42:52.673Z debug(replica): 0N: sending reply to client 323321802946825702132615013286692535219: vsr.message_header.Header.Reply{ .checksum = 152059556477938876518485625633977886279, .checksum_padding = 0, .checksum_body = 219310283719235677857233250670414289984, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 303774161206467708828674460996194695368, .request_checksum_padding = 0, .context = 117575418631240764473290073356656454255, .context_padding = 0, .client = 323321802946825702132615013286692535219, .op = 184, .commit = 184, .timestamp = 1764013366909837307, .request = 182, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
                callback(ctx, completion, result.*);
                        ^
/root/tigerbeetle/working/main/src/io/linux.zig:738:40: 0x1306477 in complete (vortex)
                    completion.callback(completion.context, completion, &result);
                                       ^
/root/tigerbeetle/working/main/src/io/linux.zig:194:49: 0x13048e0 in flush (vortex)
2025-11-24 19:42:52.673Z debug(journal): 1: recover_prepare: recovering slot=106
                .inactive => completion.complete(),
                                                ^
/root/tigerbeetle/working/main/src/io/linux.zig:149:27: 0x1306ff6 in run_for_ns (vortex)
2025-11-24 19:42:52.673Z debug(journal): 1: recover_prepare: recovering slot=107
            try self.flush(1, &timeouts, &etime);
                          ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:263:41: 0x13079f0 in run (vortex)
            try supervisor.io.run_for_ns(constants.vsr.tick_ms * std.time.ns_per_ms);
                                        ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:207:23: 0x130c45b in main (vortex)
    try supervisor.run();
                      ^
/root/tigerbeetle/working/main/src/vortex.zig:61:61: 0x1321564 in main (vortex)
        .supervisor => |supervisor_args| try Supervisor.main(allocator, supervisor_args),
                                                            ^
/root/tigerbeetle/zig/lib/std/start.zig:660:37: 0x1322037 in main (vortex)
            const result = root.main() catch |err| {
                                    ^
/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c:95:7: 0x154e4a8 in libc_start_main_stage2 (/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c)
 exit(main(argc, argv, envp));
      ^
Unwind error at address `exe:0x154e4a8` (error.AddressOutOfRange), trace may be incomplete

2025-11-24 19:42:52.680Z debug(journal): 1: recover_prepare: recovering slot=108
2025-11-24 19:42:52.682Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:52.682Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:52.682Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 19:42:52.682Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 19:42:52.688Z debug(journal): 1: recover_prepare: recovering slot=109
2025-11-24 19:42:52.689Z debug(journal): 1: recover_prepare: recovering slot=110
2025-11-24 19:42:52.689Z debug(journal): 1: recover_prepare: recovering slot=111
2025-11-24 19:42:52.690Z debug(journal): 1: recover_prepare: recovering slot=112
2025-11-24 19:42:52.692Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:52.692Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:52.692Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 19:42:52.692Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 19:42:52.694Z debug(vsr): 0: pulse_timeout fired
2025-11-24 19:42:52.694Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:52.694Z debug(journal): 1: recover_prepare: recovering slot=113
2025-11-24 19:42:52.694Z debug(journal): 1: recover_prepare: recovering slot=114
2025-11-24 19:42:52.695Z debug(journal): 1: recover_prepare: recovering slot=115
2025-11-24 19:42:52.696Z debug(journal): 1: recover_prepare: recovering slot=116
2025-11-24 19:42:52.696Z debug(journal): 1: recover_prepare: recovering slot=117
2025-11-24 19:42:52.697Z debug(journal): 1: recover_prepare: recovering slot=118
2025-11-24 19:42:52.697Z debug(journal): 1: recover_prepare: recovering slot=119
2025-11-24 19:42:52.698Z debug(journal): 1: recover_prepare: recovering slot=120
2025-11-24 19:42:52.698Z debug(journal): 1: recover_prepare: recovering slot=121
2025-11-24 19:42:52.699Z debug(journal): 1: recover_prepare: recovering slot=122
2025-11-24 19:42:52.699Z debug(journal): 1: recover_prepare: recovering slot=123
2025-11-24 19:42:52.699Z debug(journal): 1: recover_prepare: recovering slot=124
2025-11-24 19:42:52.700Z debug(journal): 1: recover_prepare: recovering slot=125
2025-11-24 19:42:52.703Z debug(journal): 1: recover_prepare: recovering slot=126
2025-11-24 19:42:52.703Z debug(journal): 1: recover_prepare: recovering slot=127
2025-11-24 19:42:52.704Z debug(vsr): 0: prepare_timeout fired
2025-11-24 19:42:52.704Z debug(vsr): 0: prepare_timeout backing off
2025-11-24 19:42:52.704Z debug(vsr): 0: prepare_timeout after=3..11 (rtt=1 min=1 max=1000 attempts=3)
2025-11-24 19:42:52.704Z debug(replica): 0N: on_prepare_timeout: waiting for replica 1
2025-11-24 19:42:52.704Z debug(replica): 0N: on_prepare_timeout: waiting for replica 2
2025-11-24 19:42:52.704Z debug(replica): 0N: on_prepare_timeout: replicating to replica 2
2025-11-24 19:42:52.704Z debug(replica): 0N: sending prepare to replica 2: vsr.message_header.Header.Prepare{ .checksum = 78803742816023192353387521627953279089, .checksum_padding = 0, .checksum_body = 300223529602328420631829499032162388468, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1018368, .epoch = 0, .view = 3, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 138515667987549449509472186458062982598, .parent_padding = 0, .request_checksum = 310918912720831606903191227949330477117, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 323321802946825702132615013286692535219, .op = 185, .commit = 184, .timestamp = 1764013370011280260, .request = 183, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-24 19:42:52.704Z debug(journal): 1: recover_prepare: recovering slot=128
2025-11-24 19:42:52.704Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:52.704Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:52.707Z debug(journal): 1: recover_prepare: recovering slot=129
2025-11-24 19:42:52.707Z debug(journal): 1: recover_prepare: recovering slot=130
2025-11-24 19:42:52.707Z debug(journal): 1: recover_prepare: recovering slot=131
2025-11-24 19:42:52.711Z debug(journal): 1: recover_prepare: recovering slot=132
2025-11-24 19:42:52.713Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:52.713Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:52.715Z debug(journal): 1: recover_prepare: recovering slot=133
2025-11-24 19:42:52.716Z debug(journal): 1: recover_prepare: recovering slot=134
2025-11-24 19:42:52.716Z debug(journal): 1: recover_prepare: recovering slot=135
2025-11-24 19:42:52.719Z debug(journal): 1: recover_prepare: recovering slot=136
2025-11-24 19:42:52.720Z debug(journal): 1: recover_prepare: recovering slot=137
2025-11-24 19:42:52.720Z debug(journal): 1: recover_prepare: recovering slot=138
2025-11-24 19:42:52.721Z debug(journal): 1: recover_prepare: recovering slot=139
2025-11-24 19:42:52.723Z debug(journal): 1: recover_prepare: recovering slot=140
2025-11-24 19:42:52.724Z debug(journal): 1: recover_prepare: recovering slot=141
2025-11-24 19:42:52.724Z debug(journal): 1: recover_prepare: recovering slot=142
2025-11-24 19:42:52.724Z debug(journal): 1: recover_prepare: recovering slot=143
2025-11-24 19:42:52.724Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:52.725Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:52.725Z debug(journal): 1: recover_prepare: recovering slot=144
2025-11-24 19:42:52.725Z debug(journal): 1: recover_prepare: recovering slot=145
2025-11-24 19:42:52.727Z debug(journal): 1: recover_prepare: recovering slot=146
2025-11-24 19:42:52.729Z debug(journal): 1: recover_prepare: recovering slot=147
2025-11-24 19:42:52.730Z debug(journal): 1: recover_prepare: recovering slot=148
2025-11-24 19:42:52.731Z debug(journal): 1: recover_prepare: recovering slot=149
2025-11-24 19:42:52.731Z debug(journal): 1: recover_prepare: recovering slot=150
2025-11-24 19:42:52.731Z debug(journal): 1: recover_prepare: recovering slot=151
2025-11-24 19:42:52.732Z debug(journal): 1: recover_prepare: recovering slot=152
2025-11-24 19:42:52.732Z debug(journal): 1: recover_prepare: recovering slot=153
2025-11-24 19:42:52.733Z debug(journal): 1: recover_prepare: recovering slot=154
2025-11-24 19:42:52.733Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:52.733Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:52.735Z debug(journal): 1: recover_prepare: recovering slot=155
2025-11-24 19:42:52.735Z debug(journal): 1: recover_prepare: recovering slot=156
2025-11-24 19:42:52.735Z debug(journal): 1: recover_prepare: recovering slot=157
2025-11-24 19:42:52.735Z debug(journal): 1: recover_prepare: recovering slot=158
2025-11-24 19:42:52.735Z debug(journal): 1: recover_prepare: recovering slot=159
2025-11-24 19:42:52.737Z debug(journal): 1: recover_prepare: recovering slot=160
2025-11-24 19:42:52.742Z debug(journal): 1: recover_prepare: recovering slot=161
2025-11-24 19:42:52.742Z debug(journal): 1: recover_prepare: recovering slot=162
2025-11-24 19:42:52.742Z debug(journal): 1: recover_prepare: recovering slot=163
2025-11-24 19:42:52.742Z debug(journal): 1: recover_prepare: recovering slot=164
2025-11-24 19:42:52.742Z debug(journal): 1: recover_prepare: recovering slot=165
2025-11-24 19:42:52.742Z debug(journal): 1: recover_prepare: recovering slot=166
2025-11-24 19:42:52.743Z debug(journal): 1: recover_prepare: recovering slot=167
2025-11-24 19:42:52.745Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:52.745Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:52.747Z debug(journal): 1: recover_prepare: recovering slot=168
2025-11-24 19:42:52.750Z debug(journal): 1: recover_prepare: recovering slot=169
2025-11-24 19:42:52.750Z debug(journal): 1: recover_prepare: recovering slot=170
2025-11-24 19:42:52.750Z debug(journal): 1: recover_prepare: recovering slot=171
2025-11-24 19:42:52.751Z debug(journal): 1: recover_prepare: recovering slot=172
2025-11-24 19:42:52.751Z debug(journal): 1: recover_prepare: recovering slot=173
2025-11-24 19:42:52.752Z debug(journal): 1: recover_prepare: recovering slot=174
2025-11-24 19:42:52.752Z debug(journal): 1: recover_prepare: recovering slot=175
2025-11-24 19:42:52.753Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:52.753Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:52.756Z debug(journal): 1: recover_prepare: recovering slot=176
2025-11-24 19:42:52.759Z debug(journal): 1: recover_prepare: recovering slot=177
2025-11-24 19:42:52.759Z debug(journal): 1: recover_prepare: recovering slot=178
2025-11-24 19:42:52.760Z debug(journal): 1: recover_prepare: recovering slot=179
2025-11-24 19:42:52.760Z debug(journal): 1: recover_prepare: recovering slot=180
2025-11-24 19:42:52.760Z debug(journal): 1: recover_prepare: recovering slot=181
2025-11-24 19:42:52.760Z debug(journal): 1: recover_prepare: recovering slot=182
2025-11-24 19:42:52.761Z debug(journal): 1: recover_prepare: recovering slot=183
2025-11-24 19:42:52.762Z debug(journal): 1: recover_prepare: recovering slot=184
2025-11-24 19:42:52.764Z debug(journal): 1: recover_prepare: recovering slot=185
2025-11-24 19:42:52.764Z debug(journal): 1: recover_prepare: recovering slot=186
2025-11-24 19:42:52.764Z debug(journal): 1: recover_prepare: recovering slot=187
2025-11-24 19:42:52.764Z debug(journal): 1: recover_prepare: recovering slot=188
2025-11-24 19:42:52.765Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:52.765Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:52.767Z debug(journal): 1: recover_prepare: recovering slot=189
2025-11-24 19:42:52.769Z debug(journal): 1: recover_prepare: recovering slot=190
2025-11-24 19:42:52.772Z debug(journal): 1: recover_prepare: recovering slot=191
2025-11-24 19:42:52.772Z debug(journal): 1: recover_prepare: recovering slot=192
2025-11-24 19:42:52.772Z debug(journal): 1: recover_prepare: recovering slot=193
2025-11-24 19:42:52.772Z debug(journal): 1: recover_prepare: recovering slot=194
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=195
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=196
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=197
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=198
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=199
2025-11-24 19:42:52.773Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:52.773Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=200
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=201
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=202
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=203
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=204
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=205
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=206
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=207
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=208
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=209
2025-11-24 19:42:52.773Z debug(journal): 1: recover_prepare: recovering slot=210
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=211
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=212
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=213
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=214
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=215
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=216
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=217
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=218
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=219
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=220
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=221
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=222
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=223
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=224
2025-11-24 19:42:52.774Z debug(journal): 1: recover_prepare: recovering slot=225
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=226
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=227
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=228
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=229
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=230
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=231
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=232
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=233
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=234
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=235
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=236
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=237
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=238
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=239
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=240
2025-11-24 19:42:52.775Z debug(journal): 1: recover_prepare: recovering slot=241
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=242
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=243
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=244
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=245
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=246
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=247
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=248
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=249
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=250
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=251
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=252
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=253
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=254
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=255
2025-11-24 19:42:52.776Z debug(journal): 1: recover_prepare: recovering slot=256
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=257
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=258
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=259
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=260
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=261
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=262
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=263
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=264
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=265
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=266
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=267
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=268
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=269
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=270
2025-11-24 19:42:52.777Z debug(journal): 1: recover_prepare: recovering slot=271
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=272
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=273
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=274
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=275
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=276
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=277
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=278
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=279
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=280
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=281
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=282
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=283
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=284
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=285
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=286
2025-11-24 19:42:52.778Z debug(journal): 1: recover_prepare: recovering slot=287
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=288
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=289
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=290
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=291
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=292
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=293
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=294
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=295
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=296
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=297
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=298
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=299
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=300
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=301
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=302
2025-11-24 19:42:52.779Z debug(journal): 1: recover_prepare: recovering slot=303
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=304
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=305
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=306
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=307
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=308
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=309
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=310
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=311
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=312
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=313
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=314
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=315
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=316
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=317
2025-11-24 19:42:52.780Z debug(journal): 1: recover_prepare: recovering slot=318
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=319
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=320
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=321
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=322
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=323
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=324
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=325
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=326
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=327
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=328
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=329
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=330
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=331
2025-11-24 19:42:52.781Z debug(journal): 1: recover_prepare: recovering slot=332
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=333
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=334
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=335
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=336
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=337
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=338
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=339
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=340
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=341
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=342
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=343
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=344
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=345
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=346
2025-11-24 19:42:52.782Z debug(journal): 1: recover_prepare: recovering slot=347
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=348
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=349
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=350
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=351
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=352
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=353
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=354
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=355
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=356
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=357
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=358
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=359
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=360
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=361
2025-11-24 19:42:52.783Z debug(journal): 1: recover_prepare: recovering slot=362
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=363
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=364
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=365
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=366
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=367
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=368
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=369
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=370
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=371
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=372
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=373
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=374
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=375
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=376
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=377
2025-11-24 19:42:52.784Z debug(journal): 1: recover_prepare: recovering slot=378
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=379
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=380
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=381
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=382
2025-11-24 19:42:52.785Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-24 19:42:52.785Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-24 19:42:52.785Z debug(vsr): 0: journal_repair_timeout fired
2025-11-24 19:42:52.785Z debug(vsr): 0: journal_repair_timeout reset
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=383
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=384
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=385
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=386
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=387
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=388
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=389
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=390
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=391
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=392
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=393
2025-11-24 19:42:52.785Z debug(journal): 1: recover_prepare: recovering slot=394
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=395
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=396
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=397
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=398
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=399
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=400
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=401
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=402
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=403
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=404
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=405
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=406
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=407
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=408
2025-11-24 19:42:52.786Z debug(journal): 1: recover_prepare: recovering slot=409
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=410
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=411
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=412
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=413
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=414
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=415
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=416
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=417
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=418
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=419
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=420
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=421
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=422
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=423
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=424
2025-11-24 19:42:52.787Z debug(journal): 1: recover_prepare: recovering slot=425
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=426
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=427
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=428
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=429
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=430
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=431
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=432
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=433
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=434
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=435
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=436
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=437
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=438
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=439
2025-11-24 19:42:52.788Z debug(journal): 1: recover_prepare: recovering slot=440
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=441
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=442
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=443
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=444
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=445
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=446
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=447
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=448
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=449
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=450
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=451
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=452
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=453
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=454
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=455
2025-11-24 19:42:52.789Z debug(journal): 1: recover_prepare: recovering slot=456
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=457
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=458
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=459
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=460
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=461
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=462
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=463
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=464
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=465
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=466
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=467
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=468
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=469
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=470
2025-11-24 19:42:52.790Z debug(journal): 1: recover_prepare: recovering slot=471
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=472
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=473
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=474
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=475
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=476
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=477
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=478
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=479
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=480
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=481
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=482
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=483
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=484
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=485
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=486
2025-11-24 19:42:52.791Z debug(journal): 1: recover_prepare: recovering slot=487
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=488
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=489
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=490
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=491
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=492
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=493
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=494
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=495
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=496
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=497
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=498
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=499
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=500
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=501
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=502
2025-11-24 19:42:52.792Z debug(journal): 1: recover_prepare: recovering slot=503
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=504
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=505
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=506
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=507
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=508
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=509
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=510
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=511
2025-11-24 19:42:52.793Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-24 19:42:52.793Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-24 19:42:52.793Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-24 19:42:52.793Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-24 19:42:52.793Z debug(vsr): 2: journal_repair_timeout fired
2025-11-24 19:42:52.793Z debug(vsr): 2: journal_repair_timeout reset
2025-11-24 19:42:52.793Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-24 19:42:52.793Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=512
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=513
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=514
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=515
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=516
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=517
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=518
2025-11-24 19:42:52.793Z debug(journal): 1: recover_prepare: recovering slot=519
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=520
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=521
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=522
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=523
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=524
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=525
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=526
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=527
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=528
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=529
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=530
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=531
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=532
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=533
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=534
2025-11-24 19:42:52.794Z debug(journal): 1: recover_prepare: recovering slot=535
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=536
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=537
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=538
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=539
2025-11-24 19:42:52.795Z debug(vsr): 0: pulse_timeout fired
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=540
2025-11-24 19:42:52.795Z debug(vsr): 0: pulse_timeout reset
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=541
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=542
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=543
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=544
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=545
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=546
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=547
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=548
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=549
2025-11-24 19:42:52.795Z debug(journal): 1: recover_prepare: recovering slot=550
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=551
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=552
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=553
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=554
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=555
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=556
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=557
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=558
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=559
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=560
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=561
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=562
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=563
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=564
2025-11-24 19:42:52.796Z debug(journal): 1: recover_prepare: recovering slot=565
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=566
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=567
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=568
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=569
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=570
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=571
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=572
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=573
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=574
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=575
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=576
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=577
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=578
2025-11-24 19:42:52.797Z debug(journal): 1: recover_prepare: recovering slot=579
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=580
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=581
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=582
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=583
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=584
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=585
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=586
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=587
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=588
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=589
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=590
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=591
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=592
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=593
2025-11-24 19:42:52.798Z debug(journal): 1: recover_prepare: recovering slot=594
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=595
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=596
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=597
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=598
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=599
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=600
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=601
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=602
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=603
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=604
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=605
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=606
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=607
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=608
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=609
2025-11-24 19:42:52.799Z debug(journal): 1: recover_prepare: recovering slot=610
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=611
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=612
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=613
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=614
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=615
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=616
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=617
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=618
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=619
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=620
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=621
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=622
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=623
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=624
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=625
2025-11-24 19:42:52.800Z debug(journal): 1: recover_prepare: recovering slot=626
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=627
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=628
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=629
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=630
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=631
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=632
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=633
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=634
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=635
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=636
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=637
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=638
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=639
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=640
2025-11-24 19:42:52.801Z debug(journal): 1: recover_prepare: recovering slot=641
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=642
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=643
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=644
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=645
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=646
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=647
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=648
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=649
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=650
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=651
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=652
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=653
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=654
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=655
2025-11-24 19:42:52.802Z debug(journal): 1: recover_prepare: recovering slot=656
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=657
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=658
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=659
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=660
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=661
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=662
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=663
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=664
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=665
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=666
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=667
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=668
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=669
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=670
2025-11-24 19:42:52.803Z debug(journal): 1: recover_prepare: recovering slot=671
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=672
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=673
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=674
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=675
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=676
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=677
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=678
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=679
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=680
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=681
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=682
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=683
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=684
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=685
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=686
2025-11-24 19:42:52.804Z debug(journal): 1: recover_prepare: recovering slot=687
2025-11-24 19:42:52.805Z debug(journal): 1: recover_prepare: recovering slot=688
2025-11-24 19:42:52.805Z debug(journal): 1: recover_prepare: recovering slot=689
2025-11-24 19:42:53.050Z info(unshare): sandboxed subprocesses exited with signal 11
