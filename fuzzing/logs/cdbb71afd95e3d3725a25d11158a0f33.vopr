0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:16.439Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:16.439Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:16.439Z debug(replica): 2N: primary_pipeline_prepare: request checksum=183699714701056283224343690620714011265 client=25527186891061144814045155886610748587
2025-11-22 02:05:16.439Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:16.440Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=114369912280738590811815879832491838480 op=415
2025-11-22 02:05:16.440Z debug(vsr): 2: prepare_timeout started
2025-11-22 02:05:16.440Z debug(vsr): 2: primary_abdicate_timeout started
2025-11-22 02:05:16.440Z debug(vsr): 2: pulse_timeout reset
2025-11-22 02:05:16.440Z debug(replica): 2N: replicate: replicating op=415 to replica 0
2025-11-22 02:05:16.440Z debug(replica): 2N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 114369912280738590811815879832491838480, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:16.440Z debug(replica): 2N: replicate: replicating op=415 to replica 1
2025-11-22 02:05:16.440Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 114369912280738590811815879832491838480, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:16.440Z debug(replica): 2N: on_prepare: advancing: op=414..415 checksum=232114159060894084050512301565877623..114369912280738590811815879832491838480
2025-11-22 02:05:16.440Z debug(journal): 2: set_header_as_dirty: op=415 checksum=114369912280738590811815879832491838480
2025-11-22 02:05:16.440Z debug(replica): 2N: append: appending to journal op=415
2025-11-22 02:05:16.440Z debug(journal): 2: write: view=2 slot=415 op=415 len=130000: 114369912280738590811815879832491838480 starting
2025-11-22 02:05:16.440Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=435159040 len=131072 locked
2025-11-22 02:05:16.440Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 114369912280738590811815879832491838480, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:16.440Z debug(replica): 0n: on_prepare: advancing commit_max=413..414
2025-11-22 02:05:16.440Z debug(replica): 0n: on_prepare: caching prepare.op=415 (commit_min=413 op=414 commit_max=414 prepare_max=1007)
2025-11-22 02:05:16.440Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 114369912280738590811815879832491838480, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:16.440Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:16.440Z debug(replica): 1n: on_prepare: advancing commit_max=413..414
2025-11-22 02:05:16.440Z debug(replica): 1n: on_prepare: caching prepare.op=415 (commit_min=413 op=414 commit_max=414 prepare_max=1007)
2025-11-22 02:05:16.440Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:16.440Z debug(replica): 0n: on_prepare: advancing: op=414..415 checksum=232114159060894084050512301565877623..114369912280738590811815879832491838480
2025-11-22 02:05:16.440Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-22 02:05:16.440Z debug(journal): 0: set_header_as_dirty: op=415 checksum=114369912280738590811815879832491838480
2025-11-22 02:05:16.440Z debug(replica): 0n: append: appending to journal op=415
2025-11-22 02:05:16.440Z debug(journal): 0: write: view=2 slot=415 op=415 len=130000: 114369912280738590811815879832491838480 starting
2025-11-22 02:05:16.440Z debug(replica): 1n: on_prepare: advancing: op=414..415 checksum=232114159060894084050512301565877623..114369912280738590811815879832491838480
2025-11-22 02:05:16.440Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=435159040 len=131072 locked
2025-11-22 02:05:16.440Z debug(journal): 1: set_header_as_dirty: op=415 checksum=114369912280738590811815879832491838480
2025-11-22 02:05:16.953Z debug(replica): 1n: append: appending to journal op=415
2025-11-22 02:05:16.440Z debug(replica): 0n: commit_start_journal: cached prepare op=414 checksum=232114159060894084050512301565877623
2025-11-22 02:05:16.953Z debug(journal): 1: write: view=2 slot=415 op=415 len=130000: 114369912280738590811815879832491838480 starting
2025-11-22 02:05:16.440Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=435159040 len=131072 unlocked
2025-11-22 02:05:16.953Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=435159040 len=131072 locked
2025-11-22 02:05:16.953Z debug(journal): 2: write_header: op=415 sectors[102400..106496]
2025-11-22 02:05:16.953Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=102400 len=4096 locked
2025-11-22 02:05:16.953Z debug(replica): 1n: commit_start_journal: cached prepare op=414 checksum=232114159060894084050512301565877623
2025-11-22 02:05:16.953Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:16.953Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:16.953Z debug(replica): 0n: repair_prepare: op=415 checksum=114369912280738590811815879832491838480 (already writing)
2025-11-22 02:05:16.953Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=102400 len=4096 unlocked
2025-11-22 02:05:16.953Z debug(journal): 2: write: view=2 slot=415 op=415 len=130000: 114369912280738590811815879832491838480 complete, marking clean
2025-11-22 02:05:16.953Z debug(replica): 2N: send_prepare_ok: op=415 checksum=114369912280738590811815879832491838480
2025-11-22 02:05:16.953Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=413)
2025-11-22 02:05:16.953Z debug(replica): 2N: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 336091302252188716552243162415875336035, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .prepare_checksum = 114369912280738590811815879832491838480, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit_min = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:16.953Z debug(replica): 0n: execute_op: executing view=2 primary=false op=414 checksum=232114159060894084050512301565877623 (lookup_accounts)
2025-11-22 02:05:16.953Z debug(replica): 0n: execute_op: commit_timestamp=1763777115802803247 prepare.header.timestamp=1763777116375337143
2025-11-22 02:05:16.953Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 336091302252188716552243162415875336035, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .prepare_checksum = 114369912280738590811815879832491838480, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit_min = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:16.953Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-22 02:05:16.954Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2025-11-22 02:05:16.954Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2025-11-22 02:05:16.954Z debug(replica): 1n: repair_prepare: op=415 checksum=114369912280738590811815879832491838480 (already writing)
2025-11-22 02:05:16.954Z debug(replica): 0n: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=412
2025-11-22 02:05:16.954Z debug(forest): entering forest.compact() op=414 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-22 02:05:16.954Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=413)
2025-11-22 02:05:16.954Z debug(replica): 1n: execute_op: executing view=2 primary=false op=414 checksum=232114159060894084050512301565877623 (lookup_accounts)
2025-11-22 02:05:16.954Z debug(replica): 1n: execute_op: commit_timestamp=1763777115802803247 prepare.header.timestamp=1763777116375337143
2025-11-22 02:05:16.954Z debug(replica): 1n: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=412
2025-11-22 02:05:16.954Z debug(replica): 1n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 236502294738470651249369735233853813415, .checksum_padding = 0, .checksum_body = 327964006529606677042488414844401738121, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 273069631816160316705011479693545754041, .request_checksum_padding = 0, .context = 19920708824830470844574968299842190240, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 414, .commit = 414, .timestamp = 1763777116375337143, .request = 412, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:16.954Z debug(client_replies): 0: write_reply: wrote (client=25527186891061144814045155886610748587 request=412)
2025-11-22 02:05:16.954Z debug(replica): 1n: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 236502294738470651249369735233853813415, .checksum_padding = 0, .checksum_body = 327964006529606677042488414844401738121, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 273069631816160316705011479693545754041, .request_checksum_padding = 0, .context = 19920708824830470844574968299842190240, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 414, .commit = 414, .timestamp = 1763777116375337143, .request = 412, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:16.954Z debug(forest): entering forest.compact() op=414 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-22 02:05:16.954Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=435159040 len=131072 unlocked
2025-11-22 02:05:16.955Z debug(journal): 0: write_header: op=415 sectors[102400..106496]
2025-11-22 02:05:16.955Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=102400 len=4096 locked
2025-11-22 02:05:16.955Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=102400 len=4096 unlocked
2025-11-22 02:05:16.955Z debug(journal): 0: write: view=2 slot=415 op=415 len=130000: 114369912280738590811815879832491838480 complete, marking clean
2025-11-22 02:05:16.955Z debug(replica): 0n: send_prepare_ok: op=415 checksum=114369912280738590811815879832491838480
2025-11-22 02:05:16.955Z debug(replica): 0n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 336818220028752539358981074332117441671, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .prepare_checksum = 114369912280738590811815879832491838480, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit_min = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:16.955Z debug(client_replies): 1: write_reply: wrote (client=25527186891061144814045155886610748587 request=412)
2025-11-22 02:05:16.955Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=435159040 len=131072 unlocked
2025-11-22 02:05:16.955Z debug(journal): 1: write_header: op=415 sectors[102400..106496]
2025-11-22 02:05:16.955Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=102400 len=4096 locked
2025-11-22 02:05:16.955Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=102400 len=4096 unlocked
2025-11-22 02:05:16.955Z debug(journal): 1: write: view=2 slot=415 op=415 len=130000: 114369912280738590811815879832491838480 complete, marking clean
2025-11-22 02:05:16.955Z debug(replica): 1n: send_prepare_ok: op=415 checksum=114369912280738590811815879832491838480
2025-11-22 02:05:16.955Z debug(replica): 1n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 155996280131244711023862698482154871769, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .prepare_checksum = 114369912280738590811815879832491838480, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit_min = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:16.964Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:16.964Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:16.965Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:16.965Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:16.966Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 155996280131244711023862698482154871769, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .prepare_checksum = 114369912280738590811815879832491838480, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit_min = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:16.966Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-22 02:05:16.966Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2025-11-22 02:05:16.966Z debug(replica): 2N: on_prepare_ok: quorum received, context=114369912280738590811815879832491838480
2025-11-22 02:05:16.966Z debug(vsr): 2: prepare_timeout stopped
2025-11-22 02:05:16.966Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-11-22 02:05:16.969Z debug(replica): 2N: execute_op: executing view=2 primary=true op=415 checksum=114369912280738590811815879832491838480 (lookup_transfers)
2025-11-22 02:05:16.969Z debug(replica): 2N: execute_op: commit_timestamp=1763777116375337143 prepare.header.timestamp=1763777116439692220
2025-11-22 02:05:16.972Z debug(replica): 2N: execute_op: advancing commit_max=414..415
2025-11-22 02:05:16.974Z debug(replica): 2N: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=413
2025-11-22 02:05:16.974Z debug(replica): 2N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 334566707531139553687677398822655128191, .checksum_padding = 0, .checksum_body = 291604305023285183924195684615135308237, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1038208, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .context = 250462913007803900732714730937641667142, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 415, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:16.974Z debug(replica): 2N: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 334566707531139553687677398822655128191, .checksum_padding = 0, .checksum_body = 291604305023285183924195684615135308237, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1038208, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .context = 250462913007803900732714730937641667142, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 415, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:16.975Z debug(forest): entering forest.compact() op=415 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=true
2025-11-22 02:05:16.975Z debug(manifest_log): 2: flush: writing 0 block(s)
2025-11-22 02:05:16.975Z debug(forest): swap_mutable_and_immutable(Account.id)
2025-11-22 02:05:16.975Z debug(forest): swap_mutable_and_immutable(Account.user_data_128)
2025-11-22 02:05:16.975Z debug(forest): swap_mutable_and_immutable(Account.user_data_64)
2025-11-22 02:05:16.975Z debug(forest): swap_mutable_and_immutable(Account.user_data_32)
2025-11-22 02:05:16.975Z debug(forest): swap_mutable_and_immutable(Account.ledger)
2025-11-22 02:05:16.975Z debug(forest): swap_mutable_and_immutable(Account.code)
2025-11-22 02:05:16.975Z debug(forest): swap_mutable_and_immutable(Account)
2025-11-22 02:05:16.976Z debug(forest): swap_mutable_and_immutable(Transfer.id)
2025-11-22 02:05:16.984Z debug(vsr): 0: start_view_change_message_timeout fired
2025-11-22 02:05:16.984Z debug(vsr): 0: start_view_change_message_timeout reset
2025-11-22 02:05:16.984Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:16.984Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:16.984Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:16.984Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:16.984Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-11-22 02:05:16.984Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-11-22 02:05:16.989Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:16.989Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.000Z debug(forest): swap_mutable_and_immutable(Transfer.debit_account_id)
2025-11-22 02:05:17.005Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.005Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.010Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.010Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.025Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.025Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.030Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.030Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.037Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.037Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:17.037Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.039Z debug(forest): swap_mutable_and_immutable(Transfer.credit_account_id)
2025-11-22 02:05:17.045Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.045Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.050Z debug(vsr): 1: start_view_change_message_timeout fired
2025-11-22 02:05:17.050Z debug(vsr): 1: start_view_change_message_timeout reset
2025-11-22 02:05:17.050Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.050Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.050Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:17.050Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:17.050Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-11-22 02:05:17.050Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-11-22 02:05:17.065Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.065Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.070Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.070Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.082Z debug(forest): swap_mutable_and_immutable(Transfer.amount)
2025-11-22 02:05:17.085Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.085Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.085Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:17.085Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:17.090Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.090Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.106Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.106Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.110Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.110Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.123Z debug(forest): swap_mutable_and_immutable(Transfer.pending_id)
2025-11-22 02:05:17.123Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_128)
2025-11-22 02:05:17.123Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_64)
2025-11-22 02:05:17.123Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_32)
2025-11-22 02:05:17.123Z debug(forest): swap_mutable_and_immutable(Transfer.ledger)
2025-11-22 02:05:17.126Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.126Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.130Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.130Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.146Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.146Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.148Z debug(forest): swap_mutable_and_immutable(Transfer.code)
2025-11-22 02:05:17.150Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.150Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.150Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:17.150Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:17.166Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.166Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.170Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.170Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.186Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.186Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.186Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:17.186Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:17.190Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.190Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.192Z debug(forest): swap_mutable_and_immutable(Transfer)
2025-11-22 02:05:17.206Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.206Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.210Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.210Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.226Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.226Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.230Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.230Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.233Z debug(forest): swap_mutable_and_immutable(Transfer.expires_at)
2025-11-22 02:05:17.233Z debug(forest): swap_mutable_and_immutable(TransferPending)
2025-11-22 02:05:17.233Z debug(forest): swap_mutable_and_immutable(TransferPending.status)
2025-11-22 02:05:17.233Z debug(forest): swap_mutable_and_immutable(AccountEvent)
2025-11-22 02:05:17.246Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.246Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.250Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.250Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.250Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:17.250Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:17.266Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.266Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.271Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.271Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.286Z debug(forest): swap_mutable_and_immutable(Account.imported)
2025-11-22 02:05:17.286Z debug(forest): swap_mutable_and_immutable(Transfer.imported)
2025-11-22 02:05:17.286Z debug(forest): swap_mutable_and_immutable(Account.closed)
2025-11-22 02:05:17.286Z debug(forest): swap_mutable_and_immutable(Transfer.closing)
2025-11-22 02:05:17.286Z debug(forest): swap_mutable_and_immutable(AccountEvent.account_timestamp)
2025-11-22 02:05:17.286Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.286Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.286Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:17.286Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:17.287Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.287Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:17.287Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.291Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.291Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.306Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.306Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.311Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.311Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.326Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.327Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.331Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_status)
2025-11-22 02:05:17.332Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.332Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.347Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.347Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.352Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.352Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.352Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:17.352Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:17.355Z debug(forest): swap_mutable_and_immutable(AccountEvent.dr_account_id_expired)
2025-11-22 02:05:17.355Z debug(forest): swap_mutable_and_immutable(AccountEvent.cr_account_id_expired)
2025-11-22 02:05:17.355Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_id_expired)
2025-11-22 02:05:17.355Z debug(forest): swap_mutable_and_immutable(AccountEvent.ledger_expired)
2025-11-22 02:05:17.355Z debug(forest): swap_mutable_and_immutable(AccountEvent.prunable)
2025-11-22 02:05:17.367Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.367Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.368Z warning(replica): 2N: commit_dispatch: slow request, request=413 size=130000 lookup_transfers time=401ms
2025-11-22 02:05:17.372Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:17.372Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:17.375Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 336818220028752539358981074332117441671, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 232114159060894084050512301565877623, .parent_padding = 0, .prepare_checksum = 114369912280738590811815879832491838480, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit_min = 414, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:17.375Z debug(replica): 2N: on_prepare_ok: not preparing op=415 checksum=114369912280738590811815879832491838480
2025-11-22 02:05:17.376Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.376Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-22 02:05:17.376Z debug(replica): 2N: on_request: repeat reply (client=25527186891061144814045155886610748587 request=413)
2025-11-22 02:05:17.376Z debug(replica): 2N: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 334566707531139553687677398822655128191, .checksum_padding = 0, .checksum_body = 291604305023285183924195684615135308237, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1038208, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .context = 250462913007803900732714730937641667142, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 415, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.376Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.376Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-22 02:05:17.376Z debug(replica): 2N: on_request: repeat reply (client=25527186891061144814045155886610748587 request=413)
2025-11-22 02:05:17.376Z debug(replica): 2N: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 334566707531139553687677398822655128191, .checksum_padding = 0, .checksum_body = 291604305023285183924195684615135308237, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1038208, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .context = 250462913007803900732714730937641667142, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 415, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.377Z debug(client_replies): 2: write_reply: wrote (client=25527186891061144814045155886610748587 request=413)
2025-11-22 02:05:17.377Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.378Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-22 02:05:17.378Z debug(client_replies): 2: read_reply: start (client=25527186891061144814045155886610748587 reply=334566707531139553687677398822655128191)
2025-11-22 02:05:17.378Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 183699714701056283224343690620714011265, .checksum_padding = 0, .checksum_body = 280719006739562848193913930238824283815, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 19920708824830470844574968299842190240, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 413, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 62810589, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.378Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-22 02:05:17.378Z debug(client_replies): 2: read_reply: busy (client=25527186891061144814045155886610748587 reply=334566707531139553687677398822655128191)
2025-11-22 02:05:17.378Z debug(replica): 2N: on_request: ignoring (client_replies busy)
2025-11-22 02:05:17.379Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.379Z debug(vsr): 2: journal_repair_budget_timeout reset
warning(client): 25527186891061144814045155886610748587: on_reply: slow request, request=413 op=415 size=130000 lookup_transfers time=943ms
2025-11-22 02:05:17.383Z debug(client_replies): 2: read_reply: done (client=25527186891061144814045155886610748587 reply=334566707531139553687677398822655128191)
2025-11-22 02:05:17.383Z debug(replica): 2N: on_request: repeat reply (client=25527186891061144814045155886610748587 request=413)
2025-11-22 02:05:17.383Z debug(replica): 2N: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 334566707531139553687677398822655128191, .checksum_padding = 0, .checksum_body = 291604305023285183924195684615135308237, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1038208, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .context = 250462913007803900732714730937641667142, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 415, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.387Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:17.387Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:17.387Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:17.387Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:17.390Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 87834938227577662541192884506571294145, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 250462913007803900732714730937641667142, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 414, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 943939682, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.390Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 87834938227577662541192884506571294145, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 250462913007803900732714730937641667142, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 414, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 943939682, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.390Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:17.390Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:17.390Z debug(replica): 2N: primary_pipeline_prepare: request checksum=87834938227577662541192884506571294145 client=25527186891061144814045155886610748587
2025-11-22 02:05:17.390Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 87834938227577662541192884506571294145, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 250462913007803900732714730937641667142, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 414, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 943939682, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.391Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=33931453055112777868827718360993313803 op=416
2025-11-22 02:05:17.391Z debug(vsr): 2: prepare_timeout started
2025-11-22 02:05:17.391Z debug(vsr): 2: primary_abdicate_timeout started
2025-11-22 02:05:17.391Z debug(vsr): 2: pulse_timeout reset
2025-11-22 02:05:17.391Z debug(replica): 2N: replicate: replicating op=416 to replica 0
2025-11-22 02:05:17.391Z debug(replica): 2N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 33931453055112777868827718360993313803, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:17.391Z debug(replica): 2N: replicate: replicating op=416 to replica 1
2025-11-22 02:05:17.391Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 33931453055112777868827718360993313803, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:17.391Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 33931453055112777868827718360993313803, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:17.391Z debug(replica): 2N: on_prepare: advancing: op=415..416 checksum=114369912280738590811815879832491838480..33931453055112777868827718360993313803
2025-11-22 02:05:17.391Z debug(replica): 0n: on_prepare: advancing commit_max=414..415
2025-11-22 02:05:17.391Z debug(journal): 2: set_header_as_dirty: op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:17.391Z debug(replica): 2N: append: appending to journal op=416
2025-11-22 02:05:17.391Z debug(replica): 0n: on_prepare: caching prepare.op=416 (commit_min=414 op=415 commit_max=415 prepare_max=1007)
2025-11-22 02:05:17.391Z debug(journal): 2: write: view=2 slot=416 op=416 len=2320: 33931453055112777868827718360993313803 starting
2025-11-22 02:05:17.391Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=436207616 len=4096 locked
2025-11-22 02:05:17.391Z debug(replica): 0n: on_prepare: advancing: op=415..416 checksum=114369912280738590811815879832491838480..33931453055112777868827718360993313803
2025-11-22 02:05:17.391Z debug(journal): 0: set_header_as_dirty: op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:17.391Z debug(replica): 0n: append: appending to journal op=416
2025-11-22 02:05:17.391Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 33931453055112777868827718360993313803, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:17.391Z debug(journal): 0: write: view=2 slot=416 op=416 len=2320: 33931453055112777868827718360993313803 starting
2025-11-22 02:05:17.391Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=436207616 len=4096 locked
2025-11-22 02:05:17.391Z debug(replica): 1n: on_prepare: advancing commit_max=414..415
2025-11-22 02:05:17.391Z debug(replica): 1n: on_prepare: caching prepare.op=416 (commit_min=414 op=415 commit_max=415 prepare_max=1007)
2025-11-22 02:05:17.391Z debug(replica): 0n: commit_start_journal: cached prepare op=415 checksum=114369912280738590811815879832491838480
2025-11-22 02:05:17.391Z debug(replica): 1n: on_prepare: advancing: op=415..416 checksum=114369912280738590811815879832491838480..33931453055112777868827718360993313803
2025-11-22 02:05:17.391Z debug(journal): 1: set_header_as_dirty: op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:17.391Z debug(replica): 1n: append: appending to journal op=416
2025-11-22 02:05:17.391Z debug(journal): 1: write: view=2 slot=416 op=416 len=2320: 33931453055112777868827718360993313803 starting
2025-11-22 02:05:17.391Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=436207616 len=4096 locked
2025-11-22 02:05:17.391Z debug(replica): 1n: commit_start_journal: cached prepare op=415 checksum=114369912280738590811815879832491838480
2025-11-22 02:05:17.391Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 87834938227577662541192884506571294145, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 250462913007803900732714730937641667142, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 414, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 943939682, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.391Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:17.391Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-22 02:05:17.391Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=436207616 len=4096 unlocked
2025-11-22 02:05:17.391Z debug(journal): 2: write_header: op=416 sectors[106496..110592]
2025-11-22 02:05:17.391Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 locked
2025-11-22 02:05:17.391Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 unlocked
2025-11-22 02:05:17.391Z debug(journal): 2: write: view=2 slot=416 op=416 len=2320: 33931453055112777868827718360993313803 complete, marking clean
2025-11-22 02:05:17.391Z debug(replica): 2N: send_prepare_ok: op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:17.391Z debug(replica): 2N: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 35131670237023305178153393338118272991, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .prepare_checksum = 33931453055112777868827718360993313803, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit_min = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:17.391Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 35131670237023305178153393338118272991, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .prepare_checksum = 33931453055112777868827718360993313803, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit_min = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:17.391Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-22 02:05:17.391Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2025-11-22 02:05:17.391Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2025-11-22 02:05:17.396Z debug(replica): 1n: repair_prepare: op=416 checksum=33931453055112777868827718360993313803 (already writing)
2025-11-22 02:05:17.396Z debug(replica): 0n: repair_prepare: op=416 checksum=33931453055112777868827718360993313803 (already writing)
2025-11-22 02:05:17.396Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=414)
2025-11-22 02:05:17.396Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=414)
2025-11-22 02:05:17.396Z debug(replica): 1n: execute_op: executing view=2 primary=false op=415 checksum=114369912280738590811815879832491838480 (lookup_transfers)
2025-11-22 02:05:17.396Z debug(replica): 0n: execute_op: executing view=2 primary=false op=415 checksum=114369912280738590811815879832491838480 (lookup_transfers)
2025-11-22 02:05:17.396Z debug(replica): 0n: execute_op: commit_timestamp=1763777116375337143 prepare.header.timestamp=1763777116439692220
2025-11-22 02:05:17.396Z debug(replica): 1n: execute_op: commit_timestamp=1763777116375337143 prepare.header.timestamp=1763777116439692220
2025-11-22 02:05:17.399Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.399Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.403Z debug(replica): 1n: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=413
2025-11-22 02:05:17.403Z debug(forest): entering forest.compact() op=415 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=true
2025-11-22 02:05:17.403Z debug(manifest_log): 1: flush: writing 0 block(s)
2025-11-22 02:05:17.404Z debug(replica): 0n: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=413
2025-11-22 02:05:17.404Z debug(forest): swap_mutable_and_immutable(Account.id)
2025-11-22 02:05:17.404Z debug(forest): swap_mutable_and_immutable(Account.user_data_128)
2025-11-22 02:05:17.404Z debug(forest): swap_mutable_and_immutable(Account.user_data_64)
2025-11-22 02:05:17.404Z debug(forest): swap_mutable_and_immutable(Account.user_data_32)
2025-11-22 02:05:17.404Z debug(replica): 0n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 334566707531139553687677398822655128191, .checksum_padding = 0, .checksum_body = 291604305023285183924195684615135308237, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1038208, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .context = 250462913007803900732714730937641667142, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 415, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.404Z debug(forest): swap_mutable_and_immutable(Account.ledger)
2025-11-22 02:05:17.419Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.757Z debug(forest): swap_mutable_and_immutable(Account.code)
2025-11-22 02:05:17.757Z debug(forest): swap_mutable_and_immutable(Account)
2025-11-22 02:05:17.757Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.757Z debug(replica): 0n: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 334566707531139553687677398822655128191, .checksum_padding = 0, .checksum_body = 291604305023285183924195684615135308237, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 1038208, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 183699714701056283224343690620714011265, .request_checksum_padding = 0, .context = 250462913007803900732714730937641667142, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 415, .commit = 415, .timestamp = 1763777116439692220, .request = 413, .operation = vsr.Operation(141), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.757Z debug(forest): entering forest.compact() op=415 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=true
2025-11-22 02:05:17.757Z debug(manifest_log): 0: flush: writing 0 block(s)
2025-11-22 02:05:17.757Z debug(forest): swap_mutable_and_immutable(Transfer.id)
2025-11-22 02:05:17.758Z debug(forest): swap_mutable_and_immutable(Account.id)
2025-11-22 02:05:17.758Z debug(forest): swap_mutable_and_immutable(Account.user_data_128)
2025-11-22 02:05:17.758Z debug(forest): swap_mutable_and_immutable(Account.user_data_64)
2025-11-22 02:05:17.758Z debug(forest): swap_mutable_and_immutable(Account.user_data_32)
2025-11-22 02:05:17.758Z debug(forest): swap_mutable_and_immutable(Account.ledger)
2025-11-22 02:05:17.758Z debug(forest): swap_mutable_and_immutable(Account.code)
2025-11-22 02:05:17.758Z debug(forest): swap_mutable_and_immutable(Account)
2025-11-22 02:05:17.760Z debug(forest): swap_mutable_and_immutable(Transfer.id)
2025-11-22 02:05:17.767Z debug(vsr): 2: journal_repair_timeout fired
2025-11-22 02:05:17.767Z debug(vsr): 2: journal_repair_timeout reset
2025-11-22 02:05:17.777Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.777Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.783Z debug(forest): swap_mutable_and_immutable(Transfer.debit_account_id)
2025-11-22 02:05:17.797Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.797Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.817Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.817Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.821Z debug(forest): swap_mutable_and_immutable(Transfer.credit_account_id)
2025-11-22 02:05:17.825Z debug(forest): swap_mutable_and_immutable(Transfer.debit_account_id)
2025-11-22 02:05:17.827Z debug(vsr): 2: pulse_timeout fired
2025-11-22 02:05:17.827Z debug(vsr): 2: pulse_timeout reset
2025-11-22 02:05:17.837Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.837Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.857Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.857Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.864Z debug(forest): swap_mutable_and_immutable(Transfer.amount)
2025-11-22 02:05:17.867Z debug(vsr): 2: journal_repair_timeout fired
2025-11-22 02:05:17.867Z debug(vsr): 2: journal_repair_timeout reset
2025-11-22 02:05:17.877Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.877Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.881Z debug(forest): swap_mutable_and_immutable(Transfer.credit_account_id)
2025-11-22 02:05:17.897Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.897Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.904Z debug(forest): swap_mutable_and_immutable(Transfer.pending_id)
2025-11-22 02:05:17.904Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_128)
2025-11-22 02:05:17.904Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_64)
2025-11-22 02:05:17.904Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_32)
2025-11-22 02:05:17.904Z debug(forest): swap_mutable_and_immutable(Transfer.ledger)
2025-11-22 02:05:17.918Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.918Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.924Z debug(forest): swap_mutable_and_immutable(Transfer.amount)
2025-11-22 02:05:17.928Z debug(vsr): 2: pulse_timeout fired
2025-11-22 02:05:17.928Z debug(vsr): 2: pulse_timeout reset
2025-11-22 02:05:17.928Z debug(forest): swap_mutable_and_immutable(Transfer.code)
2025-11-22 02:05:17.938Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.938Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.958Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.958Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.964Z debug(forest): swap_mutable_and_immutable(Transfer.pending_id)
2025-11-22 02:05:17.964Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_128)
2025-11-22 02:05:17.964Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_64)
2025-11-22 02:05:17.964Z debug(forest): swap_mutable_and_immutable(Transfer.user_data_32)
2025-11-22 02:05:17.964Z debug(forest): swap_mutable_and_immutable(Transfer.ledger)
2025-11-22 02:05:17.968Z debug(vsr): 2: ping_timeout fired
2025-11-22 02:05:17.968Z debug(vsr): 2: ping_timeout reset
2025-11-22 02:05:17.968Z debug(replica): 2N: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 214468736910393699607752700052030416603, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261008614158977, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692840448, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.968Z debug(replica): 2N: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 214468736910393699607752700052030416603, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261008614158977, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692840448, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.968Z debug(vsr): 2: commit_message_timeout fired
2025-11-22 02:05:17.968Z debug(vsr): 2: commit_message_timeout reset
2025-11-22 02:05:17.968Z debug(replica): 2N: sending commit to replica 0: vsr.message_header.Header.Commit{ .checksum = 113974119832431332344302574162425780824, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 114369912280738590811815879832491838480, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 415, .timestamp_monotonic = 15261008614325178, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.968Z debug(replica): 2N: sending commit to replica 1: vsr.message_header.Header.Commit{ .checksum = 113974119832431332344302574162425780824, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 114369912280738590811815879832491838480, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 415, .timestamp_monotonic = 15261008614325178, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.968Z debug(vsr): 2: start_view_change_message_timeout fired
2025-11-22 02:05:17.968Z debug(vsr): 2: start_view_change_message_timeout reset
2025-11-22 02:05:17.968Z debug(vsr): 2: journal_repair_timeout fired
2025-11-22 02:05:17.968Z debug(vsr): 2: journal_repair_timeout reset
2025-11-22 02:05:17.968Z debug(vsr): 2: grid_repair_budget_timeout fired
2025-11-22 02:05:17.968Z debug(vsr): 2: grid_repair_budget_timeout reset
2025-11-22 02:05:17.973Z debug(forest): swap_mutable_and_immutable(Transfer)
2025-11-22 02:05:17.978Z debug(vsr): 2: prepare_timeout fired
2025-11-22 02:05:17.978Z debug(vsr): 2: prepare_timeout backing off
2025-11-22 02:05:17.978Z debug(vsr): 2: prepare_timeout after=25..193 (rtt=95 min=1 max=1000 attempts=1)
2025-11-22 02:05:17.978Z debug(replica): 2N: on_prepare_timeout: waiting for replica 0
2025-11-22 02:05:17.978Z debug(replica): 2N: on_prepare_timeout: waiting for replica 1
2025-11-22 02:05:17.978Z debug(replica): 2N: on_prepare_timeout: replicating to replica 1
2025-11-22 02:05:17.978Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 33931453055112777868827718360993313803, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:17.979Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.979Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:17.989Z debug(forest): swap_mutable_and_immutable(Transfer.code)
2025-11-22 02:05:17.994Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 87834938227577662541192884506571294145, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 250462913007803900732714730937641667142, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 414, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 943939682, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:17.994Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:17.994Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-22 02:05:17.999Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:17.999Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.014Z debug(forest): swap_mutable_and_immutable(Transfer.expires_at)
2025-11-22 02:05:18.014Z debug(forest): swap_mutable_and_immutable(TransferPending)
2025-11-22 02:05:18.014Z debug(forest): swap_mutable_and_immutable(TransferPending.status)
2025-11-22 02:05:18.014Z debug(forest): swap_mutable_and_immutable(AccountEvent)
2025-11-22 02:05:18.019Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.019Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.029Z debug(vsr): 2: pulse_timeout fired
2025-11-22 02:05:18.029Z debug(vsr): 2: pulse_timeout reset
2025-11-22 02:05:18.033Z debug(forest): swap_mutable_and_immutable(Transfer)
2025-11-22 02:05:18.039Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.039Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.059Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.059Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.066Z debug(forest): swap_mutable_and_immutable(Account.imported)
2025-11-22 02:05:18.066Z debug(forest): swap_mutable_and_immutable(Transfer.imported)
2025-11-22 02:05:18.066Z debug(forest): swap_mutable_and_immutable(Account.closed)
2025-11-22 02:05:18.066Z debug(forest): swap_mutable_and_immutable(Transfer.closing)
2025-11-22 02:05:18.066Z debug(forest): swap_mutable_and_immutable(AccountEvent.account_timestamp)
2025-11-22 02:05:18.069Z debug(vsr): 2: journal_repair_timeout fired
2025-11-22 02:05:18.069Z debug(vsr): 2: journal_repair_timeout reset
2025-11-22 02:05:18.078Z debug(forest): swap_mutable_and_immutable(Transfer.expires_at)
2025-11-22 02:05:18.078Z debug(forest): swap_mutable_and_immutable(TransferPending)
2025-11-22 02:05:18.078Z debug(forest): swap_mutable_and_immutable(TransferPending.status)
2025-11-22 02:05:18.078Z debug(forest): swap_mutable_and_immutable(AccountEvent)
2025-11-22 02:05:18.079Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.079Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.099Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.099Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.113Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_status)
2025-11-22 02:05:18.119Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.119Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.129Z debug(vsr): 2: pulse_timeout fired
2025-11-22 02:05:18.129Z debug(vsr): 2: pulse_timeout reset
2025-11-22 02:05:18.138Z debug(forest): swap_mutable_and_immutable(Account.imported)
2025-11-22 02:05:18.138Z debug(forest): swap_mutable_and_immutable(Transfer.imported)
2025-11-22 02:05:18.138Z debug(forest): swap_mutable_and_immutable(Account.closed)
2025-11-22 02:05:18.138Z debug(forest): swap_mutable_and_immutable(Transfer.closing)
2025-11-22 02:05:18.138Z debug(forest): swap_mutable_and_immutable(AccountEvent.account_timestamp)
2025-11-22 02:05:18.139Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.139Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.139Z debug(forest): swap_mutable_and_immutable(AccountEvent.dr_account_id_expired)
2025-11-22 02:05:18.139Z debug(forest): swap_mutable_and_immutable(AccountEvent.cr_account_id_expired)
2025-11-22 02:05:18.139Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_id_expired)
2025-11-22 02:05:18.139Z debug(forest): swap_mutable_and_immutable(AccountEvent.ledger_expired)
2025-11-22 02:05:18.139Z debug(forest): swap_mutable_and_immutable(AccountEvent.prunable)
2025-11-22 02:05:18.151Z warning(replica): 1n: commit_dispatch: slow request, request=413 size=130000 lookup_transfers time=760ms
2025-11-22 02:05:18.159Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.159Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.160Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 87834938227577662541192884506571294145, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 250462913007803900732714730937641667142, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 414, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 943939682, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.160Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:18.160Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 87834938227577662541192884506571294145, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 250462913007803900732714730937641667142, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 414, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 943939682, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.160Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Ping{ .checksum = 214468736910393699607752700052030416603, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261008614158977, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692840448, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.160Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 87834938227577662541192884506571294145, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 250462913007803900732714730937641667142, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 414, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 943939682, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.160Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:18.160Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-22 02:05:18.160Z debug(replica): 1n: sending pong to replica 2: vsr.message_header.Header.Pong{ .checksum = 250751425480468554659516894290767044245, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 15261008614158977, .pong_timestamp_wall = 1763777118160414202, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.160Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Pong{ .checksum = 250751425480468554659516894290767044245, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 15261008614158977, .pong_timestamp_wall = 1763777118160414202, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.160Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Commit{ .checksum = 113974119832431332344302574162425780824, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 114369912280738590811815879832491838480, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 415, .timestamp_monotonic = 15261008614325178, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.160Z debug(clock): 2: learn: replica=1 m0=15261008614158977 t1=1763777118160414202 m2=15261008806288042 t2=1763777118160587923 one_way_delay=96064532 asymmetric_delay=0 clock_offset=95890811
2025-11-22 02:05:18.160Z debug(vsr): 1: normal_heartbeat_timeout reset
2025-11-22 02:05:18.160Z debug(replica): 1n: on_commit: checksum verified
2025-11-22 02:05:18.160Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 33931453055112777868827718360993313803, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.160Z debug(replica): 1n: on_prepare: ignoring (repair)
2025-11-22 02:05:18.160Z debug(replica): 1n: repair_header: op=416 checksum=33931453055112777868827718360993313803 (checksum dirty)
2025-11-22 02:05:18.160Z debug(journal): 1: set_header_as_dirty: op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:18.160Z debug(replica): 1n: write_prepare: ignoring op=416 checksum=33931453055112777868827718360993313803 (already writing exact)
2025-11-22 02:05:18.160Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=436207616 len=4096 unlocked
2025-11-22 02:05:18.160Z debug(journal): 1: write_header: op=416 sectors[106496..110592]
2025-11-22 02:05:18.160Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 locked
2025-11-22 02:05:18.160Z debug(client_replies): 1: write_reply: wrote (client=25527186891061144814045155886610748587 request=413)
2025-11-22 02:05:18.160Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:18.160Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:18.160Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 unlocked
2025-11-22 02:05:18.160Z debug(journal): 1: write: view=2 slot=416 op=416 len=2320: 33931453055112777868827718360993313803 complete, marking clean
2025-11-22 02:05:18.160Z debug(replica): 1n: send_prepare_ok: op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:18.160Z debug(replica): 1n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 326268230721228051210782698972987827691, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .prepare_checksum = 33931453055112777868827718360993313803, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit_min = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.160Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 326268230721228051210782698972987827691, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .prepare_checksum = 33931453055112777868827718360993313803, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit_min = 415, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.161Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-22 02:05:18.161Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2025-11-22 02:05:18.161Z debug(replica): 2N: on_prepare_ok: quorum received, context=33931453055112777868827718360993313803
2025-11-22 02:05:18.161Z debug(vsr): 2: prepare_timeout stopped
2025-11-22 02:05:18.161Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-11-22 02:05:18.161Z debug(replica): 2N: execute_op: executing view=2 primary=true op=416 checksum=33931453055112777868827718360993313803 (lookup_accounts)
2025-11-22 02:05:18.161Z debug(replica): 2N: execute_op: commit_timestamp=1763777116439692220 prepare.header.timestamp=1763777117390968178
2025-11-22 02:05:18.161Z debug(replica): 2N: execute_op: advancing commit_max=415..416
2025-11-22 02:05:18.161Z debug(replica): 2N: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=414
2025-11-22 02:05:18.161Z debug(replica): 2N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 324148796434895621655205472624499901180, .checksum_padding = 0, .checksum_body = 327964006529606677042488414844401738121, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .context = 177467258192621183641261823332435897856, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 416, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.161Z debug(replica): 2N: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 324148796434895621655205472624499901180, .checksum_padding = 0, .checksum_body = 327964006529606677042488414844401738121, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .context = 177467258192621183641261823332435897856, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 416, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.161Z debug(forest): entering forest.compact() op=416 constants.lsm_compaction_ops=32 first_beat=true last_half_beat=false half_beat=false last_beat=false
2025-11-22 02:05:18.161Z debug(compaction): Account.id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.user_data_128:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.user_data_64:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.user_data_32:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.ledger:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.code:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.debit_account_id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.credit_account_id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.amount:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.pending_id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.user_data_128:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.user_data_64:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.user_data_32:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.ledger:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.code:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.expires_at:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): TransferPending:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): TransferPending.status:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.imported:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.imported:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.closed:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.closing:1: bar_commence: nothing to compact
warning(client): 25527186891061144814045155886610748587: on_reply: slow request, request=414 op=416 size=2320 lookup_accounts time=770ms
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.account_timestamp:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.transfer_pending_status:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.dr_account_id_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.cr_account_id_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.transfer_pending_id_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.ledger_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.prunable:1: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.user_data_128:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.user_data_64:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.user_data_32:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.ledger:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.code:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.debit_account_id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.credit_account_id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.amount:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.pending_id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.user_data_128:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.user_data_64:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.user_data_32:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.ledger:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.code:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.expires_at:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): TransferPending:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): TransferPending.status:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.imported:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.imported:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.closed:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.closing:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.account_timestamp:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.transfer_pending_status:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.dr_account_id_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.cr_account_id_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z info(workload): accounts created = 128, transfers = 567723, pending transfers = 0, commands run = 207
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.transfer_pending_id_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.ledger_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.prunable:3: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.user_data_128:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.user_data_64:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.user_data_32:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.ledger:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.code:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.debit_account_id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.credit_account_id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.amount:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.pending_id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.user_data_128:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.user_data_64:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.user_data_32:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.ledger:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.code:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.expires_at:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): TransferPending:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): TransferPending.status:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.imported:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.imported:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Account.closed:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): Transfer.closing:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.account_timestamp:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.transfer_pending_status:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.dr_account_id_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.cr_account_id_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.transfer_pending_id_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.ledger_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.161Z debug(compaction): AccountEvent.prunable:5: bar_commence: nothing to compact
2025-11-22 02:05:18.162Z debug(client_replies): 2: write_reply: wrote (client=25527186891061144814045155886610748587 request=414)
2025-11-22 02:05:18.165Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 167566438970102463757668977197162457593, .checksum_padding = 0, .checksum_body = 300380134948611712208472018964147559823, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 244992, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 177467258192621183641261823332435897856, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 415, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 770909831, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.165Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:18.165Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 167566438970102463757668977197162457593, .checksum_padding = 0, .checksum_body = 300380134948611712208472018964147559823, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 244992, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 177467258192621183641261823332435897856, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 415, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 770909831, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.165Z debug(replica): 2N: primary_pipeline_prepare: request checksum=167566438970102463757668977197162457593 client=25527186891061144814045155886610748587
2025-11-22 02:05:18.165Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:18.165Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 167566438970102463757668977197162457593, .checksum_padding = 0, .checksum_body = 300380134948611712208472018964147559823, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 244992, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 177467258192621183641261823332435897856, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 415, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 770909831, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.165Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=266180726854274722396903919139857568806 op=417
2025-11-22 02:05:18.165Z debug(vsr): 2: prepare_timeout started
2025-11-22 02:05:18.165Z debug(vsr): 2: primary_abdicate_timeout started
2025-11-22 02:05:18.165Z debug(vsr): 2: pulse_timeout reset
2025-11-22 02:05:18.165Z debug(replica): 2N: replicate: replicating op=417 to replica 0
2025-11-22 02:05:18.166Z debug(replica): 2N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 266180726854274722396903919139857568806, .checksum_padding = 0, .checksum_body = 300380134948611712208472018964147559823, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 244992, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .request_checksum = 167566438970102463757668977197162457593, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.166Z debug(replica): 2N: replicate: replicating op=417 to replica 1
2025-11-22 02:05:18.166Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 266180726854274722396903919139857568806, .checksum_padding = 0, .checksum_body = 300380134948611712208472018964147559823, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 244992, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .request_checksum = 167566438970102463757668977197162457593, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.166Z debug(replica): 2N: on_prepare: advancing: op=416..417 checksum=33931453055112777868827718360993313803..266180726854274722396903919139857568806
2025-11-22 02:05:18.166Z debug(journal): 2: set_header_as_dirty: op=417 checksum=266180726854274722396903919139857568806
2025-11-22 02:05:18.166Z debug(replica): 2N: append: appending to journal op=417
2025-11-22 02:05:18.166Z debug(journal): 2: write: view=2 slot=417 op=417 len=244992: 266180726854274722396903919139857568806 starting
2025-11-22 02:05:18.166Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=437256192 len=245760 locked
2025-11-22 02:05:18.166Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 266180726854274722396903919139857568806, .checksum_padding = 0, .checksum_body = 300380134948611712208472018964147559823, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 244992, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .request_checksum = 167566438970102463757668977197162457593, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.166Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 167566438970102463757668977197162457593, .checksum_padding = 0, .checksum_body = 300380134948611712208472018964147559823, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 244992, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 177467258192621183641261823332435897856, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 415, .operation = vsr.Operation(139), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 770909831, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.167Z debug(replica): 1n: on_prepare: advancing commit_max=415..416
2025-11-22 02:05:18.167Z debug(replica): 1n: on_prepare: caching prepare.op=417 (commit_min=415 op=416 commit_max=416 prepare_max=1007)
2025-11-22 02:05:18.167Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:18.167Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-22 02:05:18.167Z debug(replica): 1n: on_prepare: advancing: op=416..417 checksum=33931453055112777868827718360993313803..266180726854274722396903919139857568806
2025-11-22 02:05:18.167Z debug(journal): 1: set_header_as_dirty: op=417 checksum=266180726854274722396903919139857568806
2025-11-22 02:05:18.167Z debug(replica): 1n: append: appending to journal op=417
2025-11-22 02:05:18.167Z debug(journal): 1: write: view=2 slot=417 op=417 len=244992: 266180726854274722396903919139857568806 starting
2025-11-22 02:05:18.167Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=437256192 len=245760 locked
2025-11-22 02:05:18.167Z debug(replica): 1n: commit_start_journal: cached prepare op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:18.167Z debug(replica): 1n: repair_prepare: op=417 checksum=266180726854274722396903919139857568806 (already writing)
2025-11-22 02:05:18.167Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=415)
2025-11-22 02:05:18.167Z debug(replica): 1n: execute_op: executing view=2 primary=false op=416 checksum=33931453055112777868827718360993313803 (lookup_accounts)
2025-11-22 02:05:18.167Z debug(replica): 1n: execute_op: commit_timestamp=1763777116439692220 prepare.header.timestamp=1763777117390968178
2025-11-22 02:05:18.167Z debug(replica): 1n: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=414
2025-11-22 02:05:18.167Z debug(replica): 1n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 324148796434895621655205472624499901180, .checksum_padding = 0, .checksum_body = 327964006529606677042488414844401738121, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .context = 177467258192621183641261823332435897856, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 416, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.167Z debug(replica): 1n: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 324148796434895621655205472624499901180, .checksum_padding = 0, .checksum_body = 327964006529606677042488414844401738121, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 87834938227577662541192884506571294145, .request_checksum_padding = 0, .context = 177467258192621183641261823332435897856, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit = 416, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.167Z debug(forest): entering forest.compact() op=416 constants.lsm_compaction_ops=32 first_beat=true last_half_beat=false half_beat=false last_beat=false
2025-11-22 02:05:18.167Z debug(compaction): Account.id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.user_data_128:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.user_data_64:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.user_data_32:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.ledger:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.code:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.debit_account_id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.credit_account_id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.amount:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.pending_id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.user_data_128:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.user_data_64:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.user_data_32:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.ledger:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.code:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.expires_at:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): TransferPending:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): TransferPending.status:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): AccountEvent:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.imported:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.imported:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.closed:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.closing:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): AccountEvent.account_timestamp:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): AccountEvent.transfer_pending_status:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): AccountEvent.dr_account_id_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): AccountEvent.cr_account_id_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): AccountEvent.transfer_pending_id_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): AccountEvent.ledger_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): AccountEvent.prunable:1: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.user_data_128:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.user_data_64:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.user_data_32:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.ledger:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.code:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.debit_account_id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.credit_account_id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.amount:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.pending_id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.user_data_128:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.user_data_64:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.user_data_32:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.ledger:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.code:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.expires_at:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): TransferPending:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): TransferPending.status:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): AccountEvent:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.imported:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.imported:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Account.closed:3: bar_commence: nothing to compact
2025-11-22 02:05:18.167Z debug(compaction): Transfer.closing:3: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): AccountEvent.account_timestamp:3: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): AccountEvent.transfer_pending_status:3: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): AccountEvent.dr_account_id_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): AccountEvent.cr_account_id_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): AccountEvent.transfer_pending_id_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): AccountEvent.ledger_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): AccountEvent.prunable:3: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): Account.id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): Account.user_data_128:5: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): Account.user_data_64:5: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): Account.user_data_32:5: bar_commence: nothing to compact
2025-11-22 02:05:18.168Z debug(compaction): Account.ledger:5: bar_commence: nothing to compact
2025-11-22 02:05:18.184Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_status)
2025-11-22 02:05:18.169Z debug(clock): 2: synchronized: truechimers=2/3 clock_offset=0ns..0ns accuracy=0ns
2025-11-22 02:05:18.665Z debug(compaction): Account.code:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Account:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(clock): 2: system time is 560ns ahead
2025-11-22 02:05:18.665Z debug(compaction): Transfer.debit_account_id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.credit_account_id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(vsr): 2: journal_repair_timeout fired
2025-11-22 02:05:18.665Z debug(compaction): Transfer.amount:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(vsr): 2: journal_repair_timeout reset
2025-11-22 02:05:18.665Z debug(compaction): Transfer.pending_id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.user_data_128:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.user_data_64:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.user_data_32:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.ledger:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.code:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.expires_at:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): TransferPending:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): TransferPending.status:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): AccountEvent:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Account.imported:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.imported:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Account.closed:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): Transfer.closing:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): AccountEvent.account_timestamp:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): AccountEvent.transfer_pending_status:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): AccountEvent.dr_account_id_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): AccountEvent.cr_account_id_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(replica): 2N: repair_prepare: op=417 checksum=266180726854274722396903919139857568806 (already writing)
2025-11-22 02:05:18.665Z debug(compaction): AccountEvent.transfer_pending_id_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): AccountEvent.ledger_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.665Z debug(compaction): AccountEvent.prunable:5: bar_commence: nothing to compact
2025-11-22 02:05:18.666Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=437256192 len=245760 unlocked
2025-11-22 02:05:18.666Z debug(journal): 2: write_header: op=417 sectors[106496..110592]
2025-11-22 02:05:18.666Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 locked
2025-11-22 02:05:18.666Z warning(replica): 1n: commit_dispatch: slow request, request=414 size=2320 lookup_accounts time=499ms
2025-11-22 02:05:18.666Z debug(client_replies): 1: write_reply: wrote (client=25527186891061144814045155886610748587 request=414)
2025-11-22 02:05:18.666Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 unlocked
2025-11-22 02:05:18.666Z debug(journal): 2: write: view=2 slot=417 op=417 len=244992: 266180726854274722396903919139857568806 complete, marking clean
2025-11-22 02:05:18.666Z debug(replica): 2N: send_prepare_ok: op=417 checksum=266180726854274722396903919139857568806
2025-11-22 02:05:18.666Z debug(replica): 2N: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 12701515802404064314032874568926543733, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .prepare_checksum = 266180726854274722396903919139857568806, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit_min = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.666Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 12701515802404064314032874568926543733, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .prepare_checksum = 266180726854274722396903919139857568806, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit_min = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.666Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-22 02:05:18.666Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2025-11-22 02:05:18.666Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2025-11-22 02:05:18.666Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=437256192 len=245760 unlocked
2025-11-22 02:05:18.666Z debug(journal): 1: write_header: op=417 sectors[106496..110592]
2025-11-22 02:05:18.666Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 locked
2025-11-22 02:05:18.666Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 unlocked
2025-11-22 02:05:18.666Z debug(journal): 1: write: view=2 slot=417 op=417 len=244992: 266180726854274722396903919139857568806 complete, marking clean
2025-11-22 02:05:18.667Z debug(replica): 1n: send_prepare_ok: op=417 checksum=266180726854274722396903919139857568806
2025-11-22 02:05:18.667Z debug(replica): 1n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 166829466350848769671745576596155340249, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .prepare_checksum = 266180726854274722396903919139857568806, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit_min = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.667Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 166829466350848769671745576596155340249, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .prepare_checksum = 266180726854274722396903919139857568806, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit_min = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.667Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-22 02:05:18.667Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2025-11-22 02:05:18.667Z debug(replica): 2N: on_prepare_ok: quorum received, context=266180726854274722396903919139857568806
2025-11-22 02:05:18.667Z debug(vsr): 2: prepare_timeout stopped
2025-11-22 02:05:18.667Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-11-22 02:05:18.670Z debug(replica): 2N: execute_op: executing view=2 primary=true op=417 checksum=266180726854274722396903919139857568806 (create_transfers)
2025-11-22 02:05:18.670Z debug(replica): 2N: execute_op: commit_timestamp=1763777117390968178 prepare.header.timestamp=1763777118165271657
2025-11-22 02:05:18.676Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:18.676Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:18.691Z debug(forest): swap_mutable_and_immutable(AccountEvent.dr_account_id_expired)
2025-11-22 02:05:18.691Z debug(forest): swap_mutable_and_immutable(AccountEvent.cr_account_id_expired)
2025-11-22 02:05:18.691Z debug(forest): swap_mutable_and_immutable(AccountEvent.transfer_pending_id_expired)
2025-11-22 02:05:18.691Z debug(forest): swap_mutable_and_immutable(AccountEvent.ledger_expired)
2025-11-22 02:05:18.691Z debug(forest): swap_mutable_and_immutable(AccountEvent.prunable)
2025-11-22 02:05:18.693Z debug(replica): 2N: execute_op: advancing commit_max=416..417
2025-11-22 02:05:18.693Z debug(replica): 2N: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=415
2025-11-22 02:05:18.693Z debug(replica): 2N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 173125285184812661673077141110947763147, .checksum_padding = 0, .checksum_body = 60448659833860529843734149287254565893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 167566438970102463757668977197162457593, .request_checksum_padding = 0, .context = 220592555989836855249744245990322818099, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit = 417, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.693Z debug(replica): 2N: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 173125285184812661673077141110947763147, .checksum_padding = 0, .checksum_body = 60448659833860529843734149287254565893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 167566438970102463757668977197162457593, .request_checksum_padding = 0, .context = 220592555989836855249744245990322818099, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit = 417, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.693Z debug(forest): entering forest.compact() op=417 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
warning(client): 25527186891061144814045155886610748587: on_reply: slow request, request=415 op=417 size=244992 create_transfers time=530ms
2025-11-22 02:05:18.696Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:18.696Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:18.702Z warning(replica): 0n: commit_dispatch: slow request, request=413 size=130000 lookup_transfers time=1311ms
2025-11-22 02:05:18.709Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Ping{ .checksum = 214468736910393699607752700052030416603, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261008614158977, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 18446744073692840448, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.709Z debug(replica): 0n: sending pong to replica 2: vsr.message_header.Header.Pong{ .checksum = 109854412708082367545995502735174222656, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 15261008614158977, .pong_timestamp_wall = 1763777118709602343, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.709Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Commit{ .checksum = 113974119832431332344302574162425780824, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.commit, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .commit_checksum = 114369912280738590811815879832491838480, .commit_checksum_padding = 0, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .commit = 415, .timestamp_monotonic = 15261008614325178, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.709Z debug(vsr): 0: normal_heartbeat_timeout reset
2025-11-22 02:05:18.709Z debug(replica): 0n: on_commit: checksum verified
2025-11-22 02:05:18.709Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 266180726854274722396903919139857568806, .checksum_padding = 0, .checksum_body = 300380134948611712208472018964147559823, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 244992, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .request_checksum = 167566438970102463757668977197162457593, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.709Z debug(replica): 0n: on_prepare: advancing commit_max=415..416
2025-11-22 02:05:18.709Z debug(replica): 0n: on_prepare: caching prepare.op=417 (commit_min=415 op=416 commit_max=416 prepare_max=1007)
2025-11-22 02:05:18.709Z debug(replica): 0n: on_prepare: advancing: op=416..417 checksum=33931453055112777868827718360993313803..266180726854274722396903919139857568806
2025-11-22 02:05:18.709Z debug(journal): 0: set_header_as_dirty: op=417 checksum=266180726854274722396903919139857568806
2025-11-22 02:05:18.709Z debug(replica): 0n: append: appending to journal op=417
2025-11-22 02:05:18.709Z debug(journal): 0: write: view=2 slot=417 op=417 len=244992: 266180726854274722396903919139857568806 starting
2025-11-22 02:05:18.709Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=437256192 len=245760 locked
2025-11-22 02:05:18.709Z debug(replica): 0n: commit_start_journal: cached prepare op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:18.710Z debug(replica): 0n: repair_prepare: op=416 checksum=33931453055112777868827718360993313803 (already writing)
2025-11-22 02:05:18.710Z debug(replica): 0n: repair_prepare: op=417 checksum=266180726854274722396903919139857568806 (already writing)
2025-11-22 02:05:18.710Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=415)
2025-11-22 02:05:18.710Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 16834116429194101251723604348018578256, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 220592555989836855249744245990322818099, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 416, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 530459209, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.710Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:18.710Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 16834116429194101251723604348018578256, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 220592555989836855249744245990322818099, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 416, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 530459209, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.710Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=436207616 len=4096 unlocked
2025-11-22 02:05:18.710Z debug(journal): 0: write_header: op=416 sectors[106496..110592]
2025-11-22 02:05:18.710Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 locked
2025-11-22 02:05:18.710Z debug(replica): 0n: execute_op: executing view=2 primary=false op=416 checksum=33931453055112777868827718360993313803 (lookup_accounts)
2025-11-22 02:05:18.710Z debug(replica): 0n: execute_op: commit_timestamp=1763777116439692220 prepare.header.timestamp=1763777117390968178
2025-11-22 02:05:18.710Z debug(replica): 0n: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=414
2025-11-22 02:05:18.710Z debug(forest): entering forest.compact() op=416 constants.lsm_compaction_ops=32 first_beat=true last_half_beat=false half_beat=false last_beat=false
2025-11-22 02:05:18.710Z debug(compaction): Account.id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.user_data_128:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.user_data_64:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.user_data_32:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.ledger:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.code:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.debit_account_id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.credit_account_id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.amount:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.pending_id:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.user_data_128:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.user_data_64:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.user_data_32:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.ledger:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.code:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.expires_at:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): TransferPending:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): TransferPending.status:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.imported:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.imported:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.closed:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.closing:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.account_timestamp:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.transfer_pending_status:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.dr_account_id_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.cr_account_id_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.transfer_pending_id_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.ledger_expired:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.prunable:1: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.user_data_128:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.user_data_64:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.user_data_32:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.ledger:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.code:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.debit_account_id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.credit_account_id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.amount:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.pending_id:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.user_data_128:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.user_data_64:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.user_data_32:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.ledger:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.code:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.expires_at:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): TransferPending:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): TransferPending.status:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.imported:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.imported:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.closed:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.closing:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.account_timestamp:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.transfer_pending_status:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.dr_account_id_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.cr_account_id_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.transfer_pending_id_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.ledger_expired:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.prunable:3: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.user_data_128:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.user_data_64:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.user_data_32:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.ledger:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.code:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.debit_account_id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.credit_account_id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.amount:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.pending_id:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.user_data_128:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.user_data_64:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.user_data_32:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.ledger:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.code:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.expires_at:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): TransferPending:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): TransferPending.status:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.imported:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.imported:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Account.closed:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): Transfer.closing:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.account_timestamp:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.transfer_pending_status:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.dr_account_id_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.cr_account_id_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.710Z debug(compaction): AccountEvent.transfer_pending_id_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.711Z debug(compaction): AccountEvent.ledger_expired:5: bar_commence: nothing to compact
2025-11-22 02:05:18.711Z debug(compaction): AccountEvent.prunable:5: bar_commence: nothing to compact
2025-11-22 02:05:18.711Z debug(client_replies): 0: write_reply: wrote (client=25527186891061144814045155886610748587 request=413)
2025-11-22 02:05:18.711Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 unlocked
2025-11-22 02:05:18.711Z debug(journal): 0: write: view=2 slot=416 op=416 len=2320: 33931453055112777868827718360993313803 complete, marking clean
2025-11-22 02:05:18.711Z debug(replica): 0n: send_prepare_ok: op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:18.711Z debug(replica): 0n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 183920906126614376019708263399001822141, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .prepare_checksum = 33931453055112777868827718360993313803, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit_min = 416, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.711Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=437256192 len=245760 unlocked
2025-11-22 02:05:18.711Z debug(journal): 0: write_header: op=417 sectors[106496..110592]
2025-11-22 02:05:18.711Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 locked
2025-11-22 02:05:18.711Z debug(client_replies): 0: write_reply: wrote (client=25527186891061144814045155886610748587 request=414)
2025-11-22 02:05:18.711Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 unlocked
2025-11-22 02:05:18.711Z debug(journal): 0: write: view=2 slot=417 op=417 len=244992: 266180726854274722396903919139857568806 complete, marking clean
2025-11-22 02:05:18.711Z debug(replica): 0n: send_prepare_ok: op=417 checksum=266180726854274722396903919139857568806
2025-11-22 02:05:18.711Z debug(replica): 0n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 231348442362967220077604985366000036375, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .prepare_checksum = 266180726854274722396903919139857568806, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit_min = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.714Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 16834116429194101251723604348018578256, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 220592555989836855249744245990322818099, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 416, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 530459209, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.714Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:18.714Z debug(replica): 2N: primary_pipeline_prepare: request checksum=16834116429194101251723604348018578256 client=25527186891061144814045155886610748587
2025-11-22 02:05:18.714Z debug(replica): 2N: primary_pipeline_prepare: prepare checksum=54797522042211716756989108494655606786 op=418
2025-11-22 02:05:18.714Z debug(vsr): 2: prepare_timeout started
2025-11-22 02:05:18.714Z debug(vsr): 2: primary_abdicate_timeout started
2025-11-22 02:05:18.714Z debug(vsr): 2: pulse_timeout reset
2025-11-22 02:05:18.714Z debug(replica): 2N: replicate: replicating op=418 to replica 0
2025-11-22 02:05:18.714Z debug(replica): 2N: sending prepare to replica 0: vsr.message_header.Header.Prepare{ .checksum = 54797522042211716756989108494655606786, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .request_checksum = 16834116429194101251723604348018578256, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.714Z debug(replica): 2N: replicate: replicating op=418 to replica 1
2025-11-22 02:05:18.714Z debug(replica): 2N: sending prepare to replica 1: vsr.message_header.Header.Prepare{ .checksum = 54797522042211716756989108494655606786, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .request_checksum = 16834116429194101251723604348018578256, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.714Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 54797522042211716756989108494655606786, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .request_checksum = 16834116429194101251723604348018578256, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.714Z debug(replica): 0n: on_prepare: advancing commit_max=416..417
2025-11-22 02:05:18.714Z debug(replica): 0n: on_prepare: caching prepare.op=418 (commit_min=416 op=417 commit_max=417 prepare_max=1007)
2025-11-22 02:05:18.714Z debug(replica): 2N: on_prepare: advancing: op=417..418 checksum=266180726854274722396903919139857568806..54797522042211716756989108494655606786
2025-11-22 02:05:18.714Z debug(replica): 0n: on_prepare: advancing: op=417..418 checksum=266180726854274722396903919139857568806..54797522042211716756989108494655606786
2025-11-22 02:05:18.714Z debug(journal): 0: set_header_as_dirty: op=418 checksum=54797522042211716756989108494655606786
2025-11-22 02:05:18.714Z debug(journal): 2: set_header_as_dirty: op=418 checksum=54797522042211716756989108494655606786
2025-11-22 02:05:18.714Z debug(replica): 0n: append: appending to journal op=418
2025-11-22 02:05:18.714Z debug(journal): 0: write: view=2 slot=418 op=418 len=2320: 54797522042211716756989108494655606786 starting
2025-11-22 02:05:18.714Z debug(replica): 2N: append: appending to journal op=418
2025-11-22 02:05:18.714Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=438304768 len=4096 locked
2025-11-22 02:05:18.715Z debug(journal): 2: write: view=2 slot=418 op=418 len=2320: 54797522042211716756989108494655606786 starting
2025-11-22 02:05:18.715Z debug(replica): 0n: commit_start_journal: cached prepare op=417 checksum=266180726854274722396903919139857568806
2025-11-22 02:05:18.714Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Prepare{ .checksum = 54797522042211716756989108494655606786, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.prepare, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .request_checksum = 16834116429194101251723604348018578256, .request_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.715Z debug(replica): 1n: on_prepare: advancing commit_max=416..417
2025-11-22 02:05:18.715Z debug(replica): 1n: on_prepare: caching prepare.op=418 (commit_min=416 op=417 commit_max=417 prepare_max=1007)
2025-11-22 02:05:18.715Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=438304768 len=4096 locked
2025-11-22 02:05:18.715Z debug(replica): 1n: on_prepare: advancing: op=417..418 checksum=266180726854274722396903919139857568806..54797522042211716756989108494655606786
2025-11-22 02:05:18.715Z debug(journal): 1: set_header_as_dirty: op=418 checksum=54797522042211716756989108494655606786
2025-11-22 02:05:18.715Z debug(replica): 1n: append: appending to journal op=418
2025-11-22 02:05:18.715Z debug(journal): 1: write: view=2 slot=418 op=418 len=2320: 54797522042211716756989108494655606786 starting
2025-11-22 02:05:18.715Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=438304768 len=4096 locked
2025-11-22 02:05:18.715Z debug(replica): 1n: commit_start_journal: cached prepare op=417 checksum=266180726854274722396903919139857568806
2025-11-22 02:05:18.715Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Pong{ .checksum = 109854412708082367545995502735174222656, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 15261008614158977, .pong_timestamp_wall = 1763777118709602343, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.715Z debug(clock): 2: learn: m0=15261008614158977 < window.monotonic=15261008815258246
2025-11-22 02:05:18.715Z debug(replica): 0n: repair_prepare: op=418 checksum=54797522042211716756989108494655606786 (already writing)
2025-11-22 02:05:18.715Z debug(client_replies): 2: write_reply: wrote (client=25527186891061144814045155886610748587 request=415)
2025-11-22 02:05:18.715Z debug(replica): 0n: commit_journal: already committing (prefetch; commit_min=416)
2025-11-22 02:05:18.715Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.715Z debug(replica): 1n: repair_prepare: op=418 checksum=54797522042211716756989108494655606786 (already writing)
2025-11-22 02:05:18.715Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.715Z debug(replica): 1n: commit_journal: already committing (prefetch; commit_min=416)
2025-11-22 02:05:18.715Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 16834116429194101251723604348018578256, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 220592555989836855249744245990322818099, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 416, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 530459209, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.715Z debug(replica): 2N: on_request: new request
2025-11-22 02:05:18.715Z debug(replica): 2N: on_request: ignoring (already preparing)
2025-11-22 02:05:18.715Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 183920906126614376019708263399001822141, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 114369912280738590811815879832491838480, .parent_padding = 0, .prepare_checksum = 33931453055112777868827718360993313803, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 416, .commit_min = 416, .timestamp = 1763777117390968178, .request = 414, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.715Z debug(replica): 2N: on_prepare_ok: not preparing op=416 checksum=33931453055112777868827718360993313803
2025-11-22 02:05:18.715Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 231348442362967220077604985366000036375, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 33931453055112777868827718360993313803, .parent_padding = 0, .prepare_checksum = 266180726854274722396903919139857568806, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit_min = 416, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.715Z debug(replica): 2N: on_prepare_ok: not preparing op=417 checksum=266180726854274722396903919139857568806
2025-11-22 02:05:18.715Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.prepares offset=438304768 len=4096 unlocked
2025-11-22 02:05:18.715Z debug(journal): 2: write_header: op=418 sectors[106496..110592]
2025-11-22 02:05:18.715Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 locked
2025-11-22 02:05:18.716Z debug(journal): 2: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 unlocked
2025-11-22 02:05:18.716Z debug(journal): 2: write: view=2 slot=418 op=418 len=2320: 54797522042211716756989108494655606786 complete, marking clean
2025-11-22 02:05:18.716Z debug(replica): 2N: send_prepare_ok: op=418 checksum=54797522042211716756989108494655606786
2025-11-22 02:05:18.716Z debug(replica): 0n: execute_op: executing view=2 primary=false op=417 checksum=266180726854274722396903919139857568806 (create_transfers)
2025-11-22 02:05:18.716Z debug(replica): 0n: execute_op: commit_timestamp=1763777117390968178 prepare.header.timestamp=1763777118165271657
2025-11-22 02:05:18.716Z debug(replica): 2N: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 84864603264569323633932719098037022207, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .prepare_checksum = 54797522042211716756989108494655606786, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit_min = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.716Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 84864603264569323633932719098037022207, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .prepare_checksum = 54797522042211716756989108494655606786, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit_min = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.716Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-22 02:05:18.716Z debug(replica): 2N: on_prepare_ok: 1 message(s)
2025-11-22 02:05:18.716Z debug(replica): 1n: execute_op: executing view=2 primary=false op=417 checksum=266180726854274722396903919139857568806 (create_transfers)
2025-11-22 02:05:18.716Z debug(replica): 1n: execute_op: commit_timestamp=1763777117390968178 prepare.header.timestamp=1763777118165271657
2025-11-22 02:05:18.716Z debug(replica): 2N: on_prepare_ok: waiting for quorum
2025-11-22 02:05:18.725Z debug(replica): 0n: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=415
2025-11-22 02:05:18.725Z debug(replica): 1n: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=415
2025-11-22 02:05:18.725Z debug(forest): entering forest.compact() op=417 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-22 02:05:18.725Z debug(replica): 1n: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 173125285184812661673077141110947763147, .checksum_padding = 0, .checksum_body = 60448659833860529843734149287254565893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 167566438970102463757668977197162457593, .request_checksum_padding = 0, .context = 220592555989836855249744245990322818099, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit = 417, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.725Z debug(replica): 1n: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 173125285184812661673077141110947763147, .checksum_padding = 0, .checksum_body = 60448659833860529843734149287254565893, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 272, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 167566438970102463757668977197162457593, .request_checksum_padding = 0, .context = 220592555989836855249744245990322818099, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 417, .commit = 417, .timestamp = 1763777118165271657, .request = 415, .operation = vsr.Operation(139), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:18.725Z debug(forest): entering forest.compact() op=417 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-22 02:05:18.732Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.prepares offset=438304768 len=4096 unlocked
2025-11-22 02:05:18.732Z debug(journal): 0: write_header: op=418 sectors[106496..110592]
2025-11-22 02:05:18.732Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 locked
2025-11-22 02:05:18.732Z debug(client_replies): 0: write_reply: wrote (client=25527186891061144814045155886610748587 request=415)
2025-11-22 02:05:18.732Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:18.732Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:18.732Z debug(journal): 0: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 unlocked
2025-11-22 02:05:18.732Z debug(journal): 0: write: view=2 slot=418 op=418 len=2320: 54797522042211716756989108494655606786 complete, marking clean
2025-11-22 02:05:18.732Z debug(replica): 0n: send_prepare_ok: op=418 checksum=54797522042211716756989108494655606786
2025-11-22 02:05:18.732Z debug(replica): 0n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 33340165154110138970540617882043151599, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .prepare_checksum = 54797522042211716756989108494655606786, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit_min = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.735Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:18.735Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:18.740Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:18.740Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:18.740Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:18.740Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:18.740Z debug(replica): 1n: repair_prepare: op=418 checksum=54797522042211716756989108494655606786 (already writing)
2025-11-22 02:05:18.740Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.prepares offset=438304768 len=4096 unlocked
2025-11-22 02:05:18.740Z debug(journal): 1: write_header: op=418 sectors[106496..110592]
2025-11-22 02:05:18.740Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 locked
2025-11-22 02:05:18.740Z debug(client_replies): 1: write_reply: wrote (client=25527186891061144814045155886610748587 request=415)
2025-11-22 02:05:18.740Z debug(journal): 1: write_sectors: ring=vsr.journal.Ring.headers offset=106496 len=4096 unlocked
2025-11-22 02:05:18.740Z debug(journal): 1: write: view=2 slot=418 op=418 len=2320: 54797522042211716756989108494655606786 complete, marking clean
2025-11-22 02:05:18.740Z debug(replica): 1n: send_prepare_ok: op=418 checksum=54797522042211716756989108494655606786
2025-11-22 02:05:18.740Z debug(replica): 1n: sending prepare_ok to replica 2: vsr.message_header.Header.PrepareOk{ .checksum = 315849798033106773076695309213793374733, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .prepare_checksum = 54797522042211716756989108494655606786, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit_min = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.740Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 315849798033106773076695309213793374733, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .prepare_checksum = 54797522042211716756989108494655606786, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit_min = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:18.740Z debug(vsr): 2: primary_abdicate_timeout reset
2025-11-22 02:05:18.752Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:18.740Z debug(replica): 2N: on_prepare_ok: 2 message(s)
2025-11-22 02:05:18.760Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.394Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.394Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.394Z debug(replica): 2N: on_prepare_ok: quorum received, context=54797522042211716756989108494655606786
2025-11-22 02:05:19.394Z debug(vsr): 2: prepare_timeout stopped
2025-11-22 02:05:19.394Z debug(vsr): 2: primary_abdicate_timeout stopped
2025-11-22 02:05:19.394Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 16834116429194101251723604348018578256, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 220592555989836855249744245990322818099, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 416, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 530459209, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.394Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:19.394Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 16834116429194101251723604348018578256, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 220592555989836855249744245990322818099, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 416, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 530459209, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.394Z debug(replica): 2N: execute_op: executing view=2 primary=true op=418 checksum=54797522042211716756989108494655606786 (lookup_accounts)
2025-11-22 02:05:19.394Z debug(replica): 2N: execute_op: commit_timestamp=1763777118165271657 prepare.header.timestamp=1763777118714600848
2025-11-22 02:05:19.394Z debug(replica): 2N: execute_op: advancing commit_max=417..418
2025-11-22 02:05:19.394Z debug(replica): 2N: client_table_entry_update: client=25527186891061144814045155886610748587 session=2 request=416
2025-11-22 02:05:19.395Z debug(replica): 2N: execute_op: replying to client: vsr.message_header.Header.Reply{ .checksum = 294319379166118852869806557666786360631, .checksum_padding = 0, .checksum_body = 165780215600270052141894972634114204451, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 16834116429194101251723604348018578256, .request_checksum_padding = 0, .context = 135478549256437541704116467753465790879, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit = 418, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.395Z debug(replica): 2N: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 294319379166118852869806557666786360631, .checksum_padding = 0, .checksum_body = 165780215600270052141894972634114204451, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 16834116429194101251723604348018578256, .request_checksum_padding = 0, .context = 135478549256437541704116467753465790879, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit = 418, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.395Z debug(forest): entering forest.compact() op=418 constants.lsm_compaction_ops=32 first_beat=false last_half_beat=false half_beat=false last_beat=false
2025-11-22 02:05:19.361Z info(supervisor): injecting network delays: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 276, .jitter_ms = 50 }, .lose = null, .corrupt = null }
2025-11-22 02:05:19.396Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.PrepareOk{ .checksum = 33340165154110138970540617882043151599, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.0, .protocol = 0, .command = vsr.Command.prepare_ok, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 266180726854274722396903919139857568806, .parent_padding = 0, .prepare_checksum = 54797522042211716756989108494655606786, .prepare_checksum_padding = 0, .checkpoint_id = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit_min = 417, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0 } }
2025-11-22 02:05:19.396Z debug(replica): 2N: on_prepare_ok: not preparing op=418 checksum=54797522042211716756989108494655606786
2025-11-22 02:05:19.396Z debug(replica): 2N: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 16834116429194101251723604348018578256, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 220592555989836855249744245990322818099, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 416, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 530459209, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.396Z debug(replica): 2N: on_request: replying to duplicate request
2025-11-22 02:05:19.396Z debug(replica): 2N: on_request: repeat reply (client=25527186891061144814045155886610748587 request=416)
2025-11-22 02:05:19.396Z debug(replica): 2N: sending reply to client 25527186891061144814045155886610748587: vsr.message_header.Header.Reply{ .checksum = 294319379166118852869806557666786360631, .checksum_padding = 0, .checksum_body = 165780215600270052141894972634114204451, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 16768, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.reply, .replica = 2, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .request_checksum = 16834116429194101251723604348018578256, .request_checksum_padding = 0, .context = 135478549256437541704116467753465790879, .context_padding = 0, .client = 25527186891061144814045155886610748587, .op = 418, .commit = 418, .timestamp = 1763777118714600848, .request = 416, .operation = vsr.Operation(140), .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.396Z debug(client_replies): 2: write_reply: wrote (client=25527186891061144814045155886610748587 request=416)
2025-11-22 02:05:19.406Z info(supervisor): injecting network loss: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 276, .jitter_ms = 50 }, .lose = 4/100, .corrupt = null }
2025-11-22 02:05:19.406Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:19.406Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:19.414Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.414Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.414Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.414Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.416Z info(supervisor): healing network
2025-11-22 02:05:19.426Z info(supervisor): injecting network loss: testing.vortex.faulty_network.Faults{ .delay = null, .lose = 5/100, .corrupt = null }
2025-11-22 02:05:19.426Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:19.426Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:19.434Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.434Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.434Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.434Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.436Z info(supervisor): healing network
2025-11-22 02:05:19.446Z info(supervisor): injecting network delays: testing.vortex.faulty_network.Faults{ .delay = testing.vortex.faulty_network.Faults.Delay{ .time_ms = 429, .jitter_ms = 50 }, .lose = null, .corrupt = null }
2025-11-22 02:05:19.446Z debug(vsr): 2: journal_repair_budget_timeout fired
2025-11-22 02:05:19.446Z debug(vsr): 2: journal_repair_budget_timeout reset
2025-11-22 02:05:19.454Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.454Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.454Z debug(vsr): 0: ping_timeout fired
2025-11-22 02:05:19.454Z debug(vsr): 0: ping_timeout reset
2025-11-22 02:05:19.454Z debug(replica): 0n: sending ping to replica 1: vsr.message_header.Header.Ping{ .checksum = 191957551063565854907363805474863374009, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261010100550527, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.454Z debug(replica): 0n: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 191957551063565854907363805474863374009, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261010100550527, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.455Z debug(vsr): 0: start_view_change_message_timeout fired
2025-11-22 02:05:19.455Z debug(vsr): 0: start_view_change_message_timeout reset
2025-11-22 02:05:19.455Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.455Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.455Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:19.455Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:19.455Z debug(vsr): 0: repair_sync_timeout fired
2025-11-22 02:05:19.455Z debug(vsr): 0: repair_sync_timeout reset
2025-11-22 02:05:19.455Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-11-22 02:05:19.455Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-11-22 02:05:19.456Z info(supervisor): healing network
2025-11-22 02:05:19.456Z debug(vsr): 2: journal_repair_timeout fired
2025-11-22 02:05:19.456Z debug(vsr): 2: journal_repair_timeout reset
2025-11-22 02:05:19.456Z debug(vsr): 2: pulse_timeout fired
2025-11-22 02:05:19.456Z debug(vsr): 2: pulse_timeout reset
2025-11-22 02:05:19.466Z info(supervisor): 2: terminating replica
2025-11-22 02:05:19.474Z debug(vsr): 1: ping_timeout fired
2025-11-22 02:05:19.474Z debug(vsr): 1: ping_timeout reset
2025-11-22 02:05:19.474Z debug(replica): 1n: sending ping to replica 0: vsr.message_header.Header.Ping{ .checksum = 53051981069633453887591684506281586546, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261010120473425, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.474Z debug(replica): 1n: sending ping to replica 2: vsr.message_header.Header.Ping{ .checksum = 53051981069633453887591684506281586546, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261010120473425, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.474Z debug(vsr): 1: start_view_change_message_timeout fired
2025-11-22 02:05:19.474Z debug(vsr): 1: start_view_change_message_timeout reset
2025-11-22 02:05:19.474Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.474Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.474Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:19.474Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:19.475Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-11-22 02:05:19.475Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-11-22 02:05:19.475Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.475Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.475Z warning(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-11-22 02:05:19.475Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Ping{ .checksum = 53051981069633453887591684506281586546, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261010120473425, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.475Z debug(replica): 0n: sending pong to replica 1: vsr.message_header.Header.Pong{ .checksum = 111613035840136041574005244292789081031, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 15261010120473425, .pong_timestamp_wall = 1763777119475676551, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.475Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-11-22 02:05:19.485Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=88ms
2025-11-22 02:05:19.485Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=88ms
2025-11-22 02:05:19.485Z info(supervisor): 2: starting replica
2025-11-22 02:05:19.486Z info(supervisor): going into 2m54s quiescence (no faults)
2025-11-22 02:05:19.487Z info(io): opening "0_2.tigerbeetle"...
2025-11-22 02:05:19.495Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.495Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.495Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.495Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.507Z info(main): multiversioning: upgrades disabled for development (0.0.1) release.
2025-11-22 02:05:19.507Z info(main): release=0.0.1
2025-11-22 02:05:19.507Z info(main): release_client_min=0.0.1
2025-11-22 02:05:19.507Z info(main): releases_bundled={ 0.0.1 }
2025-11-22 02:05:19.507Z info(main): git_commit=c96248ac9d2447c1ca0f54afc712dbc7e8aa39be
2025-11-22 02:05:19.508Z debug(superblock): null: open: started
2025-11-22 02:05:19.508Z debug(superblock): null: open: read_header: copy=0 size=8192 offset=0
2025-11-22 02:05:19.508Z debug(superblock): null: open: read_header: copy=1 size=8192 offset=24576
2025-11-22 02:05:19.508Z debug(superblock): null: open: read_header: copy=2 size=8192 offset=49152
2025-11-22 02:05:19.508Z debug(superblock): null: open: read_header: copy=3 size=8192 offset=73728
2025-11-22 02:05:19.508Z debug(superblock_quorums): copy: 0/4: checksum=ea78aec512ca9654218ea42caa57f65 parent=32858739ff31f6e76310a8283c39a4a1 sequence=5
2025-11-22 02:05:19.508Z debug(superblock_quorums): copy: 1/4: checksum=ea78aec512ca9654218ea42caa57f65 parent=32858739ff31f6e76310a8283c39a4a1 sequence=5
2025-11-22 02:05:19.508Z debug(superblock_quorums): copy: 2/4: checksum=ea78aec512ca9654218ea42caa57f65 parent=32858739ff31f6e76310a8283c39a4a1 sequence=5
2025-11-22 02:05:19.508Z debug(superblock_quorums): copy: 3/4: checksum=ea78aec512ca9654218ea42caa57f65 parent=32858739ff31f6e76310a8283c39a4a1 sequence=5
2025-11-22 02:05:19.508Z debug(superblock_quorums): quorum: checksum=ea78aec512ca9654218ea42caa57f65 parent=32858739ff31f6e76310a8283c39a4a1 sequence=5 count=4 valid=true
2025-11-22 02:05:19.508Z debug(superblock): null: open: installed working superblock: checksum=0ea78aec512ca9654218ea42caa57f65 sequence=5 release=0.0.1 cluster=00000000000000000000000000000000 replica_id=134075807420264837279280775697797238201 size=1141374976 free_set_blocks_acquired_size=0 free_set_blocks_released_size=0 client_sessions_size=0 checkpoint_id=f222e9ce156b309eaeb4af665242ac18 commit_min_checksum=108034676951432761169128540124443993015 commit_min=0 commit_max=102 log_view=2 view=2 sync_op_min=0 sync_op_max=0 manifest_oldest_checksum=0 manifest_oldest_address=0 manifest_newest_checksum=0 manifest_newest_address=0 manifest_block_count=0 snapshots_block_checksum=0 snapshots_block_address=0
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=103 checksum=120186189516173734601400780242925644884
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=102 checksum=13330470840326067635421720970941248165
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=101 checksum=285322570042136214136213121435699428223
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=100 checksum=312931321865272603439602565389025481400
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=99 checksum=295533488505820377259397309007600575282
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=98 checksum=100833430864933138909698143708010831643
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=97 checksum=317115486672690743061314808406909978946
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=96 checksum=313314077474921545146249694869807074997
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=95 checksum=250582104848844534077632294540959178343
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=47 checksum=328077536456549326473478682792850080455
2025-11-22 02:05:19.509Z debug(superblock): null: open: vsr_header: op=0 checksum=108034676951432761169128540124443993015
2025-11-22 02:05:19.509Z debug(superblock): null: open: complete
2025-11-22 02:05:19.509Z debug(journal): 2: slot_count=1024 size=1.000244140625GiB headers_size=256KiB prepares_size=1GiB
2025-11-22 02:05:19.515Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.515Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.515Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.515Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.531Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 16834116429194101251723604348018578256, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 220592555989836855249744245990322818099, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 416, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 530459209, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.531Z debug(replica): 1n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:19.531Z debug(replica): 1n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 16834116429194101251723604348018578256, .checksum_padding = 0, .checksum_body = 12537678976109067873367668761089920717, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 2320, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 220592555989836855249744245990322818099, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 416, .operation = vsr.Operation(140), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 530459209, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.535Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.535Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.535Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.535Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.555Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.555Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.555Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.555Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.555Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:19.555Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:19.573Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-11-22 02:05:19.573Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-11-22 02:05:19.573Z info(message_bus): 1: on_connect: connected to=2
2025-11-22 02:05:19.573Z info(message_bus): 0: on_connect: connected to=2
2025-11-22 02:05:19.573Z warning(faulty_network): connect failed (2,8): error.ConnectionRefused
2025-11-22 02:05:19.573Z warning(faulty_network): connect failed (2,3): error.ConnectionRefused
2025-11-22 02:05:19.573Z warning(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } error.ConnectionResetByPeer
2025-11-22 02:05:19.573Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-11-22 02:05:19.575Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=55ms
2025-11-22 02:05:19.575Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.575Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.575Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:19.575Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:19.575Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=100ms
2025-11-22 02:05:19.575Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.575Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.595Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.595Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.595Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.595Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.615Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.615Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.616Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.616Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.630Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-11-22 02:05:19.630Z info(message_bus): 1: on_connect: connected to=2
2025-11-22 02:05:19.631Z warning(faulty_network): connect failed (2,0): error.ConnectionRefused
2025-11-22 02:05:19.631Z info(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-11-22 02:05:19.636Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.636Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.636Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=88ms
2025-11-22 02:05:19.636Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.636Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.639Z debug(manifest_log): 2: Manifest.Pace.half_bar_append_blocks_max = 1
2025-11-22 02:05:19.639Z debug(manifest_log): 2: Manifest.Pace.half_bar_compact_blocks_max = 2
2025-11-22 02:05:19.639Z debug(manifest_log): 2: Manifest.Pace.log_blocks_full_max = 586
2025-11-22 02:05:19.639Z debug(manifest_log): 2: Manifest.Pace.log_blocks_cycle_max = 1172
2025-11-22 02:05:19.639Z debug(manifest_log): 2: Manifest.Pace.log_blocks_max = 1466
2025-11-22 02:05:19.639Z debug(manifest_log): 2: Manifest.Pace.tables_max = 2396744
2025-11-22 02:05:19.656Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.656Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.656Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:19.656Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:19.656Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.656Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.675Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-11-22 02:05:19.675Z info(message_bus): 0: on_connect: connected to=2
2025-11-22 02:05:19.676Z warning(faulty_network): connect failed (2,5): error.ConnectionRefused
2025-11-22 02:05:19.676Z info(message_bus): 0: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-11-22 02:05:19.676Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.676Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.676Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:19.676Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:19.676Z debug(message_bus): 0: connect_to_replica: connecting to=2 after=96ms
2025-11-22 02:05:19.676Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.676Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.696Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.696Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.696Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.696Z debug(vsr): 1: journal_repair_budget_timeout reset
warning(client): 25527186891061144814045155886610748587: on_reply: slow request, request=416 op=418 size=2320 lookup_accounts time=1006ms
2025-11-22 02:05:19.714Z info(workload): accounts created = 128, transfers = 569634, pending transfers = 0, commands run = 208
2025-11-22 02:05:19.715Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Request{ .checksum = 46116206986559632437402048397330628679, .checksum_padding = 0, .checksum_body = 37151468035599221295697772765417148184, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135478549256437541704116467753465790879, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 417, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1006550790, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.716Z debug(replica): 0n: on_request: forwarding new request to primary (view=2)
2025-11-22 02:05:19.716Z debug(replica): 0n: sending request to replica 2: vsr.message_header.Header.Request{ .checksum = 46116206986559632437402048397330628679, .checksum_padding = 0, .checksum_body = 37151468035599221295697772765417148184, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 130000, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.request, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .parent = 135478549256437541704116467753465790879, .parent_padding = 0, .client = 25527186891061144814045155886610748587, .session = 2, .timestamp = 0, .request = 417, .operation = vsr.Operation(141), .previous_request_latency_padding = { 0, 0, 0 }, .previous_request_latency = 1006550790, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.716Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.716Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.716Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.716Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.724Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-11-22 02:05:19.724Z info(message_bus): 1: on_connect: connected to=2
2025-11-22 02:05:19.724Z warning(faulty_network): connect failed (2,9): error.ConnectionRefused
2025-11-22 02:05:19.724Z info(message_bus): 1: on_recv: from=vsr.Peer{ .replica = 2 } orderly shutdown
2025-11-22 02:05:19.726Z debug(message_bus): 1: connect_to_replica: connecting to=2 after=59ms
2025-11-22 02:05:19.736Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.736Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.736Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.736Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.756Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.756Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.756Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:19.756Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:19.756Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.756Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.761Z debug(replica): 2R: init: replica_count=3 quorum_view_change=2 quorum_replication=2 release=0.0.1
2025-11-22 02:05:19.761Z info(replica): superblock release=0.0.1
2025-11-22 02:05:19.761Z debug(journal): 2: recover: recovering
2025-11-22 02:05:19.761Z debug(journal): 2: recover_headers: offset=0 size=262144 recovering
2025-11-22 02:05:19.761Z debug(journal): 2: recover_headers: offset=0 size=262144 recovered
2025-11-22 02:05:19.761Z debug(journal): 2: recover_headers: complete
2025-11-22 02:05:19.761Z debug(journal): 2: recover_prepare: recovering slot=0
2025-11-22 02:05:19.761Z debug(journal): 2: recover_prepare: recovering slot=1
2025-11-22 02:05:19.761Z debug(journal): 2: recover_prepare: recovering slot=2
2025-11-22 02:05:19.761Z debug(journal): 2: recover_prepare: recovering slot=3
2025-11-22 02:05:19.761Z debug(journal): 2: recover_prepare: recovering slot=4
2025-11-22 02:05:19.761Z debug(journal): 2: recover_prepare: recovering slot=5
2025-11-22 02:05:19.761Z debug(journal): 2: recover_prepare: recovering slot=6
2025-11-22 02:05:19.761Z debug(journal): 2: recover_prepare: recovering slot=7
2025-11-22 02:05:19.762Z debug(journal): 2: recover_prepare: recovering slot=8
2025-11-22 02:05:19.762Z debug(journal): 2: recover_prepare: recovering slot=9
2025-11-22 02:05:19.762Z debug(journal): 2: recover_prepare: recovering slot=10
2025-11-22 02:05:19.762Z debug(journal): 2: recover_prepare: recovering slot=11
2025-11-22 02:05:19.762Z debug(journal): 2: recover_prepare: recovering slot=12
2025-11-22 02:05:19.762Z debug(journal): 2: recover_prepare: recovering slot=13
2025-11-22 02:05:19.762Z debug(journal): 2: recover_prepare: recovering slot=14
2025-11-22 02:05:19.762Z debug(journal): 2: recover_prepare: recovering slot=15
2025-11-22 02:05:19.764Z debug(journal): 2: recover_prepare: recovering slot=16
2025-11-22 02:05:19.765Z debug(journal): 2: recover_prepare: recovering slot=17
2025-11-22 02:05:19.765Z debug(journal): 2: recover_prepare: recovering slot=18
2025-11-22 02:05:19.765Z debug(journal): 2: recover_prepare: recovering slot=19
2025-11-22 02:05:19.768Z debug(journal): 2: recover_prepare: recovering slot=20
2025-11-22 02:05:19.769Z debug(journal): 2: recover_prepare: recovering slot=21
2025-11-22 02:05:19.769Z debug(journal): 2: recover_prepare: recovering slot=22
2025-11-22 02:05:19.770Z debug(journal): 2: recover_prepare: recovering slot=23
2025-11-22 02:05:19.770Z debug(journal): 2: recover_prepare: recovering slot=24
2025-11-22 02:05:19.770Z debug(journal): 2: recover_prepare: recovering slot=25
2025-11-22 02:05:19.770Z debug(journal): 2: recover_prepare: recovering slot=26
2025-11-22 02:05:19.772Z debug(message_bus): 0: on_connect_with_exponential_backoff: to=2
2025-11-22 02:05:19.772Z debug(journal): 2: recover_prepare: recovering slot=27
2025-11-22 02:05:19.772Z info(message_bus): 0: on_connect: connected to=2
2025-11-22 02:05:19.772Z debug(journal): 2: recover_prepare: recovering slot=28
2025-11-22 02:05:19.772Z debug(journal): 2: recover_prepare: recovering slot=29
2025-11-22 02:05:19.773Z debug(journal): 2: recover_prepare: recovering slot=30
2025-11-22 02:05:19.775Z debug(journal): 2: recover_prepare: recovering slot=31
2025-11-22 02:05:19.775Z debug(journal): 2: recover_prepare: recovering slot=32
2025-11-22 02:05:19.776Z debug(journal): 2: recover_prepare: recovering slot=33
2025-11-22 02:05:19.776Z debug(journal): 2: recover_prepare: recovering slot=34
2025-11-22 02:05:19.776Z debug(journal): 2: recover_prepare: recovering slot=35
2025-11-22 02:05:19.776Z debug(journal): 2: recover_prepare: recovering slot=36
2025-11-22 02:05:19.776Z debug(journal): 2: recover_prepare: recovering slot=37
2025-11-22 02:05:19.776Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.776Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.777Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.777Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.777Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:19.777Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:19.779Z debug(journal): 2: recover_prepare: recovering slot=38
2025-11-22 02:05:19.781Z debug(journal): 2: recover_prepare: recovering slot=39
2025-11-22 02:05:19.782Z debug(journal): 2: recover_prepare: recovering slot=40
2025-11-22 02:05:19.782Z debug(journal): 2: recover_prepare: recovering slot=41
2025-11-22 02:05:19.782Z debug(journal): 2: recover_prepare: recovering slot=42
2025-11-22 02:05:19.782Z debug(journal): 2: recover_prepare: recovering slot=43
2025-11-22 02:05:19.783Z debug(journal): 2: recover_prepare: recovering slot=44
2025-11-22 02:05:19.783Z debug(journal): 2: recover_prepare: recovering slot=45
2025-11-22 02:05:19.784Z debug(journal): 2: recover_prepare: recovering slot=46
2025-11-22 02:05:19.785Z debug(message_bus): 1: on_connect_with_exponential_backoff: to=2
2025-11-22 02:05:19.785Z info(message_bus): 1: on_connect: connected to=2
2025-11-22 02:05:19.786Z debug(journal): 2: recover_prepare: recovering slot=47
2025-11-22 02:05:19.786Z debug(journal): 2: recover_prepare: recovering slot=48
2025-11-22 02:05:19.786Z debug(journal): 2: recover_prepare: recovering slot=49
2025-11-22 02:05:19.789Z debug(journal): 2: recover_prepare: recovering slot=50
2025-11-22 02:05:19.789Z debug(journal): 2: recover_prepare: recovering slot=51
2025-11-22 02:05:19.789Z debug(journal): 2: recover_prepare: recovering slot=52
2025-11-22 02:05:19.791Z debug(journal): 2: recover_prepare: recovering slot=53
2025-11-22 02:05:19.791Z debug(journal): 2: recover_prepare: recovering slot=54
2025-11-22 02:05:19.791Z debug(journal): 2: recover_prepare: recovering slot=55
2025-11-22 02:05:19.791Z debug(journal): 2: recover_prepare: recovering slot=56
2025-11-22 02:05:19.794Z debug(journal): 2: recover_prepare: recovering slot=57
2025-11-22 02:05:19.796Z debug(journal): 2: recover_prepare: recovering slot=58
2025-11-22 02:05:19.796Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.796Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.797Z debug(journal): 2: recover_prepare: recovering slot=59
2025-11-22 02:05:19.797Z debug(journal): 2: recover_prepare: recovering slot=60
2025-11-22 02:05:19.797Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.797Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.798Z debug(journal): 2: recover_prepare: recovering slot=61
2025-11-22 02:05:19.799Z debug(journal): 2: recover_prepare: recovering slot=62
2025-11-22 02:05:19.800Z debug(journal): 2: recover_prepare: recovering slot=63
2025-11-22 02:05:19.800Z debug(journal): 2: recover_prepare: recovering slot=64
2025-11-22 02:05:19.800Z debug(journal): 2: recover_prepare: recovering slot=65
2025-11-22 02:05:19.800Z debug(journal): 2: recover_prepare: recovering slot=66
2025-11-22 02:05:19.801Z debug(journal): 2: recover_prepare: recovering slot=67
2025-11-22 02:05:19.801Z debug(journal): 2: recover_prepare: recovering slot=68
2025-11-22 02:05:19.801Z debug(journal): 2: recover_prepare: recovering slot=69
2025-11-22 02:05:19.801Z debug(journal): 2: recover_prepare: recovering slot=70
2025-11-22 02:05:19.802Z debug(journal): 2: recover_prepare: recovering slot=71
2025-11-22 02:05:19.802Z debug(journal): 2: recover_prepare: recovering slot=72
2025-11-22 02:05:19.802Z debug(journal): 2: recover_prepare: recovering slot=73
2025-11-22 02:05:19.803Z debug(journal): 2: recover_prepare: recovering slot=74
2025-11-22 02:05:19.805Z debug(journal): 2: recover_prepare: recovering slot=75
2025-11-22 02:05:19.805Z debug(journal): 2: recover_prepare: recovering slot=76
2025-11-22 02:05:19.805Z debug(journal): 2: recover_prepare: recovering slot=77
2025-11-22 02:05:19.805Z debug(journal): 2: recover_prepare: recovering slot=78
2025-11-22 02:05:19.805Z debug(journal): 2: recover_prepare: recovering slot=79
2025-11-22 02:05:19.805Z debug(journal): 2: recover_prepare: recovering slot=80
2025-11-22 02:05:19.805Z debug(journal): 2: recover_prepare: recovering slot=81
2025-11-22 02:05:19.806Z debug(journal): 2: recover_prepare: recovering slot=82
2025-11-22 02:05:19.807Z debug(journal): 2: recover_prepare: recovering slot=83
2025-11-22 02:05:19.807Z debug(journal): 2: recover_prepare: recovering slot=84
2025-11-22 02:05:19.807Z debug(journal): 2: recover_prepare: recovering slot=85
2025-11-22 02:05:19.807Z debug(journal): 2: recover_prepare: recovering slot=86
2025-11-22 02:05:19.808Z debug(journal): 2: recover_prepare: recovering slot=87
2025-11-22 02:05:19.808Z debug(journal): 2: recover_prepare: recovering slot=88
2025-11-22 02:05:19.808Z debug(journal): 2: recover_prepare: recovering slot=89
2025-11-22 02:05:19.808Z debug(journal): 2: recover_prepare: recovering slot=90
2025-11-22 02:05:19.809Z debug(journal): 2: recover_prepare: recovering slot=91
2025-11-22 02:05:19.809Z debug(journal): 2: recover_prepare: recovering slot=92
2025-11-22 02:05:19.809Z debug(journal): 2: recover_prepare: recovering slot=93
2025-11-22 02:05:19.809Z debug(journal): 2: recover_prepare: recovering slot=94
2025-11-22 02:05:19.809Z debug(journal): 2: recover_prepare: recovering slot=95
2025-11-22 02:05:19.809Z debug(journal): 2: recover_prepare: recovering slot=96
2025-11-22 02:05:19.811Z debug(journal): 2: recover_prepare: recovering slot=97
2025-11-22 02:05:19.811Z debug(journal): 2: recover_prepare: recovering slot=98
2025-11-22 02:05:19.812Z debug(journal): 2: recover_prepare: recovering slot=99
2025-11-22 02:05:19.812Z debug(journal): 2: recover_prepare: recovering slot=100
2025-11-22 02:05:19.812Z debug(journal): 2: recover_prepare: recovering slot=101
2025-11-22 02:05:19.813Z debug(journal): 2: recover_prepare: recovering slot=102
2025-11-22 02:05:19.814Z debug(journal): 2: recover_prepare: recovering slot=103
2025-11-22 02:05:19.814Z debug(journal): 2: recover_prepare: recovering slot=104
2025-11-22 02:05:19.814Z debug(journal): 2: recover_prepare: recovering slot=105
2025-11-22 02:05:19.814Z debug(journal): 2: recover_prepare: recovering slot=106
2025-11-22 02:05:19.815Z debug(journal): 2: recover_prepare: recovering slot=107
2025-11-22 02:05:19.815Z debug(journal): 2: recover_prepare: recovering slot=108
2025-11-22 02:05:19.815Z debug(journal): 2: recover_prepare: recovering slot=109
2025-11-22 02:05:19.817Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.817Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.817Z debug(journal): 2: recover_prepare: recovering slot=110
2025-11-22 02:05:19.817Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.817Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.817Z debug(journal): 2: recover_prepare: recovering slot=111
2025-11-22 02:05:19.817Z debug(journal): 2: recover_prepare: recovering slot=112
2025-11-22 02:05:19.817Z debug(journal): 2: recover_prepare: recovering slot=113
2025-11-22 02:05:19.819Z debug(journal): 2: recover_prepare: recovering slot=114
2025-11-22 02:05:19.819Z debug(journal): 2: recover_prepare: recovering slot=115
2025-11-22 02:05:19.819Z debug(journal): 2: recover_prepare: recovering slot=116
2025-11-22 02:05:19.820Z debug(journal): 2: recover_prepare: recovering slot=117
2025-11-22 02:05:19.820Z debug(journal): 2: recover_prepare: recovering slot=118
2025-11-22 02:05:19.820Z debug(journal): 2: recover_prepare: recovering slot=119
2025-11-22 02:05:19.820Z debug(journal): 2: recover_prepare: recovering slot=120
2025-11-22 02:05:19.822Z debug(journal): 2: recover_prepare: recovering slot=121
2025-11-22 02:05:19.824Z debug(journal): 2: recover_prepare: recovering slot=122
2025-11-22 02:05:19.825Z debug(journal): 2: recover_prepare: recovering slot=123
2025-11-22 02:05:19.825Z debug(journal): 2: recover_prepare: recovering slot=124
2025-11-22 02:05:19.826Z debug(journal): 2: recover_prepare: recovering slot=125
2025-11-22 02:05:19.828Z debug(journal): 2: recover_prepare: recovering slot=126
2025-11-22 02:05:19.830Z debug(journal): 2: recover_prepare: recovering slot=127
2025-11-22 02:05:19.830Z debug(journal): 2: recover_prepare: recovering slot=128
2025-11-22 02:05:19.830Z debug(journal): 2: recover_prepare: recovering slot=129
2025-11-22 02:05:19.830Z debug(journal): 2: recover_prepare: recovering slot=130
2025-11-22 02:05:19.830Z debug(journal): 2: recover_prepare: recovering slot=131
2025-11-22 02:05:19.830Z debug(journal): 2: recover_prepare: recovering slot=132
2025-11-22 02:05:19.832Z debug(journal): 2: recover_prepare: recovering slot=133
2025-11-22 02:05:19.834Z debug(journal): 2: recover_prepare: recovering slot=134
2025-11-22 02:05:19.834Z debug(journal): 2: recover_prepare: recovering slot=135
2025-11-22 02:05:19.834Z debug(journal): 2: recover_prepare: recovering slot=136
2025-11-22 02:05:19.835Z debug(journal): 2: recover_prepare: recovering slot=137
2025-11-22 02:05:19.837Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.837Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.837Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.837Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.837Z debug(journal): 2: recover_prepare: recovering slot=138
2025-11-22 02:05:19.837Z debug(journal): 2: recover_prepare: recovering slot=139
2025-11-22 02:05:19.837Z debug(journal): 2: recover_prepare: recovering slot=140
2025-11-22 02:05:19.840Z debug(journal): 2: recover_prepare: recovering slot=141
2025-11-22 02:05:19.840Z debug(journal): 2: recover_prepare: recovering slot=142
2025-11-22 02:05:19.842Z debug(journal): 2: recover_prepare: recovering slot=143
2025-11-22 02:05:19.842Z debug(journal): 2: recover_prepare: recovering slot=144
2025-11-22 02:05:19.842Z debug(journal): 2: recover_prepare: recovering slot=145
2025-11-22 02:05:19.843Z debug(journal): 2: recover_prepare: recovering slot=146
2025-11-22 02:05:19.844Z debug(journal): 2: recover_prepare: recovering slot=147
2025-11-22 02:05:19.844Z debug(journal): 2: recover_prepare: recovering slot=148
2025-11-22 02:05:19.844Z debug(journal): 2: recover_prepare: recovering slot=149
2025-11-22 02:05:19.844Z debug(journal): 2: recover_prepare: recovering slot=150
2025-11-22 02:05:19.845Z debug(journal): 2: recover_prepare: recovering slot=151
2025-11-22 02:05:19.845Z debug(journal): 2: recover_prepare: recovering slot=152
2025-11-22 02:05:19.845Z debug(journal): 2: recover_prepare: recovering slot=153
2025-11-22 02:05:19.846Z debug(journal): 2: recover_prepare: recovering slot=154
2025-11-22 02:05:19.846Z debug(journal): 2: recover_prepare: recovering slot=155
2025-11-22 02:05:19.846Z debug(journal): 2: recover_prepare: recovering slot=156
2025-11-22 02:05:19.848Z debug(journal): 2: recover_prepare: recovering slot=157
2025-11-22 02:05:19.850Z debug(journal): 2: recover_prepare: recovering slot=158
2025-11-22 02:05:19.850Z debug(journal): 2: recover_prepare: recovering slot=159
2025-11-22 02:05:19.850Z debug(journal): 2: recover_prepare: recovering slot=160
2025-11-22 02:05:19.854Z debug(journal): 2: recover_prepare: recovering slot=161
2025-11-22 02:05:19.856Z debug(journal): 2: recover_prepare: recovering slot=162
2025-11-22 02:05:19.857Z debug(journal): 2: recover_prepare: recovering slot=163
2025-11-22 02:05:19.857Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.857Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.858Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.858Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.858Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:19.858Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:19.859Z debug(journal): 2: recover_prepare: recovering slot=164
2025-11-22 02:05:19.859Z debug(journal): 2: recover_prepare: recovering slot=165
2025-11-22 02:05:19.859Z debug(journal): 2: recover_prepare: recovering slot=166
2025-11-22 02:05:19.859Z debug(journal): 2: recover_prepare: recovering slot=167
2025-11-22 02:05:19.862Z debug(journal): 2: recover_prepare: recovering slot=168
2025-11-22 02:05:19.862Z debug(journal): 2: recover_prepare: recovering slot=169
2025-11-22 02:05:19.864Z debug(journal): 2: recover_prepare: recovering slot=170
2025-11-22 02:05:19.864Z debug(journal): 2: recover_prepare: recovering slot=171
2025-11-22 02:05:19.865Z debug(journal): 2: recover_prepare: recovering slot=172
2025-11-22 02:05:19.866Z debug(journal): 2: recover_prepare: recovering slot=173
2025-11-22 02:05:19.866Z debug(journal): 2: recover_prepare: recovering slot=174
2025-11-22 02:05:19.866Z debug(journal): 2: recover_prepare: recovering slot=175
2025-11-22 02:05:19.866Z debug(journal): 2: recover_prepare: recovering slot=176
2025-11-22 02:05:19.869Z debug(journal): 2: recover_prepare: recovering slot=177
2025-11-22 02:05:19.869Z debug(journal): 2: recover_prepare: recovering slot=178
2025-11-22 02:05:19.869Z debug(journal): 2: recover_prepare: recovering slot=179
2025-11-22 02:05:19.869Z debug(journal): 2: recover_prepare: recovering slot=180
2025-11-22 02:05:19.869Z debug(journal): 2: recover_prepare: recovering slot=181
2025-11-22 02:05:19.870Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Ping{ .checksum = 191957551063565854907363805474863374009, .checksum_padding = 0, .checksum_body = 154787626362930377789479683857780696253, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 512, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.ping, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .checkpoint_id = 321854455202207724029466302309042662424, .checkpoint_op = 0, .ping_timestamp_monotonic = 15261010100550527, .release_count = 1, .route_padding = { 0, 0, 0, 0, 0, 0 }, .route = 0, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.870Z debug(replica): 1n: sending pong to replica 0: vsr.message_header.Header.Pong{ .checksum = 169285131137913830958886729386301007890, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 15261010100550527, .pong_timestamp_wall = 1763777119870168331, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.870Z debug(replica): 0n: on_message: view=2 status=normal vsr.message_header.Header.Pong{ .checksum = 169285131137913830958886729386301007890, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 1, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 15261010100550527, .pong_timestamp_wall = 1763777119870168331, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.870Z debug(clock): 0: learn: replica=1 m0=15261010100550527 t1=1763777119870168331 m2=15261010516040731 t2=1763777119870340692 one_way_delay=207745102 asymmetric_delay=-207554481 clock_offset=18260
2025-11-22 02:05:19.870Z debug(replica): 1n: on_message: view=2 status=normal vsr.message_header.Header.Pong{ .checksum = 111613035840136041574005244292789081031, .checksum_padding = 0, .checksum_body = 98287347720187652707502696638535748739, .checksum_body_padding = 0, .nonce_reserved = 0, .cluster = 0, .size = 256, .epoch = 0, .view = 2, .release = 0.0.1, .protocol = 0, .command = vsr.Command.pong, .replica = 0, .reserved_frame = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, .ping_timestamp_monotonic = 15261010120473425, .pong_timestamp_wall = 1763777119475676551, .reserved = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }
2025-11-22 02:05:19.870Z debug(clock): 1: learn: replica=0 m0=15261010120473425 t1=1763777119475676551 m2=15261010516059411 t2=1763777119870359292 one_way_delay=197792993 asymmetric_delay=0 clock_offset=-196889748
2025-11-22 02:05:19.870Z debug(journal): 2: recover_prepare: recovering slot=182
2025-11-22 02:05:19.870Z debug(journal): 2: recover_prepare: recovering slot=183
2025-11-22 02:05:19.871Z debug(journal): 2: recover_prepare: recovering slot=184
2025-11-22 02:05:19.871Z debug(journal): 2: recover_prepare: recovering slot=185
2025-11-22 02:05:19.873Z debug(journal): 2: recover_prepare: recovering slot=186
2025-11-22 02:05:19.873Z debug(journal): 2: recover_prepare: recovering slot=187
2025-11-22 02:05:19.873Z debug(journal): 2: recover_prepare: recovering slot=188
2025-11-22 02:05:19.873Z debug(journal): 2: recover_prepare: recovering slot=189
2025-11-22 02:05:19.873Z debug(journal): 2: recover_prepare: recovering slot=190
2025-11-22 02:05:19.875Z debug(journal): 2: recover_prepare: recovering slot=191
2025-11-22 02:05:19.877Z debug(clock): 1: synchronized: truechimers=2/3 clock_offset=0ns..0ns accuracy=0ns
2025-11-22 02:05:19.877Z debug(clock): 1: system time is 120ns behind
2025-11-22 02:05:19.877Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.877Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.877Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:19.877Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:19.877Z debug(journal): 2: recover_prepare: recovering slot=192
2025-11-22 02:05:19.878Z debug(journal): 2: recover_prepare: recovering slot=193
2025-11-22 02:05:19.878Z debug(clock): 0: synchronized: truechimers=2/3 clock_offset=0ns..0ns accuracy=0ns
2025-11-22 02:05:19.878Z debug(clock): 0: system time is 160ns behind
2025-11-22 02:05:19.878Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.878Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.878Z debug(journal): 2: recover_prepare: recovering slot=194
2025-11-22 02:05:19.878Z debug(journal): 2: recover_prepare: recovering slot=195
2025-11-22 02:05:19.878Z debug(journal): 2: recover_prepare: recovering slot=196
2025-11-22 02:05:19.879Z debug(journal): 2: recover_prepare: recovering slot=197
2025-11-22 02:05:19.879Z debug(journal): 2: recover_prepare: recovering slot=198
2025-11-22 02:05:19.879Z debug(journal): 2: recover_prepare: recovering slot=199
2025-11-22 02:05:19.879Z debug(journal): 2: recover_prepare: recovering slot=200
2025-11-22 02:05:19.882Z debug(journal): 2: recover_prepare: recovering slot=201
2025-11-22 02:05:19.882Z debug(journal): 2: recover_prepare: recovering slot=202
2025-11-22 02:05:19.882Z debug(journal): 2: recover_prepare: recovering slot=203
2025-11-22 02:05:19.884Z debug(journal): 2: recover_prepare: recovering slot=204
2025-11-22 02:05:19.886Z debug(journal): 2: recover_prepare: recovering slot=205
2025-11-22 02:05:19.886Z debug(journal): 2: recover_prepare: recovering slot=206
2025-11-22 02:05:19.886Z debug(journal): 2: recover_prepare: recovering slot=207
2025-11-22 02:05:19.886Z debug(journal): 2: recover_prepare: recovering slot=208
2025-11-22 02:05:19.886Z debug(journal): 2: recover_prepare: recovering slot=209
2025-11-22 02:05:19.887Z debug(journal): 2: recover_prepare: recovering slot=210
2025-11-22 02:05:19.887Z debug(journal): 2: recover_prepare: recovering slot=211
2025-11-22 02:05:19.889Z debug(journal): 2: recover_prepare: recovering slot=212
2025-11-22 02:05:19.890Z debug(journal): 2: recover_prepare: recovering slot=213
2025-11-22 02:05:19.890Z debug(journal): 2: recover_prepare: recovering slot=214
2025-11-22 02:05:19.890Z debug(journal): 2: recover_prepare: recovering slot=215
2025-11-22 02:05:19.891Z debug(journal): 2: recover_prepare: recovering slot=216
2025-11-22 02:05:19.891Z debug(journal): 2: recover_prepare: recovering slot=217
2025-11-22 02:05:19.892Z debug(journal): 2: recover_prepare: recovering slot=218
2025-11-22 02:05:19.892Z debug(journal): 2: recover_prepare: recovering slot=219
2025-11-22 02:05:19.892Z debug(journal): 2: recover_prepare: recovering slot=220
2025-11-22 02:05:19.893Z debug(journal): 2: recover_prepare: recovering slot=221
2025-11-22 02:05:19.893Z debug(journal): 2: recover_prepare: recovering slot=222
2025-11-22 02:05:19.895Z debug(journal): 2: recover_prepare: recovering slot=223
2025-11-22 02:05:19.895Z debug(journal): 2: recover_prepare: recovering slot=224
2025-11-22 02:05:19.896Z debug(journal): 2: recover_prepare: recovering slot=225
2025-11-22 02:05:19.896Z debug(journal): 2: recover_prepare: recovering slot=226
2025-11-22 02:05:19.896Z debug(journal): 2: recover_prepare: recovering slot=227
2025-11-22 02:05:19.897Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:19.897Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:19.898Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:19.898Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:19.898Z debug(journal): 2: recover_prepare: recovering slot=228
2025-11-22 02:05:19.898Z debug(journal): 2: recover_prepare: recovering slot=229
2025-11-22 02:05:19.899Z debug(journal): 2: recover_prepare: recovering slot=230
2025-11-22 02:05:19.899Z debug(journal): 2: recover_prepare: recovering slot=231
2025-11-22 02:05:19.899Z debug(journal): 2: recover_prepare: recovering slot=232
2025-11-22 02:05:19.901Z debug(journal): 2: recover_prepare: recovering slot=233
2025-11-22 02:05:19.904Z debug(journal): 2: recover_prepare: recovering slot=234
2025-11-22 02:05:19.904Z debug(journal): 2: recover_prepare: recovering slot=235
2025-11-22 02:05:19.905Z debug(journal): 2: recover_prepare: recovering slot=236
2025-11-22 02:05:19.905Z debug(journal): 2: recover_prepare: recovering slot=237
2025-11-22 02:05:19.905Z debug(journal): 2: recover_prepare: recovering slot=238
2025-11-22 02:05:19.906Z debug(journal): 2: recover_prepare: recovering slot=239
thread 1 panic: reached unreachable code
2025-11-22 02:05:19.906Z debug(journal): 2: recover_prepare: recovering slot=240
2025-11-22 02:05:19.918Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:20.175Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:20.175Z debug(journal): 2: recover_prepare: recovering slot=241
2025-11-22 02:05:20.178Z debug(journal): 2: recover_prepare: recovering slot=242
2025-11-22 02:05:20.178Z debug(journal): 2: recover_prepare: recovering slot=243
2025-11-22 02:05:20.179Z debug(journal): 2: recover_prepare: recovering slot=244
2025-11-22 02:05:20.179Z debug(journal): 2: recover_prepare: recovering slot=245
2025-11-22 02:05:19.918Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:20.180Z debug(vsr): 0: journal_repair_budget_timeout reset
/root/tigerbeetle/zig/lib/std/debug.zig:550:14: 0x12618cb in assert (vortex)
    if (!ok) unreachable; // assertion failure
             ^
2025-11-22 02:05:20.180Z debug(journal): 2: recover_prepare: recovering slot=246
/root/tigerbeetle/working/main/src/testing/vortex/faulty_network.zig:88:15: 0x13abd5b in recv (vortex)
        assert(!pipe.recv_inflight);
              ^
2025-11-22 02:05:20.180Z debug(journal): 2: recover_prepare: recovering slot=247
/root/tigerbeetle/working/main/src/testing/vortex/faulty_network.zig:233:22: 0x13aeddb in on_send (vortex)
            pipe.recv();
                     ^
/root/tigerbeetle/working/main/src/io/linux.zig:1856:25: 0x13ae873 in erased (vortex)
                callback(ctx, completion, result.*);
                        ^
/root/tigerbeetle/working/main/src/io/linux.zig:738:40: 0x1311773 in complete (vortex2025-11-22 02:05:20.181Z debug(journal): 2: recover_prepare: recovering slot=248
)
                    completion.callback(completion.context, completion, &result);
                                       ^
/root/tigerbeetle/working/main/src/io/linux.zig:194:49: 0x130fdbb in flush (vortex)
                .inactive => completion.complete(),
                                                ^
/root/tigerbeetle/working/main/src/io/linux.zig:149:27: 0x13121a3 in run_for_ns (vortex)
            try self.flush(1, &timeouts, &etime);
                          ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:263:41: 0x1312bbf in run (vortex)
            try supervisor.io.run_for_ns(constants.vsr.tick_ms * std.time.ns_per_ms);
                                        ^
/root/tigerbeetle/working/main/src/testing/vortex/supervisor.zig:207:23: 0x1316d0f in main (vortex)
    try supervisor.run();
                      ^
/root/tigerbeetle/working/main/src/vortex.zig:61:61: 0x132895f in main (vortex)
        .supervisor => |supervisor_args| try Supervisor.main(allocator, supervisor_args),
                                                            ^
2025-11-22 02:05:20.181Z debug(journal): 2: recover_prepare: recovering slot=249
/root/tigerbeetle/zig/lib/std/start.zig:660:37: 0x132935b in main (vortex)
            const result = root.main() catch |err| {
                                    ^
/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c:95:7: 0x1546117 in libc_start_main_stage2 (/root/tigerbeetle/zig/lib/libc/musl/src/env/__libc_start_main.c)
 exit(main(argc, argv, envp));
      ^
2025-11-22 02:05:20.183Z debug(journal): 2: recover_prepare: recovering slot=250
2025-11-22 02:05:20.183Z debug(journal): 2: recover_prepare: recovering slot=251
2025-11-22 02:05:20.184Z debug(journal): 2: recover_prepare: recovering slot=252
2025-11-22 02:05:20.184Z debug(journal): 2: recover_prepare: recovering slot=253
2025-11-22 02:05:20.184Z debug(journal): 2: recover_prepare: recovering slot=254
2025-11-22 02:05:20.184Z debug(journal): 2: recover_prepare: recovering slot=255
2025-11-22 02:05:20.184Z debug(journal): 2: recover_prepare: recovering slot=256
2025-11-22 02:05:20.185Z debug(journal): 2: recover_prepare: recovering slot=257
2025-11-22 02:05:20.185Z debug(journal): 2: recover_prepare: recovering slot=258
2025-11-22 02:05:20.186Z debug(journal): 2: recover_prepare: recovering slot=259
2025-11-22 02:05:20.186Z debug(journal): 2: recover_prepare: recovering slot=260
2025-11-22 02:05:20.186Z debug(journal): 2: recover_prepare: recovering slot=261
2025-11-22 02:05:20.187Z debug(journal): 2: recover_prepare: recovering slot=262
2025-11-22 02:05:20.187Z debug(journal): 2: recover_prepare: recovering slot=263
2025-11-22 02:05:20.189Z debug(journal): 2: recover_prepare: recovering slot=264
2025-11-22 02:05:20.190Z debug(journal): 2: recover_prepare: recovering slot=265
2025-11-22 02:05:20.190Z debug(journal): 2: recover_prepare: recovering slot=266
2025-11-22 02:05:20.191Z debug(journal): 2: recover_prepare: recovering slot=267
2025-11-22 02:05:20.191Z debug(journal): 2: recover_prepare: recovering slot=268
2025-11-22 02:05:20.191Z debug(journal): 2: recover_prepare: recovering slot=269
2025-11-22 02:05:20.193Z debug(journal): 2: recover_prepare: recovering slot=270
2025-11-22 02:05:20.193Z debug(journal): 2: recover_prepare: recovering slot=271
2025-11-22 02:05:20.193Z debug(journal): 2: recover_prepare: recovering slot=272
2025-11-22 02:05:20.194Z debug(journal): 2: recover_prepare: recovering slot=273
2025-11-22 02:05:20.194Z debug(journal): 2: recover_prepare: recovering slot=274
2025-11-22 02:05:20.195Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:20.195Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:20.196Z debug(journal): 2: recover_prepare: recovering slot=275
2025-11-22 02:05:20.196Z debug(journal): 2: recover_prepare: recovering slot=276
2025-11-22 02:05:20.196Z debug(journal): 2: recover_prepare: recovering slot=277
2025-11-22 02:05:20.197Z debug(journal): 2: recover_prepare: recovering slot=278
2025-11-22 02:05:20.197Z debug(journal): 2: recover_prepare: recovering slot=279
2025-11-22 02:05:20.197Z debug(journal): 2: recover_prepare: recovering slot=280
2025-11-22 02:05:20.197Z debug(journal): 2: recover_prepare: recovering slot=281
2025-11-22 02:05:20.198Z debug(journal): 2: recover_prepare: recovering slot=282
2025-11-22 02:05:20.199Z debug(journal): 2: recover_prepare: recovering slot=283
2025-11-22 02:05:20.200Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:20.200Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:20.201Z debug(journal): 2: recover_prepare: recovering slot=284
2025-11-22 02:05:20.202Z debug(journal): 2: recover_prepare: recovering slot=285
2025-11-22 02:05:20.202Z debug(journal): 2: recover_prepare: recovering slot=286
2025-11-22 02:05:20.202Z debug(journal): 2: recover_prepare: recovering slot=287
2025-11-22 02:05:20.202Z debug(journal): 2: recover_prepare: recovering slot=288
2025-11-22 02:05:20.205Z debug(journal): 2: recover_prepare: recovering slot=289
2025-11-22 02:05:20.207Z debug(journal): 2: recover_prepare: recovering slot=290
2025-11-22 02:05:20.207Z debug(journal): 2: recover_prepare: recovering slot=291
2025-11-22 02:05:20.207Z debug(journal): 2: recover_prepare: recovering slot=292
2025-11-22 02:05:20.208Z debug(journal): 2: recover_prepare: recovering slot=293
2025-11-22 02:05:20.208Z debug(journal): 2: recover_prepare: recovering slot=294
2025-11-22 02:05:20.208Z debug(journal): 2: recover_prepare: recovering slot=295
2025-11-22 02:05:20.209Z debug(journal): 2: recover_prepare: recovering slot=296
2025-11-22 02:05:20.209Z debug(journal): 2: recover_prepare: recovering slot=297
2025-11-22 02:05:20.209Z debug(journal): 2: recover_prepare: recovering slot=298
2025-11-22 02:05:20.210Z debug(journal): 2: recover_prepare: recovering slot=299
2025-11-22 02:05:20.211Z debug(journal): 2: recover_prepare: recovering slot=300
2025-11-22 02:05:20.213Z debug(journal): 2: recover_prepare: recovering slot=301
2025-11-22 02:05:20.213Z debug(journal): 2: recover_prepare: recovering slot=302
2025-11-22 02:05:20.213Z debug(journal): 2: recover_prepare: recovering slot=303
2025-11-22 02:05:20.213Z debug(journal): 2: recover_prepare: recovering slot=304
2025-11-22 02:05:20.214Z debug(journal): 2: recover_prepare: recovering slot=305
2025-11-22 02:05:20.215Z debug(journal): 2: recover_prepare: recovering slot=306
2025-11-22 02:05:20.215Z debug(journal): 2: recover_prepare: recovering slot=307
2025-11-22 02:05:20.215Z debug(journal): 2: recover_prepare: recovering slot=308
2025-11-22 02:05:20.215Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:20.215Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:20.217Z debug(journal): 2: recover_prepare: recovering slot=309
2025-11-22 02:05:20.219Z debug(journal): 2: recover_prepare: recovering slot=310
2025-11-22 02:05:20.219Z debug(journal): 2: recover_prepare: recovering slot=311
2025-11-22 02:05:20.219Z debug(journal): 2: recover_prepare: recovering slot=312
2025-11-22 02:05:20.220Z debug(journal): 2: recover_prepare: recovering slot=313
2025-11-22 02:05:20.220Z debug(vsr): 0: start_view_change_message_timeout fired
2025-11-22 02:05:20.220Z debug(vsr): 0: start_view_change_message_timeout reset
2025-11-22 02:05:20.220Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:20.220Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:20.220Z debug(vsr): 0: journal_repair_timeout fired
2025-11-22 02:05:20.220Z debug(vsr): 0: journal_repair_timeout reset
2025-11-22 02:05:20.220Z debug(vsr): 0: grid_repair_budget_timeout fired
2025-11-22 02:05:20.220Z debug(vsr): 0: grid_repair_budget_timeout reset
2025-11-22 02:05:20.220Z debug(vsr): 0: grid_scrub_timeout fired
2025-11-22 02:05:20.220Z debug(vsr): 0: grid_scrub_timeout reset
2025-11-22 02:05:20.222Z debug(journal): 2: recover_prepare: recovering slot=314
2025-11-22 02:05:20.222Z debug(grid_scrubber): 0: read_next: address=1 checksum=6f136732a15a52292e46997490d18ab4 type=value
2025-11-22 02:05:20.223Z debug(journal): 2: recover_prepare: recovering slot=315
2025-11-22 02:05:20.223Z debug(journal): 2: recover_prepare: recovering slot=316
2025-11-22 02:05:20.223Z debug(journal): 2: recover_prepare: recovering slot=317
2025-11-22 02:05:20.223Z debug(journal): 2: recover_prepare: recovering slot=318
2025-11-22 02:05:20.223Z debug(journal): 2: recover_prepare: recovering slot=319
2025-11-22 02:05:20.224Z debug(journal): 2: recover_prepare: recovering slot=320
2025-11-22 02:05:20.224Z debug(journal): 2: recover_prepare: recovering slot=321
2025-11-22 02:05:20.224Z debug(grid_scrubber): 0: read_next_callback: result=valid (address=1 checksum=6f136732a15a52292e46997490d18ab4 type=value status=vsr.grid_scrubber.GridScrubberType(lsm.forest.ForestType(storage.StorageType(io.linux.IO),.{ .accounts = lsm.groove.GrooveType(storage.StorageType(io.linux.IO),tigerbeetle.Account,.{ .ids = .{ ... }, .batch_value_count_max = .{ ... }, .ignored = &.{ ... }, .optional = &.{ ... }, .derived = .{ ... }, .orphaned_ids = false, .objects_cache = true }), .transfers = lsm.groove.GrooveType(storage.StorageType(io.linux.IO),tigerbeetle.Transfer,.{ .ids = .{ ... }, .batch_value_count_max = .{ ... }, .ignored = &.{ ... }, .optional = &.{ ... }, .derived = .{ ... }, .orphaned_ids = true, .objects_cache = true }), .transfers_pending = lsm.groove.GrooveType(storage.StorageType(io.linux.IO),state_machine.TransferPending,.{ .ids = .{ ... }, .batch_value_count_max = .{ ... }, .ignored = &.{ ... }, .optional = &.{ ... }, .derived = .{}, .orphaned_ids = false, .objects_cache = true }), .account_events = lsm.groove.GrooveType(storage.StorageType(io.linux.IO),state_machine.AccountEvent,.{ .ids = .{ ... }, .batch_value_count_max = .{ ... }, .ignored = &.{ ... }, .optional = &.{ ... }, .derived = .{ ... }, .orphaned_ids = false, .objects_cache = false }) }),32).BlockStatus.repair)
2025-11-22 02:05:20.225Z debug(journal): 2: recover_prepare: recovering slot=322
2025-11-22 02:05:20.226Z debug(journal): 2: recover_prepare: recovering slot=323
2025-11-22 02:05:20.228Z debug(journal): 2: recover_prepare: recovering slot=324
2025-11-22 02:05:20.229Z debug(journal): 2: recover_prepare: recovering slot=325
2025-11-22 02:05:20.229Z debug(journal): 2: recover_prepare: recovering slot=326
2025-11-22 02:05:20.230Z debug(journal): 2: recover_prepare: recovering slot=327
2025-11-22 02:05:20.230Z debug(journal): 2: recover_prepare: recovering slot=328
2025-11-22 02:05:20.235Z debug(journal): 2: recover_prepare: recovering slot=329
2025-11-22 02:05:20.235Z debug(journal): 2: recover_prepare: recovering slot=330
2025-11-22 02:05:20.236Z debug(vsr): 1: start_view_change_message_timeout fired
2025-11-22 02:05:20.236Z debug(vsr): 1: start_view_change_message_timeout reset
2025-11-22 02:05:20.236Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:20.236Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:20.236Z debug(vsr): 1: journal_repair_timeout fired
2025-11-22 02:05:20.236Z debug(vsr): 1: journal_repair_timeout reset
2025-11-22 02:05:20.236Z debug(vsr): 1: grid_repair_budget_timeout fired
2025-11-22 02:05:20.236Z debug(vsr): 1: grid_repair_budget_timeout reset
2025-11-22 02:05:20.238Z debug(journal): 2: recover_prepare: recovering slot=331
2025-11-22 02:05:20.238Z debug(journal): 2: recover_prepare: recovering slot=332
2025-11-22 02:05:20.238Z debug(journal): 2: recover_prepare: recovering slot=333
2025-11-22 02:05:20.238Z debug(journal): 2: recover_prepare: recovering slot=334
2025-11-22 02:05:20.238Z debug(journal): 2: recover_prepare: recovering slot=335
2025-11-22 02:05:20.241Z debug(journal): 2: recover_prepare: recovering slot=336
2025-11-22 02:05:20.242Z debug(journal): 2: recover_prepare: recovering slot=337
2025-11-22 02:05:20.242Z debug(journal): 2: recover_prepare: recovering slot=338
2025-11-22 02:05:20.242Z debug(journal): 2: recover_prepare: recovering slot=339
2025-11-22 02:05:20.242Z debug(journal): 2: recover_prepare: recovering slot=340
2025-11-22 02:05:20.242Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:20.242Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:20.244Z debug(journal): 2: recover_prepare: recovering slot=341
2025-11-22 02:05:20.244Z debug(journal): 2: recover_prepare: recovering slot=342
2025-11-22 02:05:20.245Z debug(journal): 2: recover_prepare: recovering slot=343
2025-11-22 02:05:20.245Z debug(journal): 2: recover_prepare: recovering slot=344
2025-11-22 02:05:20.246Z debug(journal): 2: recover_prepare: recovering slot=345
2025-11-22 02:05:20.246Z debug(journal): 2: recover_prepare: recovering slot=346
2025-11-22 02:05:20.246Z debug(journal): 2: recover_prepare: recovering slot=347
2025-11-22 02:05:20.246Z debug(journal): 2: recover_prepare: recovering slot=348
2025-11-22 02:05:20.247Z debug(journal): 2: recover_prepare: recovering slot=349
2025-11-22 02:05:20.247Z debug(journal): 2: recover_prepare: recovering slot=350
2025-11-22 02:05:20.247Z debug(journal): 2: recover_prepare: recovering slot=351
2025-11-22 02:05:20.247Z debug(journal): 2: recover_prepare: recovering slot=352
2025-11-22 02:05:20.248Z debug(journal): 2: recover_prepare: recovering slot=353
2025-11-22 02:05:20.248Z debug(journal): 2: recover_prepare: recovering slot=354
2025-11-22 02:05:20.249Z debug(journal): 2: recover_prepare: recovering slot=355
2025-11-22 02:05:20.249Z debug(journal): 2: recover_prepare: recovering slot=356
2025-11-22 02:05:20.249Z debug(journal): 2: recover_prepare: recovering slot=357
2025-11-22 02:05:20.249Z debug(journal): 2: recover_prepare: recovering slot=358
2025-11-22 02:05:20.250Z debug(journal): 2: recover_prepare: recovering slot=359
2025-11-22 02:05:20.250Z debug(journal): 2: recover_prepare: recovering slot=360
2025-11-22 02:05:20.250Z debug(journal): 2: recover_prepare: recovering slot=361
2025-11-22 02:05:20.250Z debug(journal): 2: recover_prepare: recovering slot=362
2025-11-22 02:05:20.251Z debug(journal): 2: recover_prepare: recovering slot=363
2025-11-22 02:05:20.252Z debug(journal): 2: recover_prepare: recovering slot=364
2025-11-22 02:05:20.253Z debug(journal): 2: recover_prepare: recovering slot=365
2025-11-22 02:05:20.253Z debug(journal): 2: recover_prepare: recovering slot=366
2025-11-22 02:05:20.254Z debug(journal): 2: recover_prepare: recovering slot=367
2025-11-22 02:05:20.254Z debug(journal): 2: recover_prepare: recovering slot=368
2025-11-22 02:05:20.255Z debug(journal): 2: recover_prepare: recovering slot=369
2025-11-22 02:05:20.256Z debug(vsr): 1: journal_repair_budget_timeout fired
2025-11-22 02:05:20.256Z debug(vsr): 1: journal_repair_budget_timeout reset
2025-11-22 02:05:20.256Z debug(journal): 2: recover_prepare: recovering slot=370
2025-11-22 02:05:20.256Z debug(journal): 2: recover_prepare: recovering slot=371
2025-11-22 02:05:20.257Z debug(journal): 2: recover_prepare: recovering slot=372
2025-11-22 02:05:20.257Z debug(journal): 2: recover_prepare: recovering slot=373
2025-11-22 02:05:20.258Z debug(journal): 2: recover_prepare: recovering slot=374
2025-11-22 02:05:20.258Z debug(journal): 2: recover_prepare: recovering slot=375
2025-11-22 02:05:20.258Z debug(journal): 2: recover_prepare: recovering slot=376
2025-11-22 02:05:20.259Z debug(journal): 2: recover_prepare: recovering slot=377
2025-11-22 02:05:20.261Z debug(journal): 2: recover_prepare: recovering slot=378
2025-11-22 02:05:20.262Z debug(vsr): 0: journal_repair_budget_timeout fired
2025-11-22 02:05:20.263Z debug(vsr): 0: journal_repair_budget_timeout reset
2025-11-22 02:05:20.263Z debug(journal): 2: recover_prepare: recovering slot=379
2025-11-22 02:05:20.263Z debug(journal): 2: recover_prepare: recovering slot=380
2025-11-22 02:05:20.263Z debug(journal): 2: recover_prepare: recovering slot=381
2025-11-22 02:05:20.263Z debug(journal): 2: recover_prepare: recovering slot=382
2025-11-22 02:05:20.263Z debug(journal): 2: recover_prepare: recovering slot=383
2025-11-22 02:05:20.264Z debug(journal): 2: recover_prepare: recovering slot=384
2025-11-22 02:05:20.264Z debug(journal): 2: recover_prepare: recovering slot=385
2025-11-22 02:05:20.264Z debug(journal): 2: recover_prepare: recovering slot=386
2025-11-22 02:05:20.264Z debug(journal): 2: recover_prepare: recovering slot=387
2025-11-22 02:05:20.264Z debug(journal): 2: recover_prepare: recovering slot=388
warning(message_bus): 25527186891061144814045155886610748587: on_recv: from=vsr.Peer{ .replica = 1 } error.ConnectionResetByPeer
2025-11-22 02:05:20.283Z info(unshare): sandboxed subprocesses exited with signal 11
